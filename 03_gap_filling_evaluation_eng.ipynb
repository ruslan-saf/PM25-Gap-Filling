{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working code for single-stream forecasting using univariate and multivariate approaches\n",
    "\n",
    "- Added the ability to save results\n",
    "- Added multivariate XGB seq2seq model (bidirectional)\n",
    "- Use flags (e.g., in a dictionary or list) to control which models to add\n",
    "- Added multivariate UniSeq2Seq XGB model (unidirectional)\n",
    "- Added multivariate bidirectional RF Seq2Seq model\n",
    "- v. 5\n",
    "- Added multivariate unidirectional autoregressive XGB model (uniautoreg_xgb)\n",
    "- Added multivariate unidirectional UniSeq2Seq CNN Multi model\n",
    "- V. 6\n",
    "- Added multivariate unidirectional UniSeq2Seq LSTM Multi model\n",
    "- Implemented a flag-based approach for adding models to the combiner\n",
    "- ! There might be data leaks, making the model too successful.\n",
    "- V.7\n",
    "- Data leaks have been fixed\n",
    "- V. 8\n",
    "- Added multivariate bidirectional XGB Autoreg Multi model\n",
    "- Added index-based model addition to the combiner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(\"TensorFlow Version:\", tf.__version__)\n",
    "print(\"Num GPUs Available:\", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import subprocess\n",
    "# import sys\n",
    "# import platform\n",
    "# import psutil\n",
    "# import GPUtil\n",
    "\n",
    "# # CPU information\n",
    "# def get_cpu_info():\n",
    "#     cpu_info = {}\n",
    "#     cpu_info['processor'] = platform.processor()\n",
    "#     cpu_info['physical_cores'] = psutil.cpu_count(logical=False)\n",
    "#     cpu_info['total_cores'] = psutil.cpu_count(logical=True)\n",
    "    \n",
    "#     # For Linux, more detailed information can be obtained\n",
    "#     if platform.system() == \"Linux\":\n",
    "#         try:\n",
    "#             cpu_model = subprocess.check_output(\"cat /proc/cpuinfo | grep 'model name' | uniq\", shell=True).decode().strip().split(':')[1]\n",
    "#             cpu_info['model'] = cpu_model\n",
    "#         except:\n",
    "#             pass\n",
    "    \n",
    "#     return cpu_info\n",
    "\n",
    "# # GPU information via TensorFlow\n",
    "# def get_tf_gpu_info():\n",
    "#     gpus = tf.config.list_physical_devices('GPU')\n",
    "#     gpu_info = []\n",
    "#     for gpu in gpus:\n",
    "#         gpu_details = tf.config.experimental.get_device_details(gpu)\n",
    "#         gpu_info.append(gpu_details)\n",
    "#     return gpu_info\n",
    "\n",
    "# # GPU information via PyTorch\n",
    "# def get_torch_gpu_info():\n",
    "#     if torch.cuda.is_available():\n",
    "#         gpu_info = {\n",
    "#             'cuda_available': True,\n",
    "#             'device_count': torch.cuda.device_count(),\n",
    "#             'current_device': torch.cuda.current_device(),\n",
    "#             'device_name': torch.cuda.get_device_name(0),\n",
    "#             'cuda_version': torch.version.cuda,\n",
    "#         }\n",
    "#     else:\n",
    "#         gpu_info = {'cuda_available': False}\n",
    "#     return gpu_info\n",
    "\n",
    "# # GPU information via GPUtil\n",
    "# def get_gputil_info():\n",
    "#     gpus = GPUtil.getGPUs()\n",
    "#     gpu_info = []\n",
    "#     for gpu in gpus:\n",
    "#         info = {\n",
    "#             'id': gpu.id,\n",
    "#             'name': gpu.name,\n",
    "#             'memory_total': gpu.memoryTotal,\n",
    "#             'driver': gpu.driver,\n",
    "#         }\n",
    "#         gpu_info.append(info)\n",
    "#     return gpu_info\n",
    "\n",
    "# # NVIDIA information via subprocess\n",
    "# def get_nvidia_smi_info():\n",
    "#     try:\n",
    "#         nvidia_smi = subprocess.check_output('nvidia-smi --query-gpu=name,memory.total,driver_version --format=csv,noheader', shell=True).decode()\n",
    "#         return nvidia_smi\n",
    "#     except:\n",
    "#         return \"NVIDIA-SMI failed or NVIDIA driver is not installed\"\n",
    "\n",
    "# # RAM information\n",
    "# def get_ram_info():\n",
    "#     ram = psutil.virtual_memory()\n",
    "#     ram_info = {\n",
    "#         'total': ram.total / (1024**3),  # GB\n",
    "#         'available': ram.available / (1024**3),  # GB\n",
    "#         'percent_used': ram.percent\n",
    "#     }\n",
    "#     return ram_info\n",
    "\n",
    "# # Collecting all information\n",
    "# system_info = {\n",
    "#     'platform': platform.platform(),\n",
    "#     'python_version': sys.version,\n",
    "#     'cpu': get_cpu_info(),\n",
    "#     'ram': get_ram_info(),\n",
    "#     'tf_gpu': get_tf_gpu_info(),\n",
    "#     'torch_gpu': get_torch_gpu_info(),\n",
    "#     'gputil': get_gputil_info(),\n",
    "#     'nvidia_smi': get_nvidia_smi_info()\n",
    "# }\n",
    "\n",
    "# print(system_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "import tensorflow as tf\n",
    "from tcn import TCN\n",
    "from tensorflow.keras.layers import LSTM, GRU, Conv1D, SimpleRNN, Dense, Flatten, Input, Concatenate, Bidirectional, RepeatVector, TimeDistributed, Reshape\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from xgboost import XGBRegressor\n",
    "from tabulate import tabulate\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import interpolate\n",
    "import seaborn as sns\n",
    "import gc\n",
    "import tensorflow.keras.backend as K\n",
    "import re\n",
    "\n",
    "# Variables for creating and training models\n",
    "EPOCHS = 200\n",
    "BATCH_SIZE = 32\n",
    "PATIENCE = 20\n",
    "N_ESTIMATORS = 50\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(\n",
    "    file_path=\"df_data_prepared.csv\",\n",
    "    column=\"pm2_5\",\n",
    "    multivariate=False,\n",
    "    train_size_ratio=0.8\n",
    "):\n",
    "\n",
    "    # 1) Load and sort\n",
    "    df = pd.read_csv(file_path, encoding=\"utf-8\", parse_dates=[\"date\"])\n",
    "    df = df.dropna(subset=[column]).reset_index(drop=True)\n",
    "    df.sort_values(\"date\", inplace=True)\n",
    "\n",
    "    # 2) Add \"pm2_5_original\" column to have the \"true\" values later\n",
    "    df[f\"{column}_original\"] = df[column]\n",
    "\n",
    "    # 3) Split chronologically: first 80% -> train, last 20% -> test\n",
    "    n = len(df)\n",
    "    train_size = int(n * train_size_ratio)\n",
    "    df_train = df.iloc[:train_size]\n",
    "    df_test = df.iloc[train_size:]\n",
    "\n",
    "    # 4) Train scaler on the train part, then transform the test part\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    if not multivariate:\n",
    "        # --- UNIVARIATE ---\n",
    "        train_data = df_train[column].values.reshape(-1, 1)\n",
    "        test_data = df_test[column].values.reshape(-1, 1)\n",
    "\n",
    "        scaler.fit(train_data)\n",
    "\n",
    "        train_data_scaled = scaler.transform(train_data).flatten()\n",
    "        test_data_scaled = scaler.transform(test_data).flatten()\n",
    "\n",
    "        # Concatenate\n",
    "        data_scaled = np.concatenate([train_data_scaled, test_data_scaled], axis=0)\n",
    "\n",
    "    else:\n",
    "        # --- MULTIVARIATE ---\n",
    "        df_train['hour'] = df_train['date'].dt.hour\n",
    "        df_train['season'] = (df_train['date'].dt.month % 12 // 3).astype(int)\n",
    "        df_test['hour'] = df_test['date'].dt.hour\n",
    "        df_test['season'] = (df_test['date'].dt.month % 12 // 3).astype(int)\n",
    "\n",
    "        # Approximate list of features + original value of pm2_5_original\n",
    "        numeric_cols = [\n",
    "            'pm2_5', 'air_temperature', 'air_humidity', 'T', 'P0',\n",
    "            'P', 'U', 'DD', 'Ff', 'VV', 'pm2_5_original', 'hour', 'season'\n",
    "        ]\n",
    "        # Remove those that are not in df\n",
    "        numeric_cols = [c for c in numeric_cols if c in df_train.columns and c in df_test.columns]\n",
    "\n",
    "        train_data = df_train[numeric_cols].values\n",
    "        test_data = df_test[numeric_cols].values\n",
    "\n",
    "        scaler.fit(train_data)\n",
    "        train_data_scaled = scaler.transform(train_data)\n",
    "        test_data_scaled = scaler.transform(test_data)\n",
    "\n",
    "        data_scaled = np.vstack([train_data_scaled, test_data_scaled])\n",
    "\n",
    "    # Check\n",
    "    assert data_scaled.shape[0] == len(df), \"data_scaled and df have different lengths!\"\n",
    "\n",
    "    return df, data_scaled, scaler\n",
    "\n",
    "def time_based_split(X, y, train_size_ratio=0.8):\n",
    "    \"\"\"\n",
    "    Splits X and y into train/test chronologically: \n",
    "    first train_size_ratio%, then the rest.\n",
    "    \"\"\"\n",
    "    n = len(X)\n",
    "    train_size = int(n * train_size_ratio)\n",
    "    X_train = X[:train_size]\n",
    "    y_train = y[:train_size]\n",
    "    X_test = X[train_size:]\n",
    "    y_test = y[train_size:]\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def introduce_synthetic_gaps(df, column=\"pm2_5\", missing_fraction=0.05, gap_length=12, random_state=42):\n",
    "    np.random.seed(random_state)\n",
    "    df_missing = df.copy()\n",
    "    n = len(df_missing)\n",
    "    n_gaps = int(n * missing_fraction / gap_length)\n",
    "    gap_indices = np.random.choice(np.arange(gap_length, n - gap_length * 2), n_gaps, replace=False)\n",
    "    \n",
    "    # Add detailed debugging\n",
    "    # print(f\"Total gaps to create: {n_gaps}\")\n",
    "    # print(f\"Sample gap start indices: {gap_indices[:5]}\")\n",
    "    # print(f\"Gap length: {gap_length}\")\n",
    "    # print(f\"Data length: {n}\")\n",
    "    # print(f\"NaN count in {column} before gaps: {df_missing[column].isna().sum()}\")\n",
    "    \n",
    "    for idx in gap_indices:\n",
    "        df_missing.loc[idx:idx + gap_length - 1, column] = np.nan\n",
    "    \n",
    "    # print(f\"NaN count in {column} after gaps: {df_missing[column].isna().sum()}\")\n",
    "    # print(f\"Sample data after gaps with NaN:\\n{df_missing[column].head(20).isna().sum()}\")\n",
    "    \n",
    "    return df_missing, gap_indices\n",
    "\n",
    "def evaluate_model(y_true, y_pred):\n",
    "    # print(f\"y_true shape: {y_true.shape}, y_pred shape: {y_pred.shape}\")\n",
    "    # print(f\"y_true sample: {y_true[:5]}, y_pred sample: {y_pred[:5]}\")\n",
    "    epsilon = 1e-8\n",
    "    mae = round(mean_absolute_error(y_true.ravel(), y_pred.ravel()), 3)\n",
    "    rmse = round(np.sqrt(mean_squared_error(y_true.ravel(), y_pred.ravel())), 3)\n",
    "    r2 = round(r2_score(y_true.ravel(), y_pred.ravel()), 3)\n",
    "    mape = round(np.mean(np.abs((y_true - y_pred) / (y_true + epsilon))) * 100, 3)\n",
    "    return mae, rmse, r2, mape\n",
    "\n",
    "def combine_forecasts(forward_forecast, backward_forecast):\n",
    "    gap_length = len(forward_forecast)\n",
    "    combined = np.zeros_like(forward_forecast)\n",
    "    for t in range(gap_length):\n",
    "        w = 1 - t/(gap_length - 1) if gap_length > 1 else 0.5\n",
    "        combined[t] = w * forward_forecast[t] + (1 - w) * backward_forecast[t]\n",
    "    return combined\n",
    "\n",
    "\n",
    "# Data preparation functions \n",
    "def create_forward_data_seq2seq(data, pre_context_length, gap_length):\n",
    "    X_forward, y_forward = [], []\n",
    "    for i in range(pre_context_length, len(data) - gap_length):\n",
    "        X_forward.append(data[i - pre_context_length:i].reshape(-1, 1))\n",
    "        y_forward.append(data[i:i + gap_length].reshape(-1, 1))\n",
    "    return np.array(X_forward), np.array(y_forward)\n",
    "\n",
    "def create_backward_data_seq2seq(data, post_context_length, gap_length):\n",
    "    X_backward, y_backward = [], []\n",
    "    for i in range(gap_length, len(data) - post_context_length + 1):\n",
    "        X = data[i:i + post_context_length].reshape(-1, 1)\n",
    "        y = data[i - gap_length:i].reshape(-1, 1)\n",
    "        X_backward.append(np.flipud(X))\n",
    "        y_backward.append(np.flipud(y))\n",
    "    return np.array(X_backward), np.array(y_backward)\n",
    "\n",
    "def create_forward_data_autoreg(data, pre_context_length):\n",
    "    X_forward, y_forward = [], []\n",
    "    for i in range(pre_context_length, len(data) - 1):\n",
    "        X_forward.append(data[i - pre_context_length:i].reshape(-1, 1))\n",
    "        y_forward.append(data[i])  # Predict one value\n",
    "    return np.array(X_forward), np.array(y_forward)\n",
    "\n",
    "def create_backward_data_autoreg(data, post_context_length):\n",
    "    X_backward, y_backward = [], []\n",
    "    for i in range(1, len(data) - post_context_length):\n",
    "        X = data[i:i + post_context_length].reshape(-1, 1)\n",
    "        y = data[i - 1]  # Predict one value back\n",
    "        X_backward.append(np.flipud(X))\n",
    "        y_backward.append(y)\n",
    "    return np.array(X_backward), np.array(y_backward)\n",
    "\n",
    "def create_uniseq2seq_data(data, pre_context_length, gap_length, post_context_length):\n",
    "    X_left, X_right, y = [], [], []\n",
    "    for i in range(pre_context_length, len(data) - gap_length - post_context_length):\n",
    "        left_context = data[i - pre_context_length:i].reshape(-1, 1)\n",
    "        right_context = data[i + gap_length:i + gap_length + post_context_length].reshape(-1, 1)\n",
    "        gap = data[i:i + gap_length].reshape(-1, 1)\n",
    "        X_left.append(left_context)\n",
    "        X_right.append(right_context)\n",
    "        y.append(gap)\n",
    "    return np.array(X_left), np.array(X_right), np.array(y)\n",
    "\n",
    "def create_uniseq2seq_data_multi(data, pre_context_length, gap_length, post_context_length, feature_cols=None):\n",
    "    # Input data validation\n",
    "    if not isinstance(data, np.ndarray):\n",
    "        raise TypeError(f\"Input data must be a numpy array, got {type(data)}\")\n",
    "    if len(data.shape) != 2:\n",
    "        raise ValueError(f\"Input data must be 2D, got shape {data.shape}\")\n",
    "    \n",
    "    print(f\"Debug: Input data shape in create_uniseq2seq_data_multi: {data.shape}\")\n",
    "    print(f\"Debug: Input data type in create_uniseq2seq_data_multi: {type(data)}\")\n",
    "    print(f\"Debug: feature_cols: {feature_cols}\")\n",
    "    \n",
    "    if feature_cols is None:\n",
    "        feature_cols = [0]  # Default to only pm2_5\n",
    "    \n",
    "    # Validation of feature_cols\n",
    "    if not all(0 <= idx < data.shape[1] for idx in feature_cols):\n",
    "        raise ValueError(f\"Feature indices {feature_cols} are out of bounds for data with {data.shape[1]} columns\")\n",
    "    \n",
    "    X_left, X_right, y = [], [], []\n",
    "    for i in range(pre_context_length, len(data) - gap_length - post_context_length):\n",
    "        left_context = data[i - pre_context_length:i, feature_cols].reshape(pre_context_length, -1)\n",
    "        right_context = data[i + gap_length:i + gap_length + post_context_length, feature_cols].reshape(post_context_length, -1)\n",
    "        gap = data[i:i + gap_length, 0].reshape(gap_length, 1)  # pm2_5 as the target\n",
    "        X_left.append(left_context)\n",
    "        X_right.append(right_context)\n",
    "        y.append(gap)\n",
    "    return np.array(X_left), np.array(X_right), np.array(y)\n",
    "\n",
    "def create_forward_data_seq2seq_multi(data, pre_context_length, gap_length, feature_cols):\n",
    "    \"\"\"\n",
    "    Forms data for forward seq2seq forecasting in multivariate mode.\n",
    "    For each example, a context of specified features (feature_cols)\n",
    "    of size (pre_context_length, len(feature_cols)) is selected, and a target interval from the first feature.\n",
    "    \"\"\"\n",
    "    X_forward, y_forward = [], []\n",
    "    for i in range(pre_context_length, len(data) - gap_length):\n",
    "        context = data[i - pre_context_length:i, feature_cols]  # (pre_context_length, n_features)\n",
    "        target = data[i:i + gap_length, 0].reshape(-1, 1)         # only the target feature\n",
    "        X_forward.append(context)\n",
    "        y_forward.append(target)\n",
    "    return np.array(X_forward), np.array(y_forward)\n",
    "\n",
    "def create_backward_data_seq2seq_multi(data, post_context_length, gap_length, feature_cols):\n",
    "    \"\"\"\n",
    "    Forms data for backward seq2seq forecasting in multivariate mode.\n",
    "    A context after the gap (with specified features) and a target interval from the first feature are extracted.\n",
    "    Before use, the input context is reversed in time.\n",
    "    \"\"\"\n",
    "    X_backward, y_backward = [], []\n",
    "    for i in range(gap_length, len(data) - post_context_length + 1):\n",
    "        X = data[i:i + post_context_length, feature_cols]\n",
    "        y = data[i - gap_length:i, 0].reshape(-1, 1)\n",
    "        X_backward.append(np.flipud(X))\n",
    "        y_backward.append(np.flipud(y))\n",
    "    return np.array(X_backward), np.array(y_backward)\n",
    "\n",
    "def create_forward_data_autoreg_multi(data, pre_context_length, feature_cols):\n",
    "    \"\"\"\n",
    "    Forms data for autoregression in multivariate mode.\n",
    "    For each example, a window (pre_context_length, n_selected) is extracted,\n",
    "    where n_selected = len(feature_cols), and the target value is the value of the target feature from the first position (feature_cols[0]).\n",
    "    \"\"\"\n",
    "    X_forward, y_forward = [], []\n",
    "    for i in range(pre_context_length, len(data) - 1):\n",
    "        # Extract window only for selected features\n",
    "        context = data[i - pre_context_length:i][:, feature_cols]\n",
    "        X_forward.append(context)\n",
    "        # Target is the value of the target feature (feature_cols[0]) at the current position\n",
    "        y_forward.append(data[i, feature_cols[0]])\n",
    "    return np.array(X_forward), np.array(y_forward)\n",
    "\n",
    "def create_backward_data_autoreg_multi(data, post_context_length, feature_cols):\n",
    "    \"\"\"\n",
    "    Forms data for the backward part of multivariate single-step autoregression.\n",
    "    \n",
    "    Parameters:\n",
    "      - data: numpy array of shape (n_samples, n_total_features).\n",
    "      - post_context_length: length of the \"window\" (number of steps) after the gap,\n",
    "        which will be used as the context in the reverse direction.\n",
    "      - feature_cols: list of feature indices to take from data\n",
    "        (e.g., [0, 1, 2]).\n",
    "\n",
    "    Returns:\n",
    "      - X_backward: array of shape (n_samples_b, post_context_length, n_selected_features),\n",
    "        where n_selected_features = len(feature_cols).\n",
    "        Each window example is taken in forward order but is reversed\n",
    "        along the time axis (np.flipud) before saving.\n",
    "      - y_backward: vector of shape (n_samples_b, ) with values of the target feature (feature_cols[0])\n",
    "        \"one step earlier\" (data[i-1, feature_cols[0]]).\n",
    "    \"\"\"\n",
    "    X_backward, y_backward = [], []\n",
    "    n = len(data)\n",
    "    for i in range(1, n - post_context_length):\n",
    "        # Extract a window of length post_context_length, starting from i, for the required features\n",
    "        window = data[i : i + post_context_length, feature_cols]  # (post_context_length, len(feature_cols))\n",
    "        \n",
    "        # Reverse along the time axis (as in the univariate version)\n",
    "        window_flipped = np.flipud(window)\n",
    "        \n",
    "        # The target value is taken \"one step earlier\" â€” which is i - 1\n",
    "        # Use the first feature from feature_cols as the target\n",
    "        target_value = data[i - 1, feature_cols[0]]\n",
    "        \n",
    "        X_backward.append(window_flipped)\n",
    "        y_backward.append(target_value)\n",
    "    \n",
    "    return np.array(X_backward), np.array(y_backward)\n",
    "\n",
    "\n",
    "def create_uniseq2seq_data_multi_forward(data, pre_context_length, gap_length, feature_cols=None):\n",
    "    \"\"\"\n",
    "    Forms data for a unidirectional (using only the left context) seq2seq forecast\n",
    "    in multivariate mode.\n",
    "    \n",
    "    For each example, a left context with selected features of size\n",
    "    (pre_context_length, n_selected) and a target interval from the first feature (feature_cols[0]) are extracted.\n",
    "    \"\"\"\n",
    "    if feature_cols is None:\n",
    "        feature_cols = [0]\n",
    "    X_left, y = [], []\n",
    "    for i in range(pre_context_length, len(data) - gap_length + 1):\n",
    "        left_context = data[i - pre_context_length:i, feature_cols]  # (pre_context_length, n_selected)\n",
    "        gap = data[i:i + gap_length, 0].reshape(gap_length, 1)         # target is only pm2_5\n",
    "        X_left.append(left_context)\n",
    "        y.append(gap)\n",
    "    return np.array(X_left), np.array(y)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def test_synthetic_gaps_imputation_efficiency(df, data_scaled, gap_indices, forecast_func, \n",
    "                                              forward_model, backward_model, \n",
    "                                              pre_context_length, gap_length, post_context_length, \n",
    "                                              scaler, model_name, model_info):\n",
    "    all_true_gaps = []\n",
    "    all_pred_gaps = []\n",
    "    # If feature_cols are set for the model, extract them from model_info\n",
    "    fc = model_info.get(\"feature_cols\", None)\n",
    "    for gap_idx in gap_indices:\n",
    "        if gap_idx - pre_context_length < 0 or gap_idx + gap_length + post_context_length > len(data_scaled):\n",
    "            continue\n",
    "\n",
    "        # If data is multivariate and feature_cols are specified, select only these columns\n",
    "        if fc is not None and isinstance(data_scaled, np.ndarray) and data_scaled.ndim == 2:\n",
    "            initial_context_forward = data_scaled[gap_idx - pre_context_length : gap_idx][:, fc]\n",
    "            initial_context_backward = data_scaled[gap_idx + gap_length : gap_idx + gap_length + post_context_length][:, fc]\n",
    "        else:\n",
    "            initial_context_forward = data_scaled[gap_idx - pre_context_length : gap_idx]\n",
    "            initial_context_backward = data_scaled[gap_idx + gap_length : gap_idx + gap_length + post_context_length]\n",
    "\n",
    "        # Forward and backward forecasting\n",
    "        forward_forecast_scaled = forecast_func(forward_model, initial_context_forward, gap_length)\n",
    "        initial_context_backward = np.flipud(initial_context_backward)\n",
    "        backward_forecast_scaled = forecast_func(backward_model, initial_context_backward, gap_length)\n",
    "        backward_forecast_scaled = np.flipud(backward_forecast_scaled)\n",
    "        combined_forecast_scaled = combine_forecasts(forward_forecast_scaled, backward_forecast_scaled)\n",
    "        \n",
    "        # Inverse scaling: if scaler is trained on multivariate data, apply it only to the target feature (index 0)\n",
    "        if hasattr(scaler, 'mean_') and scaler.mean_.shape[0] > 1:\n",
    "            mean_pm25 = scaler.mean_[0]\n",
    "            scale_pm25 = scaler.scale_[0]\n",
    "            combined_forecast = combined_forecast_scaled * scale_pm25 + mean_pm25\n",
    "        else:\n",
    "            combined_forecast = scaler.inverse_transform(combined_forecast_scaled)\n",
    "        \n",
    "        true_gap = df.loc[gap_idx : gap_idx + gap_length - 1, \"pm2_5_original\"].values.reshape(-1, 1)\n",
    "        if np.any(np.isnan(combined_forecast)) or np.any(np.isnan(true_gap)):\n",
    "            continue\n",
    "        all_pred_gaps.append(combined_forecast)\n",
    "        all_true_gaps.append(true_gap)\n",
    "    if len(all_true_gaps) == 0 or len(all_pred_gaps) == 0:\n",
    "        raise ValueError(\"No processed gaps to calculate metrics.\")\n",
    "    all_true = np.concatenate(all_true_gaps)\n",
    "    all_pred = np.concatenate(all_pred_gaps)\n",
    "    return all_true, all_pred\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def test_synthetic_gaps_imputation_efficiency_uniseq2seq(df, data_scaled, gap_indices, forecast_func, model, \n",
    "                                                          pre_context_length, gap_length, post_context_length, \n",
    "                                                          scaler, model_name):\n",
    "    all_true = []\n",
    "    all_pred = []\n",
    "    # Get the list of features from the model if it's saved\n",
    "    if hasattr(model, 'model_info') and 'feature_cols' in model.model_info:\n",
    "        feature_cols = model.model_info['feature_cols']\n",
    "    else:\n",
    "        feature_cols = [0]  # default is only pm2_5\n",
    "\n",
    "    for gap_idx in gap_indices:\n",
    "        # Check window boundaries\n",
    "        if gap_idx - pre_context_length < 0 or gap_idx + gap_length + post_context_length > len(data_scaled):\n",
    "            continue\n",
    "        \n",
    "        # If the method is unidirectional (e.g., \"UniSeq2Seq LSTM Multi\" or \"UniSeq2Seq CNN Multi\"),\n",
    "        # use only the left context.\n",
    "        if model_name in [\"UniSeq2Seq LSTM Multi\", \"UniSeq2Seq CNN Multi\"]:\n",
    "            initial_context = data_scaled[gap_idx - pre_context_length:gap_idx][:, feature_cols]\n",
    "        else:\n",
    "            # For the bidirectional variant, extract and combine the left and right contexts\n",
    "            if len(feature_cols) == 1 and feature_cols[0] == 0:\n",
    "                left_context = data_scaled[gap_idx - pre_context_length:gap_idx].reshape(-1, 1)\n",
    "                right_context = data_scaled[gap_idx + gap_length:gap_idx + gap_length + post_context_length].reshape(-1, 1)\n",
    "            else:\n",
    "                left_context = data_scaled[gap_idx - pre_context_length:gap_idx, feature_cols]\n",
    "                right_context = data_scaled[gap_idx + gap_length:gap_idx + gap_length + post_context_length, feature_cols]\n",
    "            initial_context = np.concatenate([left_context, right_context])\n",
    "        \n",
    "        # Get the forecast\n",
    "        forecast_scaled = forecast_func(model, initial_context, gap_length)\n",
    "        \n",
    "        # Inverse scaling: if the scaler is trained on multivariate data,\n",
    "        # apply it only to the target feature (assuming it's scaler.mean_[0])\n",
    "        if hasattr(scaler, 'mean_') and scaler.mean_.shape[0] > 1:\n",
    "            forecast = forecast_scaled * float(scaler.scale_[0]) + float(scaler.mean_[0])\n",
    "            forecast = forecast.reshape(-1, 1)\n",
    "        else:\n",
    "            forecast = scaler.inverse_transform(forecast_scaled)\n",
    "        \n",
    "        true_gap = df.loc[gap_idx:gap_idx + gap_length - 1, \"pm2_5_original\"].values.reshape(-1, 1)\n",
    "        if not np.any(np.isnan(true_gap)) and not np.any(np.isnan(forecast)):\n",
    "            all_true.append(true_gap.ravel())\n",
    "            all_pred.append(forecast.ravel())\n",
    "        else:\n",
    "            print(f\"Skipping gap_idx={gap_idx} due to NaN\")\n",
    "    \n",
    "    if not all_true or not all_pred:\n",
    "        raise ValueError(\"No processed gaps to calculate metrics.\")\n",
    "    all_true = np.concatenate(all_true)\n",
    "    all_pred = np.concatenate(all_pred)\n",
    "    return all_true, all_pred\n",
    "\n",
    "\n",
    "\n",
    "def test_synthetic_gaps_imputation_efficiency_uniautoreg(df, data_scaled, gap_indices, forecast_func, model,\n",
    "                                                         pre_context_length, gap_length, scaler, method_name):\n",
    "    all_true_gaps = []\n",
    "    all_pred_gaps = []\n",
    "    for gap_idx in gap_indices:\n",
    "        if gap_idx - pre_context_length < 0 or gap_idx + gap_length > len(data_scaled):\n",
    "            continue\n",
    "        initial_context = data_scaled[gap_idx - pre_context_length:gap_idx].reshape(-1, 1)\n",
    "        forecast_scaled = forecast_func(model, initial_context, gap_length)\n",
    "        forecast = scaler.inverse_transform(forecast_scaled)\n",
    "        true_gap = df.loc[gap_idx:gap_idx + gap_length - 1, \"pm2_5_original\"].values.reshape(-1, 1)\n",
    "        if np.any(np.isnan(forecast)) or np.any(np.isnan(true_gap)):\n",
    "            continue\n",
    "        all_pred_gaps.append(forecast)\n",
    "        all_true_gaps.append(true_gap)\n",
    "    if len(all_true_gaps) == 0 or len(all_pred_gaps) == 0:\n",
    "        raise ValueError(\"No processed gaps to calculate metrics.\")\n",
    "    all_true = np.concatenate(all_true_gaps)\n",
    "    all_pred = np.concatenate(all_pred_gaps)\n",
    "    return all_true, all_pred\n",
    "\n",
    "def test_synthetic_gaps_imputation_efficiency_uniautoreg_multi(df, data_scaled, gap_indices, forecast_func, model,\n",
    "                                                               pre_context_length, gap_length, scaler, method_name, feature_cols):\n",
    "    \"\"\"\n",
    "    Function to calculate metrics for unidirectional autoregression (UniAR) for multivariate data.\n",
    "    \n",
    "    If feature_cols are passed, only these columns are selected from data_scaled.\n",
    "    During inverse scaling, a manual transformation is performed only for the target feature\n",
    "    (it is assumed that it corresponds to feature_cols[0] and was trained in the scaler as the first feature).\n",
    "    \n",
    "    Parameters:\n",
    "      - df: original DataFrame.\n",
    "      - data_scaled: scaled data (2D array, shape=(n_samples, total_features)).\n",
    "      - gap_indices: indices for synthetic gaps.\n",
    "      - forecast_func: forecasting function (e.g., direct_uniautoreg_forecast_xgb_multi).\n",
    "      - model: trained model.\n",
    "      - pre_context_length: window length.\n",
    "      - gap_length: number of forecasting steps.\n",
    "      - scaler: StandardScaler object, trained on all features.\n",
    "      - method_name: method name (for debugging).\n",
    "      - feature_cols: list of feature indices used during training.\n",
    "    \n",
    "    Returns:\n",
    "      - all_true, all_pred: concatenated arrays of true and predicted values.\n",
    "    \"\"\"\n",
    "    all_true_gaps = []\n",
    "    all_pred_gaps = []\n",
    "    for gap_idx in gap_indices:\n",
    "        if gap_idx - pre_context_length < 0 or gap_idx + gap_length > len(data_scaled):\n",
    "            continue\n",
    "        # Extract the window from data_scaled and select only the necessary features\n",
    "        window = data_scaled[gap_idx - pre_context_length:gap_idx]  # shape: (pre_context_length, total_features)\n",
    "        window = window[:, feature_cols]  # shape: (pre_context_length, n_selected)\n",
    "        \n",
    "        forecast_scaled = forecast_func(model, window, gap_length)  # (gap_length, 1)\n",
    "        \n",
    "        # Manual inverse scaling for the target feature:\n",
    "        forecast = forecast_scaled * float(scaler.scale_[0]) + float(scaler.mean_[0])\n",
    "        forecast = forecast.reshape(-1, 1)\n",
    "        \n",
    "        true_gap = df.loc[gap_idx:gap_idx + gap_length - 1, \"pm2_5_original\"].values.reshape(-1, 1)\n",
    "        if np.any(np.isnan(forecast)) or np.any(np.isnan(true_gap)):\n",
    "            continue\n",
    "        all_pred_gaps.append(forecast)\n",
    "        all_true_gaps.append(true_gap)\n",
    "    if len(all_true_gaps) == 0 or len(all_pred_gaps) == 0:\n",
    "        raise ValueError(\"No processed gaps to calculate metrics.\")\n",
    "    all_true = np.concatenate(all_true_gaps)\n",
    "    all_pred = np.concatenate(all_pred_gaps)\n",
    "    return all_true, all_pred\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Main program block\n",
    "class ImputationCombiner:\n",
    "    def __init__(self, df, data_scaled, scaler, pre_context_length=32, post_context_length=32):\n",
    "        \"\"\"\n",
    "        Initializes the combiner for testing models and imputation methods.\n",
    "        \n",
    "        Parameters:\n",
    "        - df: original DataFrame with data\n",
    "        - data_scaled: scaled data\n",
    "        - scaler: StandardScaler object for inverse transformation\n",
    "        - pre_context_length: length of the context before the gap (default 32)\n",
    "        - post_context_length: length of the context after the gap (default 32)\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        print(f\"Initializing ImputationCombiner with data_scaled shape: {data_scaled.shape if hasattr(data_scaled, 'shape') else 'No shape, type: ' + str(type(data_scaled))}\")\n",
    "        print(f\"Type of data_scaled: {type(data_scaled)}\")\n",
    "        self.data_scaled = data_scaled\n",
    "        self.scaler = scaler\n",
    "        self.pre_context_length = pre_context_length\n",
    "        self.post_context_length = post_context_length\n",
    "        self.models = {}  # Dictionary to store methods and their parameters\n",
    "        self.results = {}  # Dictionary to store metrics\n",
    "        self.predictions = {}  # Dictionary to store true and predicted values\n",
    "\n",
    "    def add_model(self, name, model_func=None, forecast_func=None, forecast_type=\"seq2seq\", window_size=None, poly_degree=None, feature_cols=[0]):\n",
    "        self.models[name] = {\n",
    "            \"func\": model_func,\n",
    "            \"forecast\": forecast_func,\n",
    "            \"forecast_type\": forecast_type,\n",
    "            \"window_size\": window_size,\n",
    "            \"poly_degree\": poly_degree,\n",
    "            \"feature_cols\": feature_cols,\n",
    "            \"model_info\": {\"feature_cols\": feature_cols}  # Save feature information\n",
    "        }\n",
    "\n",
    "    def run_tests(self, gap_lengths=[3, 5, 9, 17], n_runs=5, missing_fraction=0.05):\n",
    "        self.n_runs = n_runs\n",
    "        for gap_length in gap_lengths:\n",
    "            self.results[gap_length] = {}\n",
    "            self.predictions[gap_length] = {}\n",
    "            gaps_per_run = {}\n",
    "            for run in range(n_runs):\n",
    "                df_missing, gap_indices = introduce_synthetic_gaps(\n",
    "                    self.df, \"pm2_5\", missing_fraction, gap_length, random_state=run\n",
    "                )\n",
    "                gaps_per_run[run] = (df_missing, gap_indices)\n",
    "\n",
    "            for model_name, model_info in self.models.items():\n",
    "                print(f\"Testing {model_name} on gap length {gap_length}...\")\n",
    "                start_time = time.time()\n",
    "                metrics_runs = []\n",
    "                all_true_runs = []\n",
    "                all_pred_runs = []\n",
    "                run_times = []  # List to store the time of each run\n",
    "\n",
    "                for run in range(n_runs):\n",
    "                    run_start_time = time.time()  # Start of the run\n",
    "                    print(f\"  Run {run+1}/{n_runs} started at {time.time() - start_time:.2f} seconds\")\n",
    "                    df_missing, gap_indices = gaps_per_run[run]\n",
    "\n",
    "                    if model_info[\"forecast_type\"] in [\"seq2seq\", \"autoreg\"]:\n",
    "                        if model_info[\"forecast_type\"] == \"seq2seq\":\n",
    "                            # For seq2seq - form multi-output data:\n",
    "                            if \"feature_cols\" in model_info and isinstance(self.data_scaled, np.ndarray) and self.data_scaled.ndim == 2 and len(model_info[\"feature_cols\"]) > 1:\n",
    "                                # Multivariate variant\n",
    "                                X_forward, y_forward = create_forward_data_seq2seq_multi(\n",
    "                                    self.data_scaled, self.pre_context_length, gap_length, model_info[\"feature_cols\"]\n",
    "                                )\n",
    "                                X_backward, y_backward = create_backward_data_seq2seq_multi(\n",
    "                                    self.data_scaled, self.post_context_length, gap_length, model_info[\"feature_cols\"]\n",
    "                                )\n",
    "                            else:\n",
    "                                # Univariate variant\n",
    "                                X_forward, y_forward = create_forward_data_seq2seq(\n",
    "                                    self.data_scaled, self.pre_context_length, gap_length\n",
    "                                )\n",
    "                                X_backward, y_backward = create_backward_data_seq2seq(\n",
    "                                    self.data_scaled, self.post_context_length, gap_length\n",
    "                                )\n",
    "                        elif model_info[\"forecast_type\"] == \"autoreg\":\n",
    "                            if (\n",
    "                                \"feature_cols\" in model_info\n",
    "                                and isinstance(self.data_scaled, np.ndarray)\n",
    "                                and self.data_scaled.ndim == 2\n",
    "                                and len(model_info[\"feature_cols\"]) > 1\n",
    "                            ):\n",
    "                                # Multivariate variant\n",
    "                                X_forward, y_forward = create_forward_data_autoreg_multi(\n",
    "                                    self.data_scaled, self.pre_context_length, model_info[\"feature_cols\"]\n",
    "                                )\n",
    "                                # If you have a separate function for backward, call it too:\n",
    "                                # (often there's not much point for a 1D backward, but it can be implemented if needed)\n",
    "                                X_backward, y_backward = create_backward_data_autoreg_multi(\n",
    "                                    self.data_scaled, self.post_context_length, model_info[\"feature_cols\"]\n",
    "                                )\n",
    "                            else:\n",
    "                                # Univariate (standard) variant\n",
    "                                X_forward, y_forward = create_forward_data_autoreg(\n",
    "                                    self.data_scaled, self.pre_context_length\n",
    "                                )\n",
    "                                X_backward, y_backward = create_backward_data_autoreg(\n",
    "                                    self.data_scaled, self.post_context_length\n",
    "                                )\n",
    "                        else:\n",
    "                            raise ValueError(\"Unsupported forecast_type. Use 'seq2seq' or 'autoreg'.\")\n",
    "\n",
    "                        # Split data into train/test by time\n",
    "                        X_f_train, X_f_test, y_f_train, y_f_test = time_based_split(X_forward, y_forward, train_size_ratio=0.8)\n",
    "                        X_b_train, X_b_test, y_b_train, y_b_test = time_based_split(X_backward, y_backward, train_size_ratio=0.8)\n",
    "                        print(f\"  Data prepared at {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "                        # Train the model (the function must match the selected forecast type)\n",
    "                        forward_model, backward_model = model_info[\"func\"](\n",
    "                            X_f_train, y_f_train, X_f_test, y_f_test,\n",
    "                            X_b_train, y_b_train, X_b_test, y_b_test,\n",
    "                            self.pre_context_length, self.post_context_length, gap_length\n",
    "                        )\n",
    "                        print(f\"  Models trained at {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "                        all_true, all_pred = test_synthetic_gaps_imputation_efficiency(\n",
    "                            self.df, self.data_scaled, gap_indices, model_info[\"forecast\"],\n",
    "                            forward_model, backward_model, self.pre_context_length, gap_length,\n",
    "                            self.post_context_length, self.scaler, model_name, model_info\n",
    "                        )\n",
    "                        print(f\"  Prediction completed at {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "                    elif model_info[\"forecast_type\"] == \"uniseq2seq\":\n",
    "                        # Get the list of features (feature_cols)\n",
    "                        feature_cols = model_info[\"feature_cols\"]\n",
    "                        if not isinstance(feature_cols, list):\n",
    "                            raise TypeError(f\"feature_cols must be a list, got {type(feature_cols)}\")\n",
    "\n",
    "                        # Bridging approach: form left context (X_left) and right context (X_right).\n",
    "                        # Split into univariate / multivariate depending on feature_cols.\n",
    "                        if len(feature_cols) == 1 and feature_cols[0] == 0:\n",
    "                            # Univariate bridging\n",
    "                            # Check the shape of self.data_scaled\n",
    "                            if len(self.data_scaled.shape) == 1:\n",
    "                                # If data_scaled is a 1D array\n",
    "                                X_left, X_right, y = create_uniseq2seq_data(\n",
    "                                    self.data_scaled,\n",
    "                                    self.pre_context_length,\n",
    "                                    gap_length,\n",
    "                                    self.post_context_length\n",
    "                                )\n",
    "                            elif len(self.data_scaled.shape) == 2:\n",
    "                                # data_scaled.shape[1] == 1, i.e., [:,0]\n",
    "                                X_left, X_right, y = create_uniseq2seq_data(\n",
    "                                    self.data_scaled[:, 0],\n",
    "                                    self.pre_context_length,\n",
    "                                    gap_length,\n",
    "                                    self.post_context_length\n",
    "                                )\n",
    "                            else:\n",
    "                                raise ValueError(\n",
    "                                    f\"Unexpected shape for data_scaled in univariate mode: {self.data_scaled.shape}\"\n",
    "                                )\n",
    "                        else:\n",
    "                            # Multivariate bridging\n",
    "                            if not isinstance(self.data_scaled, np.ndarray):\n",
    "                                raise TypeError(f\"data_scaled must be a numpy array for multivariate models, got {type(self.data_scaled)}\")\n",
    "                            if len(self.data_scaled.shape) != 2:\n",
    "                                raise ValueError(\n",
    "                                    f\"Multivariate models require a 2D data_scaled array, got shape {self.data_scaled.shape}\"\n",
    "                                )\n",
    "\n",
    "                            # Debugging information\n",
    "                            print(f\"Debug: self.data_scaled shape before create_uniseq2seq_data_multi: {self.data_scaled.shape}\")\n",
    "                            print(f\"Debug: feature_cols: {feature_cols}\")\n",
    "\n",
    "                            X_left, X_right, y = create_uniseq2seq_data_multi(\n",
    "                                self.data_scaled,\n",
    "                                self.pre_context_length,\n",
    "                                gap_length,\n",
    "                                self.post_context_length,\n",
    "                                feature_cols\n",
    "                            )\n",
    "                        def time_based_split_3(X1, X2, y, train_size_ratio=0.8):\n",
    "                            n = len(X1)\n",
    "                            train_size = int(n * train_size_ratio)\n",
    "                            return (\n",
    "                                X1[:train_size], X1[train_size:], \n",
    "                                X2[:train_size], X2[train_size:], \n",
    "                                y[:train_size],  y[train_size:]\n",
    "                            )\n",
    "                        # Split into training and test sets (bridging: 2 inputs -> 6 outputs)\n",
    "                        X_left_train, X_left_test, X_right_train, X_right_test, y_train, y_test = time_based_split_3(X_left, X_right, y, train_size_ratio=0.8)\n",
    "                        print(f\"  Data prepared at {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "                        # Model training (two-input call)\n",
    "                        forward_model, _ = model_info[\"func\"](\n",
    "                            X_left_train, y_train, X_left_test, y_test,\n",
    "                            X_right_train, y_train, X_right_test, y_test,\n",
    "                            self.pre_context_length, self.post_context_length, gap_length,\n",
    "                            model_info\n",
    "                        )\n",
    "                        print(f\"  Model trained at {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "                        # Prediction and evaluation - general function\n",
    "                        all_true, all_pred = test_synthetic_gaps_imputation_efficiency_uniseq2seq(\n",
    "                            self.df, self.data_scaled, gap_indices, model_info[\"forecast\"],\n",
    "                            forward_model, self.pre_context_length, gap_length,\n",
    "                            self.post_context_length, self.scaler, model_name\n",
    "                        )\n",
    "                        print(f\"  Prediction completed at {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    \n",
    "                    elif model_info[\"forecast_type\"] == \"uniautoreg\":\n",
    "                        # For multivariate mode, use a special function to form autoregressive data\n",
    "                        if \"feature_cols\" in model_info and isinstance(self.data_scaled, np.ndarray) and self.data_scaled.ndim == 2 and len(model_info[\"feature_cols\"]) > 1:\n",
    "                            X_forward, y_forward = create_forward_data_autoreg_multi(self.data_scaled, self.pre_context_length, model_info[\"feature_cols\"])\n",
    "                        else:\n",
    "                            X_forward, y_forward = create_forward_data_autoreg(self.data_scaled, self.pre_context_length)\n",
    "                        X_f_train, X_f_test, y_f_train, y_f_test = time_based_split(X_forward, y_forward, train_size_ratio=0.8)\n",
    "\n",
    "                        print(f\"  Data prepared at {time.time() - start_time:.2f} seconds\")\n",
    "                        forward_model, _ = model_info[\"func\"](\n",
    "                            X_f_train, y_f_train, X_f_test, y_f_test,\n",
    "                            None, None, None, None,\n",
    "                            self.pre_context_length, self.post_context_length, gap_length\n",
    "                        )\n",
    "                        print(f\"  Model trained at {time.time() - start_time:.2f} seconds\")\n",
    "                        if \"feature_cols\" in model_info and isinstance(self.data_scaled, np.ndarray) and self.data_scaled.ndim == 2 and len(model_info[\"feature_cols\"]) > 1:\n",
    "                            all_true, all_pred = test_synthetic_gaps_imputation_efficiency_uniautoreg_multi(\n",
    "                                self.df, self.data_scaled, gap_indices, model_info[\"forecast\"],\n",
    "                                forward_model, self.pre_context_length, gap_length, self.scaler, model_name,\n",
    "                                model_info[\"feature_cols\"]\n",
    "                            )\n",
    "                        else:\n",
    "                            all_true, all_pred = test_synthetic_gaps_imputation_efficiency_uniautoreg(\n",
    "                                self.df, self.data_scaled, gap_indices, model_info[\"forecast\"],\n",
    "                                forward_model, self.pre_context_length, gap_length, self.scaler, model_name\n",
    "                            )\n",
    "                        print(f\"  Prediction completed at {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "                    elif model_info[\"forecast_type\"] in [\"simple\", \"local\", \"window\"]:\n",
    "                        window_size = model_info[\"window_size\"] or (15 + gap_length + 15)\n",
    "                        all_true = []\n",
    "                        all_pred = []\n",
    "                        for gap_idx in gap_indices:\n",
    "                            true_gap = df_missing.loc[gap_idx:gap_idx + gap_length - 1, \"pm2_5_original\"].values.reshape(-1, 1)\n",
    "                            # Use window_size from model_info\n",
    "                            half_window = window_size // 2  # Half of the window before and after the gap\n",
    "                            start_idx = max(0, gap_idx - half_window)\n",
    "                            end_idx = min(len(df_missing), gap_idx + gap_length + half_window)\n",
    "                            if model_info[\"forecast_type\"] == \"simple\":\n",
    "                                if model_name == \"Simple Imputer Mean\":\n",
    "                                    imputer = SimpleImputer(strategy=\"mean\")\n",
    "                                elif model_name == \"Simple Imputer Median\":\n",
    "                                    imputer = SimpleImputer(strategy=\"median\")\n",
    "                                data_with_gaps = df_missing[\"pm2_5\"].values.reshape(-1, 1)\n",
    "                                pred_gap = imputer.fit_transform(data_with_gaps)[gap_idx:gap_idx + gap_length].reshape(-1, 1)\n",
    "                            elif model_info[\"forecast_type\"] == \"local\":\n",
    "                                if model_name in [\"Local Imputation Mean\", \"Local Imputation Median\"]:\n",
    "                                    window_data = df_missing.loc[start_idx:end_idx, \"pm2_5\"].dropna()\n",
    "                                    pred_value = np.mean(window_data) if model_name == \"Local Imputation Mean\" else np.median(window_data)\n",
    "                                    pred_gap = np.full(gap_length, pred_value if not window_data.empty else np.nan).reshape(-1, 1)\n",
    "                            elif model_info[\"forecast_type\"] == \"window\":\n",
    "                                window_data = df_missing.loc[start_idx:end_idx, \"pm2_5\"]\n",
    "                                print(f\"Window NaN count: {window_data.isna().sum()}\")\n",
    "                                if model_name == \"ARIMA Imputation\":\n",
    "                                    pre_gap_data = window_data[:gap_idx - start_idx].dropna()\n",
    "                                    print(f\"ARIMA: gap_idx={gap_idx}, pre_gap_data length={len(pre_gap_data)}\")\n",
    "                                    if len(pre_gap_data) >= 10:  # Minimum length for fitting\n",
    "                                        try:\n",
    "                                            model = ARIMA(pre_gap_data.to_numpy(), \n",
    "                                                        order=(1, 0, 0),  # Best baseline\n",
    "                                                        enforce_stationarity=True, enforce_invertibility=True)\n",
    "                                            fitted_model = model.fit()\n",
    "                                            pred_gap = fitted_model.forecast(steps=gap_length)\n",
    "                                            pred_gap = pred_gap.reshape(-1, 1)\n",
    "                                            print(f\"ARIMA: Prediction successful, pred_gap={pred_gap[:5].ravel()}\")\n",
    "                                        except Exception as e:\n",
    "                                            print(f\"ARIMA: Error during fit - {str(e)}, falling back to linear interpolation\")\n",
    "                                            if len(pre_gap_data) >= 2:\n",
    "                                                x = np.arange(len(pre_gap_data))\n",
    "                                                interp_func = interpolate.interp1d(x, pre_gap_data.to_numpy(), kind='linear', fill_value=\"extrapolate\")\n",
    "                                                pred_x = np.arange(len(pre_gap_data), len(pre_gap_data) + gap_length)\n",
    "                                                pred_gap = interp_func(pred_x).reshape(-1, 1)\n",
    "                                            else:\n",
    "                                                pred_gap = np.full(gap_length, np.nan).reshape(-1, 1)\n",
    "                                    else:\n",
    "                                        print(f\"ARIMA: Not enough data for fit (len={len(pre_gap_data)})\")\n",
    "                                        pred_gap = np.full(gap_length, np.nan).reshape(-1, 1)\n",
    "                                elif model_name == \"Linear Window Interpolation\" and len(window_data.dropna()) >= 2:\n",
    "                                    x = np.arange(len(window_data))\n",
    "                                    gap_x = np.arange(gap_idx - start_idx, gap_idx - start_idx + gap_length)\n",
    "                                    y = window_data.values\n",
    "                                    mask = ~np.isnan(y)\n",
    "                                    interp_func = interpolate.interp1d(x[mask], y[mask], kind='linear', bounds_error=False, fill_value=np.nan)\n",
    "                                    pred_gap = interp_func(gap_x).reshape(-1, 1)\n",
    "                                elif model_name == \"B-spline Window Interpolation\" and len(window_data.dropna()) >= 3:\n",
    "                                    window_data = window_data.dropna()\n",
    "                                    x = np.arange(len(window_data))\n",
    "                                    y = window_data.values\n",
    "                                    gap_x = np.arange(gap_idx - start_idx, gap_idx - start_idx + gap_length)\n",
    "                                    spline = interpolate.UnivariateSpline(x, y, k=2, ext=0)\n",
    "                                    pred_gap = spline(gap_x).reshape(-1, 1)\n",
    "                                elif model_name == \"Polynomial Window Interpolation\" and len(window_data.dropna()) >= (model_info[\"poly_degree\"] or 3) + 1:\n",
    "                                    window_data = window_data.dropna()\n",
    "                                    poly_degree = model_info[\"poly_degree\"] or 3\n",
    "                                    x = np.arange(len(window_data))\n",
    "                                    y = window_data.values\n",
    "                                    coeffs = np.polyfit(x, y, poly_degree)\n",
    "                                    poly_func = np.poly1d(coeffs)\n",
    "                                    gap_x = np.arange(gap_idx - start_idx, gap_idx - start_idx + gap_length)\n",
    "                                    pred_gap = poly_func(gap_x).reshape(-1, 1)\n",
    "                                else:\n",
    "                                    pred_gap = np.full(gap_length, np.nan).reshape(-1, 1)\n",
    "                            if not np.any(np.isnan(true_gap)) and not np.any(np.isnan(pred_gap)):\n",
    "                                all_true.append(true_gap.ravel())\n",
    "                                all_pred.append(pred_gap.ravel())\n",
    "                            else:\n",
    "                                print(f\"Skipping gap_idx={gap_idx} due to NaN in true_gap or pred_gap\")\n",
    "                        if not all_true or not all_pred:\n",
    "                            raise ValueError(\"No processed gaps to calculate metrics.\")\n",
    "                        all_true = np.concatenate(all_true)\n",
    "                        all_pred = np.concatenate(all_pred)\n",
    "\n",
    "                    else:\n",
    "                        raise ValueError(\"Unsupported forecast_type. Use 'seq2seq', 'autoreg', 'uniseq2seq', 'uniautoreg', 'simple', 'local', or 'window'.\")\n",
    "\n",
    "                    metrics = evaluate_model(all_true, all_pred)\n",
    "                    metrics_runs.append(metrics)\n",
    "                    all_true_runs.append(all_true.ravel())\n",
    "                    all_pred_runs.append(all_pred.ravel())\n",
    "                    run_time = time.time() - run_start_time  # Run execution time\n",
    "                    run_times.append(run_time)\n",
    "                    print(f\"  Run {run+1}/{n_runs} completed in {run_time:.2f} seconds\")\n",
    "\n",
    "                total_time = time.time() - start_time  # Total time for the model\n",
    "                metrics_array = np.array(metrics_runs)\n",
    "                self.results[gap_length][model_name] = {\n",
    "                    \"MAE\": {\"mean\": np.mean(metrics_array[:, 0]), \"std\": np.std(metrics_array[:, 0])},\n",
    "                    \"RMSE\": {\"mean\": np.mean(metrics_array[:, 1]), \"std\": np.std(metrics_array[:, 1])},\n",
    "                    \"R2\": {\"mean\": np.mean(metrics_array[:, 2]), \"std\": np.std(metrics_array[:, 2])},\n",
    "                    \"MAPE\": {\"mean\": np.mean(metrics_array[:, 3]), \"std\": np.std(metrics_array[:, 3])},\n",
    "                    \"run_times\": run_times,  # Time for each run\n",
    "                    \"total_time\": total_time  # Total time\n",
    "                }\n",
    "                self.predictions[gap_length][model_name] = {\n",
    "                    \"all_true\": np.concatenate(all_true_runs),\n",
    "                    \"all_pred\": np.concatenate(all_pred_runs)\n",
    "                }\n",
    "                print(f\"Finished {model_name} on gap length {gap_length} in {total_time:.2f} seconds\")\n",
    "                time.sleep(0.1)\n",
    "                K.clear_session()\n",
    "                gc.collect()\n",
    "                time.sleep(0.1)\n",
    "    \n",
    "\n",
    "    # Visualization functions\n",
    "    def plot_violin_errors(self, output_dir=\"plots\"):\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        error_data = []\n",
    "        model_names = []\n",
    "        for model_name in self.models.keys():\n",
    "            all_true_agg = np.concatenate([self.predictions[gl][model_name][\"all_true\"] for gl in self.results.keys()])\n",
    "            all_pred_agg = np.concatenate([self.predictions[gl][model_name][\"all_pred\"] for gl in self.results.keys()])\n",
    "            errors = all_true_agg - all_pred_agg\n",
    "            error_data.append(errors)\n",
    "            model_names.append(model_name)\n",
    "        \n",
    "        plt.violinplot(error_data, showmedians=True)\n",
    "        plt.xticks(range(1, len(model_names) + 1), model_names, rotation=45)\n",
    "        plt.xlabel(\"Model\")\n",
    "        plt.ylabel(\"Prediction Error (True - Predicted)\")\n",
    "        plt.title(\"Distribution of Prediction Errors Across Models\")\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(output_dir, \"violin_plot_errors.png\"), dpi=600)\n",
    "        plt.close()\n",
    "\n",
    "    def plot_scatter_by_model(self, gap_length, model_name, output_dir=\"plots\"):\n",
    "        all_true = self.predictions[gap_length][model_name][\"all_true\"]\n",
    "        all_pred = self.predictions[gap_length][model_name][\"all_pred\"]\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.scatter(all_true, all_pred, alpha=0.5, label=f\"{model_name} (gap={gap_length})\")\n",
    "        plt.plot([min(all_true), max(all_true)], [min(all_true), max(all_true)], 'r--', lw=2)\n",
    "        plt.xlabel(\"True Values\")\n",
    "        plt.ylabel(\"Predicted Values\")\n",
    "        plt.title(f\"Scatter Plot: {model_name} (Gap Length: {gap_length})\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.savefig(os.path.join(output_dir, f\"scatter_{model_name}_gap_{gap_length}.png\"), dpi=600)\n",
    "        plt.close()\n",
    "\n",
    "    def plot_scatter_aggregated(self, model_name, output_dir=\"plots\"):\n",
    "        all_true_agg = np.concatenate([self.predictions[gl][model_name][\"all_true\"] for gl in self.results.keys()])\n",
    "        all_pred_agg = np.concatenate([self.predictions[gl][model_name][\"all_pred\"] for gl in self.results.keys()])\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.scatter(all_true_agg, all_pred_agg, alpha=0.5, label=f\"{model_name} (All Gaps)\")\n",
    "        plt.plot([min(all_true_agg), max(all_true_agg)], [min(all_true_agg), max(all_true_agg)], 'r--', lw=2)\n",
    "        plt.xlabel(\"True Values\")\n",
    "        plt.ylabel(\"Predicted Values\")\n",
    "        plt.title(f\"Aggregated Scatter Plot: {model_name}\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.savefig(os.path.join(output_dir, f\"scatter_{model_name}_aggregated.png\"), dpi=600)\n",
    "        plt.close()\n",
    "\n",
    "    def plot_time_series(self, gap_length, model_name, idx=0, output_dir=\"plots\"):\n",
    "        true_vals = self.predictions[gap_length][model_name][\"all_true\"]\n",
    "        pred_vals = self.predictions[gap_length][model_name][\"all_pred\"]\n",
    "        gap_size = gap_length * self.n_runs\n",
    "        start_idx = idx * gap_length\n",
    "        true_segment = true_vals[start_idx:start_idx + gap_length]\n",
    "        pred_segment = pred_vals[start_idx:start_idx + gap_length]\n",
    "        \n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(range(gap_length), true_segment, label=\"True\", marker=\"o\")\n",
    "        plt.plot(range(gap_length), pred_segment, label=\"Predicted\", marker=\"x\")\n",
    "        plt.xlabel(\"Time Step\")\n",
    "        plt.ylabel(\"pm2_5\")\n",
    "        plt.title(f\"{model_name} - Time Series Example (Gap Length: {gap_length})\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.savefig(os.path.join(output_dir, f\"time_series_{model_name}_gap_{gap_length}_idx_{idx}.png\"), dpi=600)\n",
    "        plt.close()\n",
    "\n",
    "    def plot_metrics_heatmap(self, output_dir=\"plots\"):\n",
    "        metrics = [\"MAE\", \"RMSE\", \"R2\", \"MAPE\"]\n",
    "        for metric in metrics:\n",
    "            data = {gl: {m: self.results[gl][m][metric][\"mean\"] for m in self.results[gl]} \n",
    "                    for gl in self.results}\n",
    "            df = pd.DataFrame(data).T\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            sns.heatmap(df, annot=True, cmap=\"YlGnBu\", fmt=\".3f\")\n",
    "            plt.title(f\"{metric} Heatmap Across Models and Gap Lengths\")\n",
    "            plt.xlabel(\"Model\")\n",
    "            plt.ylabel(\"Gap Length\")\n",
    "            plt.savefig(os.path.join(output_dir, f\"heatmap_{metric}.png\"), dpi=600)\n",
    "            plt.close()\n",
    "\n",
    "    def plot_metric_trend(self, metric=\"MAE\", output_dir=\"plots\"):\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        for model_name in self.models.keys():\n",
    "            means = [self.results[gl][model_name][metric][\"mean\"] for gl in sorted(self.results.keys())]\n",
    "            plt.plot(sorted(self.results.keys()), means, label=model_name, marker=\"o\")\n",
    "        plt.xlabel(\"Gap Length\")\n",
    "        plt.ylabel(f\"{metric}\")\n",
    "        plt.title(f\"{metric} Trend Across Gap Lengths\")\n",
    "        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(output_dir, f\"trend_{metric}.png\"), dpi=600)\n",
    "        plt.close()\n",
    "\n",
    "    def plot_grouped_boxplot(self, output_dir=\"plots\"):\n",
    "        groups = {}\n",
    "        for model_name, model_info in self.models.items():\n",
    "            group = model_info[\"forecast_type\"]\n",
    "            if group not in groups:\n",
    "                groups[group] = []\n",
    "            for gl in self.results:\n",
    "                true = self.predictions[gl][model_name][\"all_true\"]\n",
    "                pred = self.predictions[gl][model_name][\"all_pred\"]\n",
    "                errors = true - pred\n",
    "                groups[group].extend(errors)\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.boxplot([errors for errors in groups.values()], labels=groups.keys())\n",
    "        plt.xlabel(\"Method Group\")\n",
    "        plt.ylabel(\"Prediction Error\")\n",
    "        plt.title(\"Error Distribution by Method Group\")\n",
    "        plt.grid(True)\n",
    "        plt.savefig(os.path.join(output_dir, \"boxplot_by_group.png\"), dpi=600)\n",
    "        plt.close()\n",
    "    \n",
    "    \n",
    "        \n",
    "    def summarize_results(self, visualize=False, output_dir=\"plots\"):\n",
    "        \"\"\"\n",
    "        Outputs the test results, creates visualizations (if visualize=True)\n",
    "        and saves aggregated results for later analysis.\n",
    "        \n",
    "        Parameters:\n",
    "        - visualize: flag to enable/disable visualization (default False)\n",
    "        - output_dir: directory for saving graphs and results (default \"plots\")\n",
    "        \"\"\"\n",
    "        summary_list = []\n",
    "        for gap_length, models in self.results.items():\n",
    "            print(f\"\\nResults for gap length {gap_length} (n_runs={self.n_runs}):\")\n",
    "            table_data = []\n",
    "            for model_name, metrics in models.items():\n",
    "                avg_run_time = np.mean(metrics[\"run_times\"])  # Average time per run\n",
    "                total_time = metrics[\"total_time\"]\n",
    "                row = [\n",
    "                    model_name,\n",
    "                    f\"{metrics['MAE']['mean']:.3f} Â± {metrics['MAE']['std']:.3f}\",\n",
    "                    f\"{metrics['RMSE']['mean']:.3f} Â± {metrics['RMSE']['std']:.3f}\",\n",
    "                    f\"{metrics['R2']['mean']:.3f} Â± {metrics['R2']['std']:.3f}\",\n",
    "                    f\"{metrics['MAPE']['mean']:.3f} Â± {metrics['MAPE']['std']:.3f}\",\n",
    "                    f\"{avg_run_time:.2f} | {total_time:.2f}\"\n",
    "                ]\n",
    "                table_data.append(row)\n",
    "                # Add a row to the aggregated list of results with model parameters\n",
    "                summary_list.append({\n",
    "                    \"Gap Length\": gap_length,\n",
    "                    \"Model\": model_name,\n",
    "                    \"Forecast Type\": self.models[model_name][\"forecast_type\"] if model_name in self.models else None,\n",
    "                    \"n_runs\": self.n_runs,\n",
    "                    \"MAE Mean\": metrics[\"MAE\"][\"mean\"],\n",
    "                    \"MAE Std\": metrics[\"MAE\"][\"std\"],\n",
    "                    \"RMSE Mean\": metrics[\"RMSE\"][\"mean\"],\n",
    "                    \"RMSE Std\": metrics[\"RMSE\"][\"std\"],\n",
    "                    \"R2 Mean\": metrics[\"R2\"][\"mean\"],\n",
    "                    \"R2 Std\": metrics[\"R2\"][\"std\"],\n",
    "                    \"MAPE Mean\": metrics[\"MAPE\"][\"mean\"],\n",
    "                    \"MAPE Std\": metrics[\"MAPE\"][\"std\"],\n",
    "                    \"Average Run Time\": avg_run_time,\n",
    "                    \"Total Time\": total_time,\n",
    "                    \"Epochs\": EPOCHS,\n",
    "                    \"Batch Size\": BATCH_SIZE,\n",
    "                    \"Patience\": PATIENCE,\n",
    "                    \"Estimators\": N_ESTIMATORS\n",
    "                })\n",
    "            headers = [\"Model\", \"MAE (meanÂ±std)\", \"RMSE (meanÂ±std)\", \"R2 (meanÂ±std)\", \"MAPE (meanÂ±std)\", \"Time (avg run | total, s)\"]\n",
    "            print(tabulate(table_data, headers=headers, tablefmt=\"grid\", floatfmt=\".3f\"))\n",
    "        \n",
    "        # Saving aggregated results to a DataFrame and writing to a CSV file\n",
    "        summary_df = pd.DataFrame(summary_list)\n",
    "        self.results_summary = summary_df  # save in an attribute for later use\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "        summary_file = os.path.join(output_dir, \"results_summary.csv\")\n",
    "        summary_df.to_csv(summary_file, index=False)\n",
    "        print(f\"\\nAggregated results saved to {summary_file}\")\n",
    "        \n",
    "        if visualize:\n",
    "            self.plot_violin_errors(output_dir)\n",
    "            for gap_length in self.results.keys():\n",
    "                for model_name in self.models.keys():\n",
    "                    self.plot_scatter_by_model(gap_length, model_name, output_dir)\n",
    "            for model_name in self.models.keys():\n",
    "                self.plot_scatter_aggregated(model_name, output_dir)\n",
    "            self.plot_metrics_heatmap(output_dir)\n",
    "            self.plot_metric_trend(metric=\"MAE\", output_dir=output_dir)\n",
    "            self.plot_metric_trend(metric=\"RMSE\", output_dir=output_dir)\n",
    "            self.plot_grouped_boxplot(output_dir)\n",
    "            for gap_length in self.results.keys():\n",
    "                for model_name in self.models.keys():\n",
    "                    self.plot_time_series(gap_length, model_name, idx=0, output_dir=output_dir)\n",
    "                \n",
    "# Functions for creating and training models\n",
    "\n",
    "# Functions for creating and training Seq2Seq models with a bidirectional approach\n",
    "def create_and_train_tcn_seq2seq(X_f_train, y_f_train, X_f_test, y_f_test, X_b_train, y_b_train, X_b_test, y_b_test, \n",
    "                                 pre_context_length, post_context_length, gap_length):\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=PATIENCE, restore_best_weights=True)\n",
    "    forward_model = Sequential([\n",
    "        TCN(nb_filters=32, kernel_size=3, dilations=[1, 2, 4], padding='causal', \n",
    "            use_skip_connections=True, dropout_rate=0.2, return_sequences=False, \n",
    "            input_shape=(pre_context_length, 1)),\n",
    "        Dense(gap_length)\n",
    "    ])\n",
    "    forward_model.compile(optimizer='adam', loss='mse')\n",
    "    forward_model.fit(X_f_train, y_f_train.reshape(-1, gap_length), epochs=EPOCHS, batch_size=BATCH_SIZE, \n",
    "                      validation_data=(X_f_test, y_f_test.reshape(-1, gap_length)), callbacks=[early_stopping], verbose=0)\n",
    "    \n",
    "    backward_model = Sequential([\n",
    "        TCN(nb_filters=32, kernel_size=3, dilations=[1, 2, 4], padding='causal', \n",
    "            use_skip_connections=True, dropout_rate=0.2, return_sequences=False, \n",
    "            input_shape=(post_context_length, 1)),\n",
    "        Dense(gap_length)\n",
    "    ])\n",
    "    backward_model.compile(optimizer='adam', loss='mse')\n",
    "    backward_model.fit(X_b_train, y_b_train.reshape(-1, gap_length), epochs=EPOCHS, batch_size=BATCH_SIZE, \n",
    "                       validation_data=(X_b_test, y_b_test.reshape(-1, gap_length)), callbacks=[early_stopping], verbose=0)\n",
    "    return forward_model, backward_model\n",
    "\n",
    "def create_and_train_lstm_seq2seq(X_f_train, y_f_train, X_f_test, y_f_test, X_b_train, y_b_train, X_b_test, y_b_test, \n",
    "                                  pre_context_length, post_context_length, gap_length):\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=PATIENCE, restore_best_weights=True)\n",
    "    forward_model = Sequential([\n",
    "        LSTM(64, input_shape=(pre_context_length, 1), return_sequences=False),\n",
    "        Dense(gap_length)\n",
    "    ])\n",
    "    forward_model.compile(optimizer='adam', loss='mse')\n",
    "    forward_model.fit(X_f_train, y_f_train.reshape(-1, gap_length), epochs=EPOCHS, batch_size=BATCH_SIZE, \n",
    "                      validation_data=(X_f_test, y_f_test.reshape(-1, gap_length)), callbacks=[early_stopping], verbose=0)\n",
    "    \n",
    "    backward_model = Sequential([\n",
    "        LSTM(64, input_shape=(post_context_length, 1), return_sequences=False),\n",
    "        Dense(gap_length)\n",
    "    ])\n",
    "    backward_model.compile(optimizer='adam', loss='mse')\n",
    "    backward_model.fit(X_b_train, y_b_train.reshape(-1, gap_length), epochs=EPOCHS, batch_size=BATCH_SIZE, \n",
    "                       validation_data=(X_b_test, y_b_test.reshape(-1, gap_length)), callbacks=[early_stopping], verbose=0)\n",
    "    return forward_model, backward_model\n",
    "\n",
    "def create_and_train_gru_seq2seq(X_f_train, y_f_train, X_f_test, y_f_test, X_b_train, y_b_train, X_b_test, y_b_test, \n",
    "                                 pre_context_length, post_context_length, gap_length):\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=PATIENCE, restore_best_weights=True)\n",
    "    forward_model = Sequential([\n",
    "        GRU(64, input_shape=(pre_context_length, 1), return_sequences=False),\n",
    "        Dense(gap_length)\n",
    "    ])\n",
    "    forward_model.compile(optimizer='adam', loss='mse')\n",
    "    forward_model.fit(X_f_train, y_f_train.reshape(-1, gap_length), epochs=EPOCHS, batch_size=BATCH_SIZE, \n",
    "                      validation_data=(X_f_test, y_f_test.reshape(-1, gap_length)), callbacks=[early_stopping], verbose=0)\n",
    "    \n",
    "    backward_model = Sequential([\n",
    "        GRU(64, input_shape=(post_context_length, 1), return_sequences=False),\n",
    "        Dense(gap_length)\n",
    "    ])\n",
    "    backward_model.compile(optimizer='adam', loss='mse')\n",
    "    backward_model.fit(X_b_train, y_b_train.reshape(-1, gap_length), epochs=EPOCHS, batch_size=BATCH_SIZE, \n",
    "                       validation_data=(X_b_test, y_b_test.reshape(-1, gap_length)), callbacks=[early_stopping], verbose=0)\n",
    "    return forward_model, backward_model\n",
    "\n",
    "def create_and_train_cnn_seq2seq(X_f_train, y_f_train, X_f_test, y_f_test, X_b_train, y_b_train, X_b_test, y_b_test, \n",
    "                                 pre_context_length, post_context_length, gap_length):\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=PATIENCE, restore_best_weights=True)\n",
    "    forward_model = Sequential([\n",
    "        Conv1D(filters=32, kernel_size=3, activation='relu', padding='causal', input_shape=(pre_context_length, 1)),\n",
    "        Conv1D(filters=32, kernel_size=3, activation='relu', padding='causal'),\n",
    "        Flatten(),\n",
    "        Dense(gap_length)\n",
    "    ])\n",
    "    forward_model.compile(optimizer='adam', loss='mse')\n",
    "    forward_model.fit(X_f_train, y_f_train.reshape(-1, gap_length), epochs=EPOCHS, batch_size=BATCH_SIZE, \n",
    "                      validation_data=(X_f_test, y_f_test.reshape(-1, gap_length)), callbacks=[early_stopping], verbose=0)\n",
    "    \n",
    "    backward_model = Sequential([\n",
    "        Conv1D(filters=32, kernel_size=3, activation='relu', padding='causal', input_shape=(post_context_length, 1)),\n",
    "        Conv1D(filters=32, kernel_size=3, activation='relu', padding='causal'),\n",
    "        Flatten(),\n",
    "        Dense(gap_length)\n",
    "    ])\n",
    "    backward_model.compile(optimizer='adam', loss='mse')\n",
    "    backward_model.fit(X_b_train, y_b_train.reshape(-1, gap_length), epochs=EPOCHS, batch_size=BATCH_SIZE, \n",
    "                       validation_data=(X_b_test, y_b_test.reshape(-1, gap_length)), callbacks=[early_stopping], verbose=0)\n",
    "    return forward_model, backward_model\n",
    "\n",
    "def create_and_train_rf_seq2seq(X_f_train, y_f_train, X_f_test, y_f_test, X_b_train, y_b_train, X_b_test, y_b_test, \n",
    "                                pre_context_length, post_context_length, gap_length):\n",
    "    # Convert 3D data (samples, timesteps, features) to 2D (samples, timesteps*features) for RandomForest\n",
    "    X_f_train_flat = X_f_train.reshape(X_f_train.shape[0], -1)\n",
    "    y_f_train_flat = y_f_train.reshape(y_f_train.shape[0], -1)\n",
    "    X_f_test_flat = X_f_test.reshape(X_f_test.shape[0], -1)\n",
    "    X_b_train_flat = X_b_train.reshape(X_b_train.shape[0], -1)\n",
    "    y_b_train_flat = y_b_train.reshape(y_b_train.shape[0], -1)\n",
    "    X_b_test_flat = X_b_test.reshape(X_b_test.shape[0], -1)\n",
    "\n",
    "    forward_model = RandomForestRegressor(n_estimators=N_ESTIMATORS, random_state=42)\n",
    "    forward_model.fit(X_f_train_flat, y_f_train_flat)\n",
    "    \n",
    "    backward_model = RandomForestRegressor(n_estimators=N_ESTIMATORS, random_state=42)\n",
    "    backward_model.fit(X_b_train_flat, y_b_train_flat)\n",
    "    \n",
    "    return forward_model, backward_model\n",
    "\n",
    "def create_and_train_xgb_seq2seq(X_f_train, y_f_train, X_f_test, y_f_test, X_b_train, y_b_train, X_b_test, y_b_test, \n",
    "                                 pre_context_length, post_context_length, gap_length):\n",
    "    # Convert 3D data (samples, timesteps, features) to 2D (samples, timesteps*features) for XGBoost\n",
    "    X_f_train_flat = X_f_train.reshape(X_f_train.shape[0], -1)\n",
    "    y_f_train_flat = y_f_train.reshape(y_f_train.shape[0], -1)\n",
    "    X_f_test_flat = X_f_test.reshape(X_f_test.shape[0], -1)\n",
    "    X_b_train_flat = X_b_train.reshape(X_b_train.shape[0], -1)\n",
    "    y_b_train_flat = y_b_train.reshape(y_b_train.shape[0], -1)\n",
    "    X_b_test_flat = X_b_test.reshape(X_b_test.shape[0], -1)\n",
    "\n",
    "    forward_model = XGBRegressor(n_estimators=N_ESTIMATORS, random_state=42)\n",
    "    forward_model.fit(X_f_train_flat, y_f_train_flat)\n",
    "    \n",
    "    backward_model = XGBRegressor(n_estimators=N_ESTIMATORS, random_state=42)\n",
    "    backward_model.fit(X_b_train_flat, y_b_train_flat)\n",
    "    \n",
    "    return forward_model, backward_model\n",
    "\n",
    "def create_and_train_rnn_seq2seq(X_f_train, y_f_train, X_f_test, y_f_test, X_b_train, y_b_train, X_b_test, y_b_test, \n",
    "                                 pre_context_length, post_context_length, gap_length):\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=PATIENCE, restore_best_weights=True)\n",
    "    forward_model = Sequential([\n",
    "        SimpleRNN(64, input_shape=(pre_context_length, 1), return_sequences=False),\n",
    "        Dense(gap_length)\n",
    "    ])\n",
    "    forward_model.compile(optimizer='adam', loss='mse')\n",
    "    forward_model.fit(X_f_train, y_f_train.reshape(-1, gap_length), epochs=EPOCHS, batch_size=BATCH_SIZE, \n",
    "                      validation_data=(X_f_test, y_f_test.reshape(-1, gap_length)), callbacks=[early_stopping], verbose=0)\n",
    "    \n",
    "    backward_model = Sequential([\n",
    "        SimpleRNN(64, input_shape=(post_context_length, 1), return_sequences=False),\n",
    "        Dense(gap_length)\n",
    "    ])\n",
    "    backward_model.compile(optimizer='adam', loss='mse')\n",
    "    backward_model.fit(X_b_train, y_b_train.reshape(-1, gap_length), epochs=EPOCHS, batch_size=BATCH_SIZE, \n",
    "                       validation_data=(X_b_test, y_b_test.reshape(-1, gap_length)), callbacks=[early_stopping], verbose=0)\n",
    "    return forward_model, backward_model\n",
    "\n",
    "# Functions for for creating and training autoregression models with a bidirectional approach\n",
    "def create_and_train_rnn_autoreg(X_f_train, y_f_train, X_f_test, y_f_test, X_b_train, y_b_train, X_b_test, y_b_test, \n",
    "                                 pre_context_length, post_context_length, gap_length):\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=PATIENCE, restore_best_weights=True)\n",
    "\n",
    "    # Forward model\n",
    "    forward_input = Input(shape=(pre_context_length, 1))\n",
    "    x = SimpleRNN(64, activation='tanh')(forward_input)\n",
    "    forward_output = Dense(1)(x)\n",
    "    forward_model = Model(forward_input, forward_output)\n",
    "    forward_model.compile(optimizer=Adam(), loss='mse')\n",
    "    forward_model.fit(X_f_train, y_f_train, epochs=EPOCHS, batch_size=BATCH_SIZE, \n",
    "                      validation_data=(X_f_test, y_f_test), callbacks=[early_stopping], verbose=0)\n",
    "\n",
    "    # Backward model\n",
    "    backward_input = Input(shape=(post_context_length, 1))\n",
    "    x = SimpleRNN(64, activation='tanh')(backward_input)\n",
    "    backward_output = Dense(1)(x)\n",
    "    backward_model = Model(backward_input, backward_output)\n",
    "    backward_model.compile(optimizer=Adam(), loss='mse')\n",
    "    backward_model.fit(X_b_train, y_b_train, epochs=EPOCHS, batch_size=BATCH_SIZE, \n",
    "                       validation_data=(X_b_test, y_b_test), callbacks=[early_stopping], verbose=0)\n",
    "\n",
    "    return forward_model, backward_model\n",
    "\n",
    "def create_and_train_lstm_autoreg(X_f_train, y_f_train, X_f_test, y_f_test, X_b_train, y_b_train, X_b_test, y_b_test, \n",
    "                                  pre_context_length, post_context_length, gap_length):\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=PATIENCE, restore_best_weights=True)\n",
    "\n",
    "    # Forward model\n",
    "    forward_input = Input(shape=(pre_context_length, 1))\n",
    "    x = LSTM(64, activation='tanh')(forward_input)\n",
    "    forward_output = Dense(1)(x)\n",
    "    forward_model = Model(forward_input, forward_output)\n",
    "    forward_model.compile(optimizer=Adam(), loss='mse')\n",
    "    forward_model.fit(X_f_train, y_f_train, epochs=EPOCHS, batch_size=BATCH_SIZE, \n",
    "                      validation_data=(X_f_test, y_f_test), callbacks=[early_stopping], verbose=0)\n",
    "\n",
    "    # Backward model\n",
    "    backward_input = Input(shape=(post_context_length, 1))\n",
    "    x = LSTM(64, activation='tanh')(backward_input)\n",
    "    backward_output = Dense(1)(x)\n",
    "    backward_model = Model(backward_input, backward_output)\n",
    "    backward_model.compile(optimizer=Adam(), loss='mse')\n",
    "    backward_model.fit(X_b_train, y_b_train, epochs=EPOCHS, batch_size=BATCH_SIZE, \n",
    "                       validation_data=(X_b_test, y_b_test), callbacks=[early_stopping], verbose=0)\n",
    "\n",
    "    return forward_model, backward_model\n",
    "\n",
    "def create_and_train_gru_autoreg(X_f_train, y_f_train, X_f_test, y_f_test, X_b_train, y_b_train, X_b_test, y_b_test, \n",
    "                                 pre_context_length, post_context_length, gap_length):\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=PATIENCE, restore_best_weights=True)\n",
    "\n",
    "    # Forward model\n",
    "    forward_input = Input(shape=(pre_context_length, 1))\n",
    "    x = GRU(64, activation='tanh')(forward_input)\n",
    "    forward_output = Dense(1)(x)\n",
    "    forward_model = Model(forward_input, forward_output)\n",
    "    forward_model.compile(optimizer=Adam(), loss='mse')\n",
    "    forward_model.fit(X_f_train, y_f_train, epochs=EPOCHS, batch_size=BATCH_SIZE, \n",
    "                      validation_data=(X_f_test, y_f_test), callbacks=[early_stopping], verbose=0)\n",
    "\n",
    "    # Backward model\n",
    "    backward_input = Input(shape=(post_context_length, 1))\n",
    "    x = GRU(64, activation='tanh')(backward_input)\n",
    "    backward_output = Dense(1)(x)\n",
    "    backward_model = Model(backward_input, backward_output)\n",
    "    backward_model.compile(optimizer=Adam(), loss='mse')\n",
    "    backward_model.fit(X_b_train, y_b_train, epochs=EPOCHS, batch_size=BATCH_SIZE, \n",
    "                       validation_data=(X_b_test, y_b_test), callbacks=[early_stopping], verbose=0)\n",
    "\n",
    "    return forward_model, backward_model\n",
    "\n",
    "def create_and_train_cnn_autoreg(X_f_train, y_f_train, X_f_test, y_f_test, X_b_train, y_b_train, X_b_test, y_b_test, \n",
    "                                 pre_context_length, post_context_length, gap_length):\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=PATIENCE, restore_best_weights=True)\n",
    "\n",
    "    # Forward model\n",
    "    forward_input = Input(shape=(pre_context_length, 1))\n",
    "    x = Conv1D(filters=32, kernel_size=3, activation='relu', padding='causal')(forward_input)\n",
    "    x = Flatten()(x)\n",
    "    forward_output = Dense(1)(x)\n",
    "    forward_model = Model(forward_input, forward_output)\n",
    "    forward_model.compile(optimizer=Adam(), loss='mse')\n",
    "    forward_model.fit(X_f_train, y_f_train, epochs=EPOCHS, batch_size=BATCH_SIZE, \n",
    "                      validation_data=(X_f_test, y_f_test), callbacks=[early_stopping], verbose=0)\n",
    "\n",
    "    # Backward model\n",
    "    backward_input = Input(shape=(post_context_length, 1))\n",
    "    x = Conv1D(filters=32, kernel_size=3, activation='relu', padding='causal')(backward_input)\n",
    "    x = Flatten()(x)\n",
    "    backward_output = Dense(1)(x)\n",
    "    backward_model = Model(backward_input, backward_output)\n",
    "    backward_model.compile(optimizer=Adam(), loss='mse')\n",
    "    backward_model.fit(X_b_train, y_b_train, epochs=EPOCHS, batch_size=BATCH_SIZE, \n",
    "                       validation_data=(X_b_test, y_b_test), callbacks=[early_stopping], verbose=0)\n",
    "\n",
    "    return forward_model, backward_model\n",
    "\n",
    "def create_and_train_rf_autoreg(X_f_train, y_f_train, X_f_test, y_f_test, X_b_train, y_b_train, X_b_test, y_b_test, \n",
    "                                pre_context_length, post_context_length, gap_length):\n",
    "    # Convert 3D data (samples, timesteps, features) to 2D (samples, timesteps*features) for RandomForest\n",
    "    X_f_train_flat = X_f_train.reshape(X_f_train.shape[0], -1)\n",
    "    X_f_test_flat = X_f_test.reshape(X_f_test.shape[0], -1)\n",
    "    X_b_train_flat = X_b_train.reshape(X_b_train.shape[0], -1)\n",
    "    X_b_test_flat = X_b_test.reshape(X_b_test.shape[0], -1)\n",
    "\n",
    "    # Forward model\n",
    "    forward_model = RandomForestRegressor(n_estimators=N_ESTIMATORS, random_state=42)\n",
    "    forward_model.fit(X_f_train_flat, y_f_train.reshape(-1, 1))  \n",
    "    \n",
    "\n",
    "\n",
    "    # Backward model\n",
    "    backward_model = RandomForestRegressor(n_estimators=N_ESTIMATORS, random_state=42)\n",
    "    backward_model.fit(X_b_train_flat, y_b_train.reshape(-1, 1))  \n",
    "    \n",
    "    return forward_model, backward_model\n",
    "\n",
    "def create_and_train_xgb_autoreg(X_f_train, y_f_train, X_f_test, y_f_test, X_b_train, y_b_train, X_b_test, y_b_test, \n",
    "                                 pre_context_length, post_context_length, gap_length):\n",
    "    # Convert 3D data (samples, timesteps, features) to 2D (samples, timesteps*features) for XGBoost\n",
    "    X_f_train_flat = X_f_train.reshape(X_f_train.shape[0], -1)\n",
    "    X_f_test_flat = X_f_test.reshape(X_f_test.shape[0], -1)\n",
    "    X_b_train_flat = X_b_train.reshape(X_b_train.shape[0], -1)\n",
    "    X_b_test_flat = X_b_test.reshape(X_b_test.shape[0], -1)\n",
    "\n",
    "    # Forward model\n",
    "    forward_model = XGBRegressor(n_estimators=N_ESTIMATORS, random_state=42)\n",
    "    forward_model.fit(X_f_train_flat, y_f_train.ravel())  # Use ravel() for 1D output\n",
    "    \n",
    "    # Backward model\n",
    "    backward_model = XGBRegressor(n_estimators=N_ESTIMATORS, random_state=42)\n",
    "    backward_model.fit(X_b_train_flat, y_b_train.ravel())  # Use ravel() for 1D output\n",
    "    \n",
    "    return forward_model, backward_model\n",
    "\n",
    "def create_and_train_tcn_autoreg(X_f_train, y_f_train, X_f_test, y_f_test, X_b_train, y_b_train, X_b_test, y_b_test, \n",
    "                                 pre_context_length, post_context_length, gap_length):\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=PATIENCE, restore_best_weights=True)\n",
    "\n",
    "    # Forward model\n",
    "    forward_input = Input(shape=(pre_context_length, 1))\n",
    "    x = TCN(nb_filters=32, kernel_size=3, dilations=[1, 2, 4], padding='causal', \n",
    "            use_skip_connections=True, dropout_rate=0.2, return_sequences=False)(forward_input)\n",
    "    forward_output = Dense(1)(x)\n",
    "    forward_model = Model(forward_input, forward_output)\n",
    "    forward_model.compile(optimizer=Adam(), loss='mse')\n",
    "    forward_model.fit(X_f_train, y_f_train, epochs=EPOCHS, batch_size=BATCH_SIZE, \n",
    "                      validation_data=(X_f_test, y_f_test), callbacks=[early_stopping], verbose=0)\n",
    "\n",
    "    # Backward model\n",
    "    backward_input = Input(shape=(post_context_length, 1))\n",
    "    x = TCN(nb_filters=32, kernel_size=3, dilations=[1, 2, 4], padding='causal', \n",
    "            use_skip_connections=True, dropout_rate=0.2, return_sequences=False)(backward_input)\n",
    "    backward_output = Dense(1)(x)\n",
    "    backward_model = Model(backward_input, backward_output)\n",
    "    backward_model.compile(optimizer=Adam(), loss='mse')\n",
    "    backward_model.fit(X_b_train, y_b_train, epochs=EPOCHS, batch_size=BATCH_SIZE, \n",
    "                       validation_data=(X_b_test, y_b_test), callbacks=[early_stopping], verbose=0)\n",
    "\n",
    "    return forward_model, backward_model\n",
    "\n",
    "# Functions for creating and training Context-combined single-step Seq2Seq models (UniSeq2Seq)\n",
    "# Function for creating and training a UniSeq2Seq LSTM model\n",
    "def create_and_train_uniseq2seq_lstm(X_f_train, y_f_train, X_f_test, y_f_test, \n",
    "                                   X_b_train, y_b_train, X_b_test, y_b_test, \n",
    "                                   pre_context_length, post_context_length, gap_length, model_info=None):\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=PATIENCE, restore_best_weights=True)\n",
    "    n_features = X_f_train.shape[2]  # Number of features (always 1 for univariate)\n",
    "    encoder_input_left = Input(shape=(pre_context_length, n_features), name=\"encoder_input_left\")\n",
    "    encoder_input_right = Input(shape=(post_context_length, n_features), name=\"encoder_input_right\")\n",
    "    encoder_inputs = Concatenate(axis=1)([encoder_input_left, encoder_input_right])\n",
    "    encoder = Bidirectional(LSTM(64, return_sequences=False))(encoder_inputs)\n",
    "    decoder_input = RepeatVector(gap_length)(encoder)\n",
    "    decoder = LSTM(64, return_sequences=True)(decoder_input)\n",
    "    decoder_dense = TimeDistributed(Dense(1))(decoder)\n",
    "    model = Model(inputs=[encoder_input_left, encoder_input_right], outputs=decoder_dense)\n",
    "    model.compile(optimizer=Adam(), loss='mse')\n",
    "    model.fit([X_f_train, X_b_train], y_f_train.reshape(-1, gap_length, 1),\n",
    "              epochs=EPOCHS, batch_size=BATCH_SIZE, \n",
    "              validation_data=([X_f_test, X_b_test], y_f_test.reshape(-1, gap_length, 1)),\n",
    "              callbacks=[early_stopping], verbose=0)\n",
    "    # Save model_info as a model attribute\n",
    "    if model_info:\n",
    "        model.model_info = model_info\n",
    "    return model, None\n",
    "\n",
    "# Function for creating and training a UniSeq2Seq GRU model\n",
    "def create_and_train_uniseq2seq_gru(X_f_train, y_f_train, X_f_test, y_f_test, \n",
    "                                    X_b_train, y_b_train, X_b_test, y_b_test, \n",
    "                                    pre_context_length, post_context_length, gap_length, model_info=None):\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=PATIENCE, restore_best_weights=True)\n",
    "    \n",
    "    # Inputs: left and right contexts\n",
    "    encoder_input_left = Input(shape=(pre_context_length, 1), name=\"encoder_input_left\")\n",
    "    encoder_input_right = Input(shape=(post_context_length, 1), name=\"encoder_input_right\")\n",
    "    \n",
    "    # Concatenate contexts along the time axis\n",
    "    encoder_inputs = Concatenate(axis=1)([encoder_input_left, encoder_input_right])\n",
    "    \n",
    "    # Encoder: Bidirectional GRU\n",
    "    encoder = Bidirectional(GRU(64, return_sequences=False))\n",
    "    encoder_output = encoder(encoder_inputs)\n",
    "    \n",
    "    # Decoder: repeat the hidden state gap_length times\n",
    "    decoder_input = RepeatVector(gap_length)(encoder_output)\n",
    "    decoder = GRU(64, return_sequences=True)\n",
    "    decoder_output = decoder(decoder_input)\n",
    "    decoder_dense = TimeDistributed(Dense(1))\n",
    "    decoder_predictions = decoder_dense(decoder_output)\n",
    "    \n",
    "    # Create and compile the model\n",
    "    model = Model(inputs=[encoder_input_left, encoder_input_right], outputs=decoder_predictions)\n",
    "    model.compile(optimizer=Adam(), loss='mse')\n",
    "    \n",
    "    # Model training\n",
    "    model.fit([X_f_train, X_b_train], y_f_train.reshape(-1, gap_length, 1),\n",
    "              epochs=EPOCHS, batch_size=BATCH_SIZE, \n",
    "              validation_data=([X_f_test, X_b_test], y_f_test.reshape(-1, gap_length, 1)),\n",
    "              callbacks=[early_stopping], verbose=0)\n",
    "    if model_info:\n",
    "        model.model_info = model_info\n",
    "    return model, None  # UniSeq2Seq uses one model\n",
    "\n",
    "# Function for creating and training a UniSeq2Seq TCN model\n",
    "def create_and_train_uniseq2seq_tcn(X_f_train, y_f_train, X_f_test, y_f_test, \n",
    "                                    X_b_train, y_b_train, X_b_test, y_b_test, \n",
    "                                    pre_context_length, post_context_length, gap_length, model_info=None):\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=PATIENCE, restore_best_weights=True)\n",
    "    \n",
    "    # Inputs: left and right contexts\n",
    "    encoder_input_left = Input(shape=(pre_context_length, 1), name=\"encoder_input_left\")\n",
    "    encoder_input_right = Input(shape=(post_context_length, 1), name=\"encoder_input_right\")\n",
    "    \n",
    "    # Concatenate contexts along the time axis\n",
    "    encoder_inputs = Concatenate(axis=1)([encoder_input_left, encoder_input_right])\n",
    "    \n",
    "    # Encoder: TCN\n",
    "    encoder = TCN(nb_filters=64, kernel_size=3, dilations=[1, 2, 4, 8], return_sequences=False)\n",
    "    encoder_output = encoder(encoder_inputs)\n",
    "    \n",
    "    # Decoder: repeat the hidden state gap_length times\n",
    "    decoder_input = RepeatVector(gap_length)(encoder_output)\n",
    "    decoder = TCN(nb_filters=64, kernel_size=3, dilations=[1, 2, 4, 8], return_sequences=True)\n",
    "    decoder_output = decoder(decoder_input)\n",
    "    decoder_dense = TimeDistributed(Dense(1))\n",
    "    decoder_predictions = decoder_dense(decoder_output)\n",
    "    \n",
    "    # Create and compile the model\n",
    "    model = Model(inputs=[encoder_input_left, encoder_input_right], outputs=decoder_predictions)\n",
    "    model.compile(optimizer=Adam(), loss='mse')\n",
    "    \n",
    "    # Model training\n",
    "    model.fit([X_f_train, X_b_train], y_f_train.reshape(-1, gap_length, 1),\n",
    "              epochs=EPOCHS, batch_size=BATCH_SIZE, \n",
    "              validation_data=([X_f_test, X_b_test], y_f_test.reshape(-1, gap_length, 1)),\n",
    "              callbacks=[early_stopping], verbose=0)\n",
    "    if model_info:\n",
    "        model.model_info = model_info\n",
    "    return model, None  # UniSeq2Seq uses one model\n",
    "\n",
    "# Function for creating and training a UniSeq2Seq CNN model\n",
    "def create_and_train_uniseq2seq_cnn(X_f_train, y_f_train, X_f_test, y_f_test, \n",
    "                                    X_b_train, y_b_train, X_b_test, y_b_test, \n",
    "                                    pre_context_length, post_context_length, gap_length, model_info=None):\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=PATIENCE, restore_best_weights=True)\n",
    "    \n",
    "    # Inputs: left and right contexts\n",
    "    encoder_input_left = Input(shape=(pre_context_length, 1), name=\"encoder_input_left\")\n",
    "    encoder_input_right = Input(shape=(post_context_length, 1), name=\"encoder_input_right\")\n",
    "    \n",
    "    # Concatenate contexts along the time axis\n",
    "    encoder_inputs = Concatenate(axis=1)([encoder_input_left, encoder_input_right])\n",
    "    \n",
    "    # Encoder: CNN\n",
    "    x = Conv1D(filters=32, kernel_size=3, activation='relu', padding='causal')(encoder_inputs)\n",
    "    x = Conv1D(filters=64, kernel_size=3, activation='relu', padding='causal')(x)\n",
    "    encoder_output = Flatten()(x)  # Flatten to a vector\n",
    "    latent = Dense(128, activation='relu')(encoder_output)  # Latent representation\n",
    "    \n",
    "    # Decoder: transform the latent vector into a sequence\n",
    "    decoder_dense = Dense(gap_length * 64, activation='relu')(latent)\n",
    "    x_dec = Reshape((gap_length, 64))(decoder_dense)\n",
    "    x_dec = Conv1D(filters=32, kernel_size=3, activation='relu', padding='same')(x_dec)\n",
    "    decoder_predictions = Conv1D(filters=1, kernel_size=3, activation='linear', padding='same')(x_dec)\n",
    "    \n",
    "    # Create and compile the model\n",
    "    model = Model(inputs=[encoder_input_left, encoder_input_right], outputs=decoder_predictions)\n",
    "    model.compile(optimizer=Adam(), loss='mse')\n",
    "    \n",
    "    # Model training\n",
    "    model.fit([X_f_train, X_b_train], y_f_train.reshape(-1, gap_length, 1),\n",
    "              epochs=EPOCHS, batch_size=BATCH_SIZE, \n",
    "              validation_data=([X_f_test, X_b_test], y_f_test.reshape(-1, gap_length, 1)),\n",
    "              callbacks=[early_stopping], verbose=0)\n",
    "    if model_info:\n",
    "        model.model_info = model_info\n",
    "    return model, None  # UniSeq2Seq uses one model\n",
    "\n",
    "# Function for creating and training a UniSeq2Seq RNN model\n",
    "def create_and_train_uniseq2seq_rnn(X_f_train, y_f_train, X_f_test, y_f_test, \n",
    "                                    X_b_train, y_b_train, X_b_test, y_b_test, \n",
    "                                    pre_context_length, post_context_length, gap_length, model_info=None):\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=PATIENCE, restore_best_weights=True)\n",
    "    \n",
    "    # Inputs: left and right contexts\n",
    "    encoder_input_left = Input(shape=(pre_context_length, 1), name=\"encoder_input_left\")\n",
    "    encoder_input_right = Input(shape=(post_context_length, 1), name=\"encoder_input_right\")\n",
    "    \n",
    "    # Concatenate contexts along the time axis\n",
    "    encoder_inputs = Concatenate(axis=1)([encoder_input_left, encoder_input_right])\n",
    "    \n",
    "    # Encoder: Bidirectional SimpleRNN\n",
    "    encoder = Bidirectional(SimpleRNN(64, return_sequences=False))\n",
    "    encoder_output = encoder(encoder_inputs)\n",
    "    \n",
    "    # Decoder: repeat the hidden state gap_length times\n",
    "    decoder_input = RepeatVector(gap_length)(encoder_output)\n",
    "    decoder = SimpleRNN(64, return_sequences=True)\n",
    "    decoder_output = decoder(decoder_input)\n",
    "    decoder_dense = TimeDistributed(Dense(1))\n",
    "    decoder_predictions = decoder_dense(decoder_output)\n",
    "    \n",
    "    # Create and compile the model\n",
    "    model = Model(inputs=[encoder_input_left, encoder_input_right], outputs=decoder_predictions)\n",
    "    model.compile(optimizer=Adam(), loss='mse')\n",
    "    \n",
    "    # Model training\n",
    "    model.fit([X_f_train, X_b_train], y_f_train.reshape(-1, gap_length, 1),\n",
    "              epochs=EPOCHS, batch_size=BATCH_SIZE, \n",
    "              validation_data=([X_f_test, X_b_test], y_f_test.reshape(-1, gap_length, 1)),\n",
    "              callbacks=[early_stopping], verbose=0)\n",
    "    if model_info:\n",
    "        model.model_info = model_info\n",
    "    return model, None  # UniSeq2Seq uses one model\n",
    "\n",
    "# Function for creating and training a UniSeq2Seq Random Forest model\n",
    "def create_and_train_uniseq2seq_rf(X_f_train, y_f_train, X_f_test, y_f_test, \n",
    "                                 X_b_train, y_b_train, X_b_test, y_b_test, \n",
    "                                 pre_context_length, post_context_length, gap_length, model_info=None):\n",
    "    X_train_combined = np.concatenate([X_f_train, X_b_train], axis=1)  # Combine contexts\n",
    "    X_train_flat = X_train_combined.reshape(X_train_combined.shape[0], -1)  # Flat vector\n",
    "    y_train_flat = y_f_train.reshape(y_f_train.shape[0], -1)\n",
    "    X_test_combined = np.concatenate([X_f_test, X_b_test], axis=1)\n",
    "    X_test_flat = X_test_combined.reshape(X_test_combined.shape[0], -1)\n",
    "    rf_model = MultiOutputRegressor(RandomForestRegressor(n_estimators=N_ESTIMATORS, random_state=42))\n",
    "    rf_model.fit(X_train_flat, y_train_flat)\n",
    "    # Save model_info as a model attribute\n",
    "    if model_info:\n",
    "        rf_model.model_info = model_info\n",
    "    \n",
    "    return rf_model, None  # UniSeq2Seq uses one model\n",
    "\n",
    "# Function for creating and training a UniSeq2Seq XGBoost model\n",
    "def create_and_train_uniseq2seq_xgb(X_f_train, y_f_train, X_f_test, y_f_test, \n",
    "                                    X_b_train, y_b_train, X_b_test, y_b_test, \n",
    "                                    pre_context_length, post_context_length, gap_length):\n",
    "    # Convert 3D data (samples, timesteps, features) to 2D (samples, timesteps*features)\n",
    "    X_train_combined = np.concatenate([X_f_train, X_b_train], axis=1)  # Combine left and right contexts\n",
    "    X_train_flat = X_train_combined.reshape(X_train_combined.shape[0], -1)  # Flat vector\n",
    "    y_train_flat = y_f_train.reshape(y_f_train.shape[0], -1)  # Flat vector of the gap\n",
    "    \n",
    "    X_test_combined = np.concatenate([X_f_test, X_b_test], axis=1)\n",
    "    X_test_flat = X_test_combined.reshape(X_test_combined.shape[0], -1)\n",
    "    \n",
    "    # Create and train the model\n",
    "    xgb_model = MultiOutputRegressor(XGBRegressor(n_estimators=N_ESTIMATORS, random_state=42))\n",
    "    xgb_model.fit(X_train_flat, y_train_flat)\n",
    "    \n",
    "    return xgb_model, None  # UniSeq2Seq uses one model\n",
    "\n",
    "\n",
    "# Functions for creating and training single-step unidirectional autoregressive models (UniAutoreg)\n",
    "def create_and_train_uniautoreg_lstm(X_f_train, y_f_train, X_f_test, y_f_test, \n",
    "                                     X_b_train, y_b_train, X_b_test, y_b_test, \n",
    "                                     pre_context_length, post_context_length, gap_length):\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=PATIENCE, restore_best_weights=True)\n",
    "    model = Sequential([\n",
    "        LSTM(64, input_shape=(pre_context_length, 1), return_sequences=False),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(), loss='mse')\n",
    "    model.fit(X_f_train, y_f_train, epochs=EPOCHS, batch_size=BATCH_SIZE, \n",
    "              validation_data=(X_f_test, y_f_test), callbacks=[early_stopping], verbose=1)\n",
    "    return model, None\n",
    "\n",
    "def create_and_train_uniautoreg_gru(X_f_train, y_f_train, X_f_test, y_f_test, \n",
    "                                    X_b_train, y_b_train, X_b_test, y_b_test, \n",
    "                                    pre_context_length, post_context_length, gap_length):\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=PATIENCE, restore_best_weights=True)\n",
    "    model = Sequential([\n",
    "        GRU(64, input_shape=(pre_context_length, 1), return_sequences=False),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(), loss='mse')\n",
    "    model.fit(X_f_train, y_f_train, epochs=EPOCHS, batch_size=BATCH_SIZE, \n",
    "              validation_data=(X_f_test, y_f_test), callbacks=[early_stopping], verbose=1)\n",
    "    return model, None\n",
    "\n",
    "def create_and_train_uniautoreg_tcn(X_f_train, y_f_train, X_f_test, y_f_test, \n",
    "                                    X_b_train, y_b_train, X_b_test, y_b_test, \n",
    "                                    pre_context_length, post_context_length, gap_length):\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=PATIENCE, restore_best_weights=True)\n",
    "    model = Sequential([\n",
    "        TCN(nb_filters=64, kernel_size=3, dilations=[1, 2, 4, 8], input_shape=(pre_context_length, 1), return_sequences=False),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(), loss='mse')\n",
    "    model.fit(X_f_train, y_f_train, epochs=EPOCHS, batch_size=BATCH_SIZE, \n",
    "              validation_data=(X_f_test, y_f_test), callbacks=[early_stopping], verbose=1)\n",
    "    return model, None\n",
    "\n",
    "def create_and_train_uniautoreg_cnn(X_f_train, y_f_train, X_f_test, y_f_test, \n",
    "                                    X_b_train, y_b_train, X_b_test, y_b_test, \n",
    "                                    pre_context_length, post_context_length, gap_length):\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=PATIENCE, restore_best_weights=True)\n",
    "    model = Sequential([\n",
    "        Conv1D(filters=32, kernel_size=3, activation='relu', padding='causal', input_shape=(pre_context_length, 1)),\n",
    "        Conv1D(filters=64, kernel_size=3, activation='relu', padding='causal'),\n",
    "        Flatten(),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(), loss='mse')\n",
    "    model.fit(X_f_train, y_f_train, epochs=EPOCHS, batch_size=BATCH_SIZE, \n",
    "              validation_data=(X_f_test, y_f_test), callbacks=[early_stopping], verbose=1)\n",
    "    return model, None\n",
    "\n",
    "def create_and_train_uniautoreg_rnn(X_f_train, y_f_train, X_f_test, y_f_test, \n",
    "                                    X_b_train, y_b_train, X_b_test, y_b_test, \n",
    "                                    pre_context_length, post_context_length, gap_length):\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=PATIENCE, restore_best_weights=True)\n",
    "    model = Sequential([\n",
    "        SimpleRNN(64, input_shape=(pre_context_length, 1), return_sequences=False),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(), loss='mse')\n",
    "    model.fit(X_f_train, y_f_train, epochs=EPOCHS, batch_size=BATCH_SIZE, \n",
    "              validation_data=(X_f_test, y_f_test), callbacks=[early_stopping], verbose=1)\n",
    "    return model, None\n",
    "\n",
    "def create_and_train_uniautoreg_rf(X_f_train, y_f_train, X_f_test, y_f_test, \n",
    "                                   X_b_train, y_b_train, X_b_test, y_b_test, \n",
    "                                   pre_context_length, post_context_length, gap_length):\n",
    "    X_train_flat = X_f_train.reshape(X_f_train.shape[0], -1)  # Convert to a flat vector\n",
    "    X_test_flat = X_f_test.reshape(X_f_test.shape[0], -1)\n",
    "    model = RandomForestRegressor(n_estimators=N_ESTIMATORS, random_state=42)\n",
    "    model.fit(X_train_flat, y_f_train.ravel())  # Predict one value\n",
    "    return model, None\n",
    "\n",
    "def create_and_train_uniautoreg_xgb(X_f_train, y_f_train, X_f_test, y_f_test, \n",
    "                                    X_b_train, y_b_train, X_b_test, y_b_test, \n",
    "                                    pre_context_length, post_context_length, gap_length):\n",
    "    X_train_flat = X_f_train.reshape(X_f_train.shape[0], -1)  # Convert to a flat vector\n",
    "    X_test_flat = X_f_test.reshape(X_f_test.shape[0], -1)\n",
    "    model = XGBRegressor(n_estimators=N_ESTIMATORS, random_state=42)\n",
    "    model.fit(X_train_flat, y_f_train.ravel())  # Predict one value\n",
    "    return model, None\n",
    "\n",
    "# Multivariate models\n",
    "def create_and_train_uniseq2seq_rf_multi(X_f_train, y_f_train, X_f_test, y_f_test, \n",
    "                                         X_b_train, y_b_train, X_b_test, y_b_test, \n",
    "                                         pre_context_length, post_context_length, gap_length, model_info=None):\n",
    "    X_train_combined = np.concatenate([X_f_train, X_b_train], axis=1)  # Combine contexts\n",
    "    X_train_flat = X_train_combined.reshape(X_train_combined.shape[0], -1)  # Flat vector\n",
    "    y_train_flat = y_f_train.reshape(y_f_train.shape[0], -1)\n",
    "    X_test_combined = np.concatenate([X_f_test, X_b_test], axis=1)\n",
    "    X_test_flat = X_test_combined.reshape(X_test_combined.shape[0], -1)\n",
    "    rf_model = MultiOutputRegressor(RandomForestRegressor(n_estimators=N_ESTIMATORS, random_state=42))\n",
    "    rf_model.fit(X_train_flat, y_train_flat)\n",
    "    # Save model_info as a model attribute\n",
    "    if model_info:\n",
    "        rf_model.model_info = model_info\n",
    "    return rf_model, None\n",
    "\n",
    "def create_and_train_uniseq2seq_lstm_multi(X_f_train, y_f_train, X_f_test, y_f_test, \n",
    "                                           X_b_train, y_b_train, X_b_test, y_b_test, \n",
    "                                           pre_context_length, post_context_length, gap_length, model_info=None):\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=PATIENCE, restore_best_weights=True)\n",
    "    n_features = X_f_train.shape[2]  # Number of features\n",
    "    encoder_input_left = Input(shape=(pre_context_length, n_features), name=\"encoder_input_left\")\n",
    "    encoder_input_right = Input(shape=(post_context_length, n_features), name=\"encoder_input_right\")\n",
    "    encoder_inputs = Concatenate(axis=1)([encoder_input_left, encoder_input_right])\n",
    "    encoder = Bidirectional(LSTM(64, return_sequences=False))(encoder_inputs)\n",
    "    decoder_input = RepeatVector(gap_length)(encoder)\n",
    "    decoder = LSTM(64, return_sequences=True)(decoder_input)\n",
    "    decoder_dense = TimeDistributed(Dense(1))(decoder)\n",
    "    model = Model(inputs=[encoder_input_left, encoder_input_right], outputs=decoder_dense)\n",
    "    model.compile(optimizer=Adam(), loss='mse')\n",
    "    model.fit([X_f_train, X_b_train], y_f_train.reshape(-1, gap_length, 1),\n",
    "              epochs=EPOCHS, batch_size=BATCH_SIZE, \n",
    "              validation_data=([X_f_test, X_b_test], y_f_test.reshape(-1, gap_length, 1)),\n",
    "              callbacks=[early_stopping], verbose=0)\n",
    "    # Save model_info as a model attribute\n",
    "    if model_info:\n",
    "        model.model_info = model_info\n",
    "    return model, None\n",
    "\n",
    "def create_and_train_xgb_seq2seq_multi(X_f_train, y_f_train, X_f_test, y_f_test, \n",
    "                                       X_b_train, y_b_train, X_b_test, y_b_test, \n",
    "                                       pre_context_length, post_context_length, gap_length, model_info=None):\n",
    "    \"\"\"\n",
    "    Trains a multivariate XGB Seq2Seq model for bidirectional forecasting.\n",
    "    Data with multiple features is used, where the input data for the forward and backward\n",
    "    models have the shape (samples, timesteps, n_features). The forecasts of both models can then be\n",
    "    combined using the combine_forecasts function.\n",
    "\n",
    "    Parameters:\n",
    "      - X_f_train, y_f_train, X_f_test, y_f_test: data for training and testing the forward model.\n",
    "      - X_b_train, y_b_train, X_b_test, y_b_test: data for training and testing the backward model.\n",
    "      - pre_context_length: length of the input context for the forward model.\n",
    "      - post_context_length: length of the input context for the backward model.\n",
    "      - gap_length: length of the forecasted interval.\n",
    "      - model_info: additional model parameters (optional).\n",
    "\n",
    "    Returns:\n",
    "      - forward_model, backward_model: trained XGBRegressor models for forward and backward forecasting.\n",
    "    \"\"\"\n",
    "    # Convert 3D data (samples, timesteps, n_features) to 2D (samples, timesteps*n_features)\n",
    "    X_f_train_flat = X_f_train.reshape(X_f_train.shape[0], -1)\n",
    "    y_f_train_flat = y_f_train.reshape(y_f_train.shape[0], -1)\n",
    "    X_f_test_flat = X_f_test.reshape(X_f_test.shape[0], -1)\n",
    "    \n",
    "    X_b_train_flat = X_b_train.reshape(X_b_train.shape[0], -1)\n",
    "    y_b_train_flat = y_b_train.reshape(y_b_train.shape[0], -1)\n",
    "    X_b_test_flat = X_b_test.reshape(X_b_test.shape[0], -1)\n",
    "\n",
    "    # Train the forward model\n",
    "    forward_model = XGBRegressor(n_estimators=N_ESTIMATORS, random_state=42)\n",
    "    forward_model.fit(X_f_train_flat, y_f_train_flat)\n",
    "    \n",
    "    # Train the backward model\n",
    "    backward_model = XGBRegressor(n_estimators=N_ESTIMATORS, random_state=42)\n",
    "    backward_model.fit(X_b_train_flat, y_b_train_flat)\n",
    "    \n",
    "    # If additional model parameters are passed, save them as attributes\n",
    "    if model_info is not None:\n",
    "        forward_model.model_info = model_info\n",
    "        backward_model.model_info = model_info\n",
    "\n",
    "    return forward_model, backward_model\n",
    "\n",
    "def create_and_train_uniseq2seq_xgb(X_left_train, y_train, X_left_test, y_test, \n",
    "                                    X_right_train, y_train_right, X_right_test, y_test_right, \n",
    "                                    pre_context_length, post_context_length, gap_length, model_info=None):\n",
    "    \"\"\"\n",
    "    Trains a multivariate unidirectional UniSeq2Seq XGB model.\n",
    "    The data is fed as two contexts: left (X_left) and right (X_right). For training,\n",
    "    both contexts are combined, then flattened, and a single XGB model is trained\n",
    "    to predict the missing values (the target feature, usually pm2_5).\n",
    "    \n",
    "    Parameters:\n",
    "      - X_left_train, y_train, X_left_test, y_test: training and test data for the left context.\n",
    "      - X_right_train, y_train_right, X_right_test, y_test_right: training and test data for the right context.\n",
    "         (Usually y_train and y_train_right are the same, as the target is taken from the left context.)\n",
    "      - pre_context_length: length of the left context.\n",
    "      - post_context_length: length of the right context.\n",
    "      - gap_length: length of the forecasted interval (number of time steps to predict).\n",
    "      - model_info: additional information (e.g., feature_cols).\n",
    "      \n",
    "    Returns:\n",
    "      - xgb_model: trained XGBRegressor.\n",
    "      - None: only one network is used in this model.\n",
    "    \"\"\"\n",
    "    # Combine left and right contexts along the time axis\n",
    "    X_train_combined = np.concatenate([X_left_train, X_right_train], axis=1)\n",
    "    X_train_flat = X_train_combined.reshape(X_train_combined.shape[0], -1)\n",
    "    y_train_flat = y_train.reshape(y_train.shape[0], -1)  # target feature (univariate)\n",
    "    \n",
    "    X_test_combined = np.concatenate([X_left_test, X_right_test], axis=1)\n",
    "    X_test_flat = X_test_combined.reshape(X_test_combined.shape[0], -1)\n",
    "    \n",
    "    # Train the XGBoost model\n",
    "    xgb_model = XGBRegressor(n_estimators=N_ESTIMATORS, random_state=42)\n",
    "    xgb_model.fit(X_train_flat, y_train_flat)\n",
    "    \n",
    "    if model_info:\n",
    "        xgb_model.model_info = model_info\n",
    "        \n",
    "    return xgb_model, None\n",
    "\n",
    "def create_and_train_rf_seq2seq_multi(X_f_train, y_f_train, X_f_test, y_f_test, \n",
    "                                      X_b_train, y_b_train, X_b_test, y_b_test, \n",
    "                                      pre_context_length, post_context_length, gap_length, model_info=None):\n",
    "    \"\"\"\n",
    "    Trains a multivariate bidirectional RF Seq2Seq model.\n",
    "    \n",
    "    Parameters:\n",
    "      - X_f_train, y_f_train, X_f_test, y_f_test:\n",
    "          Data for training and testing the forward model.\n",
    "          X_f_train has the shape (samples, pre_context_length, n_features),\n",
    "          and y_f_train has (samples, gap_length, 1) (the target feature is taken from the first column).\n",
    "      - X_b_train, y_b_train, X_b_test, y_b_test:\n",
    "          Data for training and testing the backward model.\n",
    "          X_b_train has the shape (samples, post_context_length, n_features),\n",
    "          and y_b_train has (samples, gap_length, 1).\n",
    "      - pre_context_length: length of the left context.\n",
    "      - post_context_length: length of the right context.\n",
    "      - gap_length: length of the forecasted interval.\n",
    "      - model_info: dictionary with additional information (e.g., feature_cols).\n",
    "      \n",
    "    Returns:\n",
    "      - forward_model, backward_model: trained RandomForestRegressor models for forward and backward forecasting.\n",
    "    \"\"\"\n",
    "    # Convert forward data: (samples, timesteps, n_features) -> (samples, timesteps*n_features)\n",
    "    X_f_train_flat = X_f_train.reshape(X_f_train.shape[0], -1)\n",
    "    y_f_train_flat = y_f_train.reshape(y_f_train.shape[0], -1)\n",
    "    X_f_test_flat = X_f_test.reshape(X_f_test.shape[0], -1)\n",
    "    \n",
    "    # Same for backward data\n",
    "    X_b_train_flat = X_b_train.reshape(X_b_train.shape[0], -1)\n",
    "    y_b_train_flat = y_b_train.reshape(y_b_train.shape[0], -1)\n",
    "    X_b_test_flat = X_b_test.reshape(X_b_test.shape[0], -1)\n",
    "    \n",
    "    # Train the forward model\n",
    "    forward_model = RandomForestRegressor(n_estimators=N_ESTIMATORS, random_state=42)\n",
    "    forward_model.fit(X_f_train_flat, y_f_train_flat)\n",
    "    \n",
    "    # Train the backward model\n",
    "    backward_model = RandomForestRegressor(n_estimators=N_ESTIMATORS, random_state=42)\n",
    "    backward_model.fit(X_b_train_flat, y_b_train_flat)\n",
    "    \n",
    "    if model_info is not None:\n",
    "        forward_model.model_info = model_info\n",
    "        backward_model.model_info = model_info\n",
    "        \n",
    "    return forward_model, backward_model\n",
    "\n",
    "def create_and_train_uniautoreg_xgb_multi(X_f_train, y_train, X_f_test, y_test,\n",
    "                                          X_b_train, y_b_train, X_b_test, y_test_b,\n",
    "                                          pre_context_length, post_context_length, gap_length, model_info=None):\n",
    "    \"\"\"\n",
    "    Trains a multivariate unidirectional autoregressive XGB model (UniAR XGB Multi).\n",
    "    Only forward data is used.\n",
    "    \n",
    "    Parameters:\n",
    "      - X_f_train: array with shape (samples, pre_context_length, n_total_features) for forward data.\n",
    "      - y_train: target values with shape (samples, 1) (the target feature is taken from the first column of the original data).\n",
    "      - Other parameters are passed for compatibility.\n",
    "      - model_info: if passed and contains the key \"feature_cols\", then only these columns are selected from X_f_train and X_f_test.\n",
    "    \n",
    "    Returns:\n",
    "      - xgb_model: trained XGBRegressor model.\n",
    "      - None: for compatibility.\n",
    "    \"\"\"\n",
    "    if model_info is not None and \"feature_cols\" in model_info:\n",
    "        fc = model_info[\"feature_cols\"]\n",
    "        # Extract only the selected features along the last axis\n",
    "        X_f_train = X_f_train[:, :, fc]\n",
    "        X_f_test = X_f_test[:, :, fc]\n",
    "    X_train_flat = X_f_train.reshape(X_f_train.shape[0], -1)\n",
    "    y_train_flat = y_train.ravel()\n",
    "    \n",
    "    xgb_model = XGBRegressor(n_estimators=N_ESTIMATORS, random_state=42)\n",
    "    xgb_model.fit(X_train_flat, y_train_flat)\n",
    "    \n",
    "    if model_info is not None:\n",
    "        xgb_model.model_info = model_info\n",
    "        \n",
    "    return xgb_model, None\n",
    "\n",
    "def create_and_train_uniseq2seq_cnn_multi(X_f_train, y_train, X_f_test, y_test, \n",
    "                                          X_b_train, y_train_b, X_b_test, y_test_b, \n",
    "                                          pre_context_length, post_context_length, gap_length, model_info=None):\n",
    "    \"\"\"\n",
    "    Trains a multivariate unidirectional UniSeq2Seq CNN Multi model.\n",
    "    Only forward data is used, so arguments related to backward data are ignored.\n",
    "    \n",
    "    Parameters:\n",
    "      - X_f_train, y_train, X_f_test, y_test: data for the forward model.\n",
    "      - X_b_train, y_train_b, X_b_test, y_test_b: not used, but passed for compatibility.\n",
    "      - pre_context_length: context length.\n",
    "      - post_context_length: not used (can be set, but has no effect).\n",
    "      - gap_length: length of the forecasted interval.\n",
    "      - model_info: additional information (e.g., feature_cols).\n",
    "      \n",
    "    Returns:\n",
    "      - model: trained CNN model that outputs a forecast of length gap_length.\n",
    "      - None.\n",
    "    \"\"\"\n",
    "    # Determine the number of selected features using data from model_info (if provided)\n",
    "    if model_info is not None and \"feature_cols\" in model_info:\n",
    "        # Assume feature_cols is a list of indices\n",
    "        n_features = len(model_info[\"feature_cols\"])\n",
    "    else:\n",
    "        n_features = X_f_train.shape[2]  # if not specified, take all features\n",
    "\n",
    "    input_layer = Input(shape=(pre_context_length, n_features))\n",
    "    x = Conv1D(filters=32, kernel_size=3, activation='relu', padding='causal')(input_layer)\n",
    "    x = Conv1D(filters=32, kernel_size=3, activation='relu', padding='causal')(x)\n",
    "    x = Flatten()(x)\n",
    "    output_layer = Dense(gap_length)(x)\n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    model.compile(optimizer=Adam(), loss='mse')\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=PATIENCE, restore_best_weights=True)\n",
    "    model.fit(X_f_train, y_train.reshape(y_train.shape[0], gap_length),\n",
    "              epochs=EPOCHS, batch_size=BATCH_SIZE, \n",
    "              validation_data=(X_f_test, y_test.reshape(y_test.shape[0], gap_length)),\n",
    "              callbacks=[early_stopping], verbose=0)\n",
    "    if model_info:\n",
    "        model.model_info = model_info\n",
    "    return model, None\n",
    "\n",
    "\n",
    "def create_and_train_xgb_autoreg_multi(\n",
    "    X_f_train, y_f_train, X_f_test, y_f_test,\n",
    "    X_b_train, y_b_train, X_b_test, y_b_test,\n",
    "    pre_context_length, post_context_length, gap_length,\n",
    "    model_info=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Trains two XGBoost models (forward and backward) in a multivariate bidirectional\n",
    "    autoregression. It is assumed that X_f_train and X_b_train have the shape:\n",
    "      (n_samples, pre_context_length (or post_context_length), n_features),\n",
    "    where n_features = len(feature_cols).\n",
    "\n",
    "    Parameters:\n",
    "      - X_f_train, y_f_train, X_f_test, y_f_test: data for the forward model.\n",
    "      - X_b_train, y_b_train, X_b_test, y_b_test: data for the backward model.\n",
    "      - pre_context_length, post_context_length: context sizes on the left and right.\n",
    "      - gap_length: length of the gap (number of points to restore).\n",
    "      - model_info: for storing auxiliary information (e.g., feature_cols).\n",
    "\n",
    "    Returns:\n",
    "      - forward_model, backward_model: trained XGBRegressor models.\n",
    "    \"\"\"\n",
    "\n",
    "    # If a list of features feature_cols is given, extract them\n",
    "    if model_info is not None and \"feature_cols\" in model_info:\n",
    "        feature_cols = model_info[\"feature_cols\"]\n",
    "        # Keep only the required features along the last axis\n",
    "        X_f_train = X_f_train[:, :, feature_cols]  # (samples, pre_context_length, n_selected)\n",
    "        X_f_test  = X_f_test[:,  :, feature_cols]\n",
    "        X_b_train = X_b_train[:, :, feature_cols]  # (samples, post_context_length, n_selected)\n",
    "        X_b_test  = X_b_test[:,  :, feature_cols]\n",
    "\n",
    "    # Convert 3D (samples, timesteps, features) -> 2D (samples, timesteps * features)\n",
    "    X_f_train_flat = X_f_train.reshape(X_f_train.shape[0], -1)\n",
    "    X_f_test_flat  = X_f_test.reshape(X_f_test.shape[0],   -1)\n",
    "    y_f_train_flat = y_f_train.ravel()   # target variable (1D)\n",
    "    y_f_test_flat  = y_f_test.ravel()\n",
    "\n",
    "    X_b_train_flat = X_b_train.reshape(X_b_train.shape[0], -1)\n",
    "    X_b_test_flat  = X_b_test.reshape(X_b_test.shape[0],   -1)\n",
    "    y_b_train_flat = y_b_train.ravel()\n",
    "    y_b_test_flat  = y_b_test.ravel()\n",
    "\n",
    "    # Create and train the forward model\n",
    "    forward_model = XGBRegressor(n_estimators=50, random_state=42)\n",
    "    forward_model.fit(X_f_train_flat, y_f_train_flat)\n",
    "\n",
    "    # Create and train the backward model\n",
    "    backward_model = XGBRegressor(n_estimators=50, random_state=42)\n",
    "    backward_model.fit(X_b_train_flat, y_b_train_flat)\n",
    "\n",
    "    # Save feature_cols inside the models if desired\n",
    "    if model_info:\n",
    "        forward_model.model_info = model_info\n",
    "        backward_model.model_info = model_info\n",
    "\n",
    "    return forward_model, backward_model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Functions for direct Seq2Seq forecasting\n",
    "def direct_seq2seq_forecast_tcn(model, initial_context, steps):\n",
    "    input_seq = initial_context.reshape(1, initial_context.shape[0], 1)\n",
    "    prediction = model.predict(input_seq, verbose=0)\n",
    "    return prediction.reshape(-1, 1)\n",
    "\n",
    "def direct_seq2seq_forecast_lstm(model, initial_context, steps):\n",
    "    input_seq = initial_context.reshape(1, initial_context.shape[0], 1)\n",
    "    prediction = model.predict(input_seq, verbose=0)\n",
    "    return prediction.reshape(-1, 1)\n",
    "\n",
    "def direct_seq2seq_forecast_gru(model, initial_context, steps):\n",
    "    input_seq = initial_context.reshape(1, initial_context.shape[0], 1)\n",
    "    prediction = model.predict(input_seq, verbose=0)\n",
    "    return prediction.reshape(-1, 1)\n",
    "\n",
    "def direct_seq2seq_forecast_cnn(model, initial_context, steps):\n",
    "    input_seq = initial_context.reshape(1, initial_context.shape[0], 1)\n",
    "    prediction = model.predict(input_seq, verbose=0)\n",
    "    return prediction.reshape(-1, 1)\n",
    "\n",
    "def direct_seq2seq_forecast_rf(model, initial_context, steps):\n",
    "    input_flat = initial_context.reshape(1, -1)  # Convert 2D to 1D for RandomForest\n",
    "    prediction = model.predict(input_flat)\n",
    "    return prediction.reshape(-1, 1)\n",
    "\n",
    "def direct_seq2seq_forecast_xgb(model, initial_context, steps):\n",
    "    input_flat = initial_context.reshape(1, -1)  # Convert 2D to 1D for XGBoost\n",
    "    prediction = model.predict(input_flat)\n",
    "    return prediction.reshape(-1, 1)\n",
    "\n",
    "def direct_seq2seq_forecast_rnn(model, initial_context, steps):\n",
    "    input_seq = initial_context.reshape(1, initial_context.shape[0], 1)\n",
    "    prediction = model.predict(input_seq, verbose=0)\n",
    "    return prediction.reshape(-1, 1)\n",
    "\n",
    "# Functions for direct autoregressive forecasting\n",
    "def direct_autoreg_forecast_rnn(model, initial_context, gap_length):\n",
    "    predictions = []\n",
    "    current_context = initial_context.copy()\n",
    "    for _ in range(gap_length):\n",
    "        input_seq = current_context.reshape(1, current_context.shape[0], 1)\n",
    "        pred = model.predict(input_seq, verbose=0)[0][0]\n",
    "        predictions.append(pred)\n",
    "        current_context = np.roll(current_context, -1)\n",
    "        current_context[-1] = pred\n",
    "    return np.array(predictions).reshape(-1, 1)\n",
    "\n",
    "def direct_autoreg_forecast_lstm(model, initial_context, gap_length):\n",
    "    predictions = []\n",
    "    current_context = initial_context.copy()\n",
    "    for i in range(gap_length):\n",
    "        input_seq = current_context.reshape(1, current_context.shape[0], 1)\n",
    "        pred = model.predict(input_seq, verbose=0)[0][0]\n",
    "        predictions.append(pred)\n",
    "        current_context = np.roll(current_context, -1)\n",
    "        current_context[-1] = pred\n",
    "    return np.array(predictions).reshape(-1, 1)\n",
    "\n",
    "def direct_autoreg_forecast_gru(model, initial_context, gap_length):\n",
    "    predictions = []\n",
    "    current_context = initial_context.copy()\n",
    "    for _ in range(gap_length):\n",
    "        input_seq = current_context.reshape(1, current_context.shape[0], 1)\n",
    "        pred = model.predict(input_seq, verbose=0)[0][0]\n",
    "        predictions.append(pred)\n",
    "        current_context = np.roll(current_context, -1)\n",
    "        current_context[-1] = pred\n",
    "    return np.array(predictions).reshape(-1, 1)\n",
    "\n",
    "def direct_autoreg_forecast_cnn(model, initial_context, gap_length):\n",
    "    predictions = []\n",
    "    current_context = initial_context.copy()\n",
    "    for _ in range(gap_length):\n",
    "        input_seq = current_context.reshape(1, current_context.shape[0], 1)\n",
    "        pred = model.predict(input_seq, verbose=0)[0][0]\n",
    "        predictions.append(pred)\n",
    "        current_context = np.roll(current_context, -1)\n",
    "        current_context[-1] = pred\n",
    "    return np.array(predictions).reshape(-1, 1)\n",
    "\n",
    "def direct_autoreg_forecast_rf(model, initial_context, gap_length):\n",
    "    predictions = []\n",
    "    current_context = initial_context.copy()\n",
    "    for _ in range(gap_length):\n",
    "        input_flat = current_context.reshape(1, -1)  # Convert to 2D for RandomForest\n",
    "        pred = model.predict(input_flat)[0]\n",
    "        predictions.append(pred)\n",
    "        current_context = np.roll(current_context, -1)\n",
    "        current_context[-1] = pred\n",
    "    return np.array(predictions).reshape(-1, 1)\n",
    "\n",
    "def direct_autoreg_forecast_xgb(model, initial_context, gap_length):\n",
    "    predictions = []\n",
    "    current_context = initial_context.copy()\n",
    "    for _ in range(gap_length):\n",
    "        input_flat = current_context.reshape(1, -1)  # Convert to 2D for XGBoost\n",
    "        pred = model.predict(input_flat)[0]\n",
    "        predictions.append(pred)\n",
    "        current_context = np.roll(current_context, -1)\n",
    "        current_context[-1] = pred\n",
    "    return np.array(predictions).reshape(-1, 1)\n",
    "\n",
    "def direct_autoreg_forecast_tcn(model, initial_context, gap_length):\n",
    "    predictions = []\n",
    "    current_context = initial_context.copy()\n",
    "    for _ in range(gap_length):\n",
    "        input_seq = current_context.reshape(1, current_context.shape[0], 1)\n",
    "        pred = model.predict(input_seq, verbose=0)[0][0]\n",
    "        predictions.append(pred)\n",
    "        current_context = np.roll(current_context, -1)\n",
    "        current_context[-1] = pred\n",
    "    return np.array(predictions).reshape(-1, 1)\n",
    "\n",
    "# Forecasting function for UniSeq2Seq LSTM\n",
    "def direct_uniseq2seq_forecast_gru(model, initial_context, gap_length,\n",
    "                                   pre_context_length=32, post_context_length=32):\n",
    "    # Extract left and right chunks strictly according to the specified sizes:\n",
    "    pre_context = initial_context[:pre_context_length]    # (32, n_features)\n",
    "    post_context = initial_context[-post_context_length:] # (32, n_features)\n",
    "\n",
    "    # Add batch dimension: (1, pre_context_length, n_features)\n",
    "    input_left = pre_context.reshape(1, pre_context_length, pre_context.shape[1])\n",
    "    input_right = post_context.reshape(1, post_context_length, post_context.shape[1])\n",
    "\n",
    "    # Forecast\n",
    "    prediction = model.predict([input_left, input_right], verbose=0)\n",
    "    return prediction.reshape(-1, 1)\n",
    "\n",
    "# Forecasting function for UniSeq2Seq TCN\n",
    "def direct_uniseq2seq_forecast_tcn(model, initial_context, gap_length,\n",
    "                                   pre_context_length=32, post_context_length=32):\n",
    "    # Extract left and right chunks strictly according to the specified sizes:\n",
    "    pre_context = initial_context[:pre_context_length]    # (32, n_features)\n",
    "    post_context = initial_context[-post_context_length:] # (32, n_features)\n",
    "\n",
    "    # Add batch dimension: (1, pre_context_length, n_features)\n",
    "    input_left = pre_context.reshape(1, pre_context_length, pre_context.shape[1])\n",
    "    input_right = post_context.reshape(1, post_context_length, post_context.shape[1])\n",
    "\n",
    "    # Forecast\n",
    "    prediction = model.predict([input_left, input_right], verbose=0)\n",
    "    return prediction.reshape(-1, 1)\n",
    "\n",
    "# Forecasting function for UniSeq2Seq CNN\n",
    "def direct_uniseq2seq_forecast_cnn(model, initial_context, gap_length,\n",
    "                                   pre_context_length=32, post_context_length=32):\n",
    "# Extract left and right chunks strictly according to the specified sizes:\n",
    "    pre_context = initial_context[:pre_context_length]    # (32, n_features)\n",
    "    post_context = initial_context[-post_context_length:] # (32, n_features)\n",
    "\n",
    "    # Add batch dimension: (1, pre_context_length, n_features)\n",
    "    input_left = pre_context.reshape(1, pre_context_length, pre_context.shape[1])\n",
    "    input_right = post_context.reshape(1, post_context_length, post_context.shape[1])\n",
    "\n",
    "    prediction = model.predict([input_left, input_right], verbose=0)\n",
    "    return prediction.reshape(-1, 1)\n",
    "\n",
    "# Forecasting function for UniSeq2Seq RNN\n",
    "def direct_uniseq2seq_forecast_rnn(model, initial_context, gap_length,\n",
    "                                   pre_context_length=32, post_context_length=32):\n",
    "# Extract left and right chunks strictly according to the specified sizes:\n",
    "    pre_context = initial_context[:pre_context_length]    # (32, n_features)\n",
    "    post_context = initial_context[-post_context_length:] # (32, n_features)\n",
    "\n",
    "    # Add batch dimension: (1, pre_context_length, n_features)\n",
    "    input_left = pre_context.reshape(1, pre_context_length, pre_context.shape[1])\n",
    "    input_right = post_context.reshape(1, post_context_length, post_context.shape[1])\n",
    "\n",
    "    prediction = model.predict([input_left, input_right], verbose=0)\n",
    "    return prediction.reshape(-1, 1)\n",
    "\n",
    "# Forecasting function for UniSeq2Seq Random Forest\n",
    "def direct_uniseq2seq_forecast_rf(model, initial_context, gap_length):\n",
    "    # initial_context is the combined context (pre_context + post_context)\n",
    "    context_flat = initial_context.reshape(1, -1)  # Convert to a flat vector\n",
    "    prediction = model.predict(context_flat)\n",
    "    return prediction.reshape(-1, 1)\n",
    "\n",
    "# Forecasting function for UniSeq2Seq XGBoost\n",
    "def direct_uniseq2seq_forecast_xgb(model, initial_context, gap_length):\n",
    "    # initial_context is the combined context (pre_context + post_context)\n",
    "    context_flat = initial_context.reshape(1, -1)  # Convert to a flat vector\n",
    "    prediction = model.predict(context_flat)\n",
    "    return prediction.reshape(-1, 1)\n",
    "\n",
    "def direct_uniseq2seq_forecast_lstm(model, initial_context, gap_length,\n",
    "                                    pre_context_length=32, post_context_length=32):\n",
    "    \"\"\"\n",
    "    Forecast for the UniSeq2Seq LSTM model, which accepts [left_context, right_context].\n",
    "    initial_context has the shape (pre_context_length + post_context_length, n_features).\n",
    "    \"\"\"\n",
    "    # Extract left and right chunks strictly according to the specified sizes:\n",
    "    pre_context = initial_context[:pre_context_length]    # (32, n_features)\n",
    "    post_context = initial_context[-post_context_length:] # (32, n_features)\n",
    "\n",
    "    # Add batch dimension: (1, pre_context_length, n_features)\n",
    "    input_left = pre_context.reshape(1, pre_context_length, pre_context.shape[1])\n",
    "    input_right = post_context.reshape(1, post_context_length, post_context.shape[1])\n",
    "\n",
    "    # Forecast\n",
    "    prediction = model.predict([input_left, input_right], verbose=0)\n",
    "    return prediction.reshape(-1, 1)\n",
    "\n",
    "# Forecasting functions for UniAutoreg \n",
    "def direct_uniautoreg_forecast_lstm(model, initial_context, gap_length):\n",
    "    predictions = []\n",
    "    current_context = initial_context.copy()\n",
    "    for _ in range(gap_length):\n",
    "        input_seq = current_context.reshape(1, current_context.shape[0], 1)\n",
    "        pred = model.predict(input_seq, verbose=0)[0][0]\n",
    "        predictions.append(pred)\n",
    "        current_context = np.roll(current_context, -1)\n",
    "        current_context[-1] = pred\n",
    "    return np.array(predictions).reshape(-1, 1)\n",
    "\n",
    "def direct_uniautoreg_forecast_gru(model, initial_context, gap_length):\n",
    "    predictions = []\n",
    "    current_context = initial_context.copy()\n",
    "    for _ in range(gap_length):\n",
    "        input_seq = current_context.reshape(1, current_context.shape[0], 1)\n",
    "        pred = model.predict(input_seq, verbose=0)[0][0]\n",
    "        predictions.append(pred)\n",
    "        current_context = np.roll(current_context, -1)\n",
    "        current_context[-1] = pred\n",
    "    return np.array(predictions).reshape(-1, 1)\n",
    "\n",
    "def direct_uniautoreg_forecast_tcn(model, initial_context, gap_length):\n",
    "    predictions = []\n",
    "    current_context = initial_context.copy()\n",
    "    for _ in range(gap_length):\n",
    "        input_seq = current_context.reshape(1, current_context.shape[0], 1)\n",
    "        pred = model.predict(input_seq, verbose=0)[0][0]\n",
    "        predictions.append(pred)\n",
    "        current_context = np.roll(current_context, -1)\n",
    "        current_context[-1] = pred\n",
    "    return np.array(predictions).reshape(-1, 1)\n",
    "\n",
    "def direct_uniautoreg_forecast_cnn(model, initial_context, gap_length):\n",
    "    predictions = []\n",
    "    current_context = initial_context.copy()\n",
    "    for _ in range(gap_length):\n",
    "        input_seq = current_context.reshape(1, current_context.shape[0], 1)\n",
    "        pred = model.predict(input_seq, verbose=0)[0][0]\n",
    "        predictions.append(pred)\n",
    "        current_context = np.roll(current_context, -1)\n",
    "        current_context[-1] = pred\n",
    "    return np.array(predictions).reshape(-1, 1)\n",
    "\n",
    "def direct_uniautoreg_forecast_rnn(model, initial_context, gap_length):\n",
    "    predictions = []\n",
    "    current_context = initial_context.copy()\n",
    "    for _ in range(gap_length):\n",
    "        input_seq = current_context.reshape(1, current_context.shape[0], 1)\n",
    "        pred = model.predict(input_seq, verbose=0)[0][0]\n",
    "        predictions.append(pred)\n",
    "        current_context = np.roll(current_context, -1)\n",
    "        current_context[-1] = pred\n",
    "    return np.array(predictions).reshape(-1, 1)\n",
    "\n",
    "def direct_uniautoreg_forecast_rf(model, initial_context, gap_length):\n",
    "    predictions = []\n",
    "    current_context = initial_context.copy()\n",
    "    for _ in range(gap_length):\n",
    "        input_flat = current_context.reshape(1, -1)\n",
    "        pred = model.predict(input_flat)[0]\n",
    "        predictions.append(pred)\n",
    "        current_context = np.roll(current_context, -1)\n",
    "        current_context[-1] = pred\n",
    "    return np.array(predictions).reshape(-1, 1)\n",
    "\n",
    "def direct_uniautoreg_forecast_xgb(model, initial_context, gap_length):\n",
    "    predictions = []\n",
    "    current_context = initial_context.copy()\n",
    "    for _ in range(gap_length):\n",
    "        input_flat = current_context.reshape(1, -1)\n",
    "        pred = model.predict(input_flat)[0]\n",
    "        predictions.append(pred)\n",
    "        current_context = np.roll(current_context, -1)\n",
    "        current_context[-1] = pred\n",
    "    return np.array(predictions).reshape(-1, 1)\n",
    "\n",
    "# Forecasting functions for UniSeq2Seq LSTM (multi-factor)\n",
    "def direct_uniseq2seq_forecast_lstm_multi(\n",
    "    model, initial_context, gap_length,\n",
    "    pre_context_length=32, post_context_length=32\n",
    "):\n",
    "    \"\"\"\n",
    "    Forecasting for the multivariate unidirectional UniSeq2Seq LSTM Multi model.\n",
    "\n",
    "    :param model: trained model.\n",
    "    :param initial_context: combined array of size (pre_context_length + post_context_length, n_features).\n",
    "    :param gap_length: length of the forecasted interval.\n",
    "    :param pre_context_length: length of the left context (default 32).\n",
    "    :param post_context_length: length of the right context (default 32).\n",
    "\n",
    "    Returns an array of shape (gap_length, 1).\n",
    "    \"\"\"\n",
    "    # Check the shape of initial_context, it should be (32+32, n_features) if you want exactly 32 / 32.\n",
    "    left_part  = initial_context[:pre_context_length]      # (32, n_features)\n",
    "    right_part = initial_context[-post_context_length:]    # (32, n_features)\n",
    "\n",
    "    # Add batch dimension\n",
    "    left_part  = left_part.reshape(1, pre_context_length,  left_part.shape[1]) \n",
    "    right_part = right_part.reshape(1, post_context_length, right_part.shape[1])\n",
    "\n",
    "    prediction = model.predict([left_part, right_part], verbose=0)\n",
    "    return prediction.reshape(-1, 1)\n",
    "\n",
    "# Forecasting functions for UniSeq2Seq RF (multi-factor)\n",
    "def direct_uniseq2seq_forecast_rf_multi(model, initial_context, gap_length):\n",
    "    context_flat = initial_context.reshape(1, -1)\n",
    "    prediction = model.predict(context_flat)\n",
    "    return prediction.reshape(-1, 1)\n",
    "\n",
    "def direct_seq2seq_forecast_xgb_multi(model, initial_context, gap_length):\n",
    "    \"\"\"\n",
    "    Forecasting for multivariate XGB Seq2Seq.\n",
    "    initial_context is expected to be a combined context with shape (timesteps, n_features),\n",
    "    where n_features corresponds to the number of features selected via feature_cols.\n",
    "    The function converts this context into a 1D vector and returns the model's forecast.\n",
    "    \"\"\"\n",
    "    context_flat = initial_context.reshape(1, -1)\n",
    "    prediction = model.predict(context_flat)\n",
    "    return prediction.reshape(-1, 1)\n",
    "\n",
    "def direct_uniseq2seq_forecast_xgb(model, initial_context, gap_length):\n",
    "    \"\"\"\n",
    "    Forecasting for the multivariate unidirectional UniSeq2Seq XGB model.\n",
    "    initial_context is expected to be a combined context (e.g., concatenation of left and right contexts)\n",
    "    with shape (pre_context_length + post_context_length, n_features). The function converts it to a 1D vector\n",
    "    and returns the model's forecast as an array with shape (gap_length, 1).\n",
    "    \"\"\"\n",
    "    context_flat = initial_context.reshape(1, -1)\n",
    "    prediction = model.predict(context_flat)\n",
    "    return prediction.reshape(-1, 1)\n",
    "\n",
    "def direct_seq2seq_forecast_rf_multi(model, initial_context, gap_length):\n",
    "    \"\"\"\n",
    "    Forecasting for the multivariate bidirectional RF Seq2Seq model.\n",
    "    \n",
    "    initial_context is expected to be a combined context with shape \n",
    "    (timesteps, n_features), where timesteps equals the length of the input context (pre_context or post_context).\n",
    "    The function converts the context into a 1D vector and returns the model's forecast as an array of shape (gap_length, 1).\n",
    "    \"\"\"\n",
    "    context_flat = initial_context.reshape(1, -1)\n",
    "    prediction = model.predict(context_flat)\n",
    "    return prediction.reshape(-1, 1)\n",
    "\n",
    "def direct_uniautoreg_forecast_xgb_multi(model, initial_context, gap_length):\n",
    "    \"\"\"\n",
    "    Forecasting for the multivariate unidirectional autoregressive XGB model (UniAR XGB Multi).\n",
    "    \n",
    "    Parameters:\n",
    "      - model: trained XGBRegressor model.\n",
    "      - initial_context: current context with shape (pre_context_length, n_selected),\n",
    "                         where n_selected = len(feature_cols).\n",
    "      - gap_length: number of steps to forecast ahead.\n",
    "    \n",
    "    The function performs an autoregressive loop: at each step, the window is converted into a 1D vector,\n",
    "    a single value for the target feature is predicted, then the window is updated (shifted by one step)\n",
    "    and the predicted value for the first feature is inserted into the last row.\n",
    "    \n",
    "    Returns:\n",
    "      - An array of predictions of shape (gap_length, 1).\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    current_context = initial_context.copy()  # (pre_context_length, n_selected)\n",
    "    for _ in range(gap_length):\n",
    "        input_flat = current_context.reshape(1, -1)\n",
    "        pred = model.predict(input_flat)\n",
    "        pred = np.array(pred)\n",
    "        if pred.ndim > 1:\n",
    "            scalar_pred = pred[0, 0]\n",
    "        else:\n",
    "            scalar_pred = pred[0]\n",
    "        predictions.append(scalar_pred)\n",
    "        current_context = np.roll(current_context, -1, axis=0)\n",
    "        current_context[-1, 0] = scalar_pred\n",
    "    return np.array(predictions).reshape(-1, 1)\n",
    "\n",
    "def direct_uniseq2seq_forecast_cnn_multi(model, initial_context, gap_length):\n",
    "    \"\"\"\n",
    "    Forecasting for the multivariate unidirectional UniSeq2Seq CNN Multi model.\n",
    "    initial_context has the shape (pre_context_length, n_selected), where n_selected = len(feature_cols).\n",
    "    The function converts it to the format (1, pre_context_length, n_selected), gets the forecast, and returns an array of shape (gap_length, 1).\n",
    "    \"\"\"\n",
    "    input_data = initial_context.reshape(1, initial_context.shape[0], initial_context.shape[1])\n",
    "    prediction = model.predict(input_data, verbose=0)\n",
    "    return prediction.reshape(-1, 1)\n",
    "\n",
    "def direct_autoreg_forecast_xgb_multi(model, initial_context, gap_length):\n",
    "    \"\"\"\n",
    "    Forecasting for the multivariate bidirectional XGB autoregression (Forward or Backward model).\n",
    "    \n",
    "    Parameters:\n",
    "      - model: trained XGBRegressor model.\n",
    "      - initial_context: array of shape (context_length, n_selected),\n",
    "                         where n_selected = len(feature_cols).\n",
    "      - gap_length: number of consecutive forecasting steps.\n",
    "    \n",
    "    Mechanism:\n",
    "      - At each step, the current context (last context_length steps) is taken,\n",
    "        flattened into a vector, and fed into the XGB model to forecast \"+1 step\".\n",
    "      - The resulting prediction is inserted at the \"tail\" of the context (shifting the window forward).\n",
    "    Returns:\n",
    "      - An array of predictions of shape (gap_length, 1).\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    current_context = initial_context.copy()  # (context_length, n_selected)\n",
    "    for _ in range(gap_length):\n",
    "        # Convert window (context_length, n_selected) -> (1, context_length*n_selected)\n",
    "        input_flat = current_context.reshape(1, -1)\n",
    "        # Predict\n",
    "        pred = model.predict(input_flat)[0]\n",
    "        # Save\n",
    "        predictions.append(pred)\n",
    "        # Shift the window: the last row \"is removed\", and the predicted value for the target column [0] is inserted in its place\n",
    "        current_context = np.roll(current_context, -1, axis=0)\n",
    "        current_context[-1, 0] = pred  # only the first column (0) is the target feature\n",
    "    return np.array(predictions).reshape(-1, 1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def parse_model_indices(indices_str):\n",
    "    indices = set()\n",
    "    parts = indices_str.split(',')\n",
    "    for part in parts:\n",
    "        if '-' in part:\n",
    "            start, end = map(int, part.split('-'))\n",
    "            indices.update(range(start, end + 1))\n",
    "        else:\n",
    "            indices.add(int(part.strip()))\n",
    "    return sorted(indices)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df, data_scaled, scaler = load_and_preprocess_data()\n",
    "\n",
    "\n",
    "    # List of models with parameters: (name, training_function, forecasting_function, forecast_type, [window_size], [poly_degree])\n",
    "    models_to_add = [\n",
    "        # Simple Imputers 1-2\n",
    "        (\"Simple Imputer Mean\", None, None, \"simple\"),      #1\n",
    "        (\"Simple Imputer Median\", None, None, \"simple\"),    #2\n",
    "        \n",
    "        # Local Imputers 3-4\n",
    "        (\"Local Imputation Mean\", None, None, \"local\", None),   #3\n",
    "        (\"Local Imputation Median\", None, None, \"local\", None), #4\n",
    "        \n",
    "        # Window Interpolation Methods 5-8\n",
    "        (\"Linear Window Interpolation\", None, None, \"window\", 10),          #5\n",
    "        (\"Polynomial Window Interpolation\", None, None, \"window\", None, 3), #6\n",
    "        (\"B-spline Window Interpolation\", None, None, \"window\", None),      #7\n",
    "        (\"ARIMA Imputation\", None, None, \"window\", 100),                    #8\n",
    "        \n",
    "        # Unidirectional Autoregression Methods (UniAutoreg) 9-15\n",
    "        (\"UniAR LSTM\", create_and_train_uniautoreg_lstm, direct_uniautoreg_forecast_lstm, \"uniautoreg\"), #9\n",
    "        (\"UniAR RNN\", create_and_train_uniautoreg_rnn, direct_uniautoreg_forecast_rnn, \"uniautoreg\"),   #10\n",
    "        (\"UniAR CNN\", create_and_train_uniautoreg_cnn, direct_uniautoreg_forecast_cnn, \"uniautoreg\"),   #11\n",
    "        (\"UniAR RF\", create_and_train_uniautoreg_rf, direct_uniautoreg_forecast_rf, \"uniautoreg\"),      #12\n",
    "        (\"UniAR XGB\", create_and_train_uniautoreg_xgb, direct_uniautoreg_forecast_xgb, \"uniautoreg\"),   #13\n",
    "        (\"UniAR GRU\", create_and_train_uniautoreg_gru, direct_uniautoreg_forecast_gru, \"uniautoreg\"),   #14\n",
    "        (\"UniAR TCN\", create_and_train_uniautoreg_tcn, direct_uniautoreg_forecast_tcn, \"uniautoreg\"),   #15\n",
    "        \n",
    "        # Unidirectional seq2seq Methods (UniSeq2Seq) 16-22\n",
    "        (\"UniSeq2Seq RNN\", create_and_train_uniseq2seq_rnn, direct_uniseq2seq_forecast_rnn, \"uniseq2seq\"),      #16\n",
    "        (\"UniSeq2Seq CNN\", create_and_train_uniseq2seq_cnn, direct_uniseq2seq_forecast_cnn, \"uniseq2seq\"),      #17\n",
    "        (\"UniSeq2Seq RF\", create_and_train_uniseq2seq_rf, direct_uniseq2seq_forecast_rf, \"uniseq2seq\"),         #18\n",
    "        (\"UniSeq2Seq XGB\", create_and_train_uniseq2seq_xgb, direct_uniseq2seq_forecast_xgb, \"uniseq2seq\"),      #19\n",
    "        (\"UniSeq2Seq LSTM\", create_and_train_uniseq2seq_lstm, direct_uniseq2seq_forecast_lstm, \"uniseq2seq\"),   #20 \n",
    "        (\"UniSeq2Seq GRU\", create_and_train_uniseq2seq_gru, direct_uniseq2seq_forecast_gru, \"uniseq2seq\"),      #21\n",
    "        (\"UniSeq2Seq TCN\", create_and_train_uniseq2seq_tcn, direct_uniseq2seq_forecast_tcn, \"uniseq2seq\"),      #22\n",
    "        \n",
    "        # Bidirectional Autoregression Methods (autoreg) 23-29\n",
    "        (\"RNN Autoreg\", create_and_train_rnn_autoreg, direct_autoreg_forecast_rnn, \"autoreg\"),      #23\n",
    "        (\"CNN Autoreg\", create_and_train_cnn_autoreg, direct_autoreg_forecast_cnn, \"autoreg\"),      #24\n",
    "        (\"RF Autoreg\", create_and_train_rf_autoreg, direct_autoreg_forecast_rf, \"autoreg\"),         #25\n",
    "        (\"XGB Autoreg\", create_and_train_xgb_autoreg, direct_autoreg_forecast_xgb, \"autoreg\"),      #26\n",
    "        (\"LSTM Autoreg\", create_and_train_lstm_autoreg, direct_autoreg_forecast_lstm, \"autoreg\"),   #27\n",
    "        (\"GRU Autoreg\", create_and_train_gru_autoreg, direct_autoreg_forecast_gru, \"autoreg\"),      #28\n",
    "        (\"TCN Autoreg\", create_and_train_tcn_autoreg, direct_autoreg_forecast_tcn, \"autoreg\"),      #29\n",
    "        \n",
    "        # Bidirectional seq2seq Methods (Seq2Seq) 30-36\n",
    "        (\"RNN Seq2Seq\", create_and_train_rnn_seq2seq, direct_seq2seq_forecast_rnn, \"seq2seq\"),      #30\n",
    "        (\"CNN Seq2Seq\", create_and_train_cnn_seq2seq, direct_seq2seq_forecast_cnn, \"seq2seq\"),      #31\n",
    "        (\"RF Seq2Seq\", create_and_train_rf_seq2seq, direct_seq2seq_forecast_rf, \"seq2seq\"),         #32\n",
    "        (\"XGB Seq2Seq\", create_and_train_xgb_seq2seq, direct_seq2seq_forecast_xgb, \"seq2seq\"),      #33\n",
    "        (\"LSTM Seq2Seq\", create_and_train_lstm_seq2seq, direct_seq2seq_forecast_lstm, \"seq2seq\"),   #34\n",
    "        (\"GRU Seq2Seq\", create_and_train_gru_seq2seq, direct_seq2seq_forecast_gru, \"seq2seq\"),      #35\n",
    "        (\"TCN Seq2Seq\", create_and_train_tcn_seq2seq, direct_seq2seq_forecast_tcn, \"seq2seq\")       #36\n",
    "    ]\n",
    "\n",
    "    # Set the indices of the models to include\n",
    "    user_input = \"13, 17-20, 26, 32, 33\"\n",
    "    # user_input = \"1-36\"\n",
    "    model_indices = parse_model_indices(user_input)\n",
    "\n",
    "    print(\"Models to add:\", sorted(model_indices))\n",
    "\n",
    "    combiner = ImputationCombiner(df, data_scaled, scaler)\n",
    "\n",
    "    # Add models by index\n",
    "    for idx in sorted(model_indices):\n",
    "        if idx - 1 < len(models_to_add):\n",
    "            tup = models_to_add[idx - 1]\n",
    "            model_name = tup[0]\n",
    "            train_func = tup[1]\n",
    "            forecast_func = tup[2]\n",
    "            forecast_type = tup[3]\n",
    "            feature_cols = tup[4] if len(tup) > 4 else [0]\n",
    "\n",
    "            combiner.add_model(\n",
    "                model_name,\n",
    "                model_func=train_func,\n",
    "                forecast_func=forecast_func,\n",
    "                forecast_type=forecast_type,\n",
    "                feature_cols=feature_cols\n",
    "            )\n",
    "            print(f\"Added model [{idx}]: {model_name}\")\n",
    "\n",
    "    # Start testing\n",
    "    combiner.run_tests(gap_lengths=[5, 12, 24, 48, 72], n_runs=5)\n",
    "    combiner.summarize_results(visualize=True, output_dir=\"plots_uni\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    df, data_scaled, scaler = load_and_preprocess_data(multivariate=True)\n",
    "    print(\"Shape of data_scaled for multivariate:\", data_scaled.shape)\n",
    "    print(\"Type of data_scaled:\", type(data_scaled))\n",
    "\n",
    "    combiner_multi = ImputationCombiner(df, data_scaled, scaler)\n",
    "\n",
    "    # numeric_cols = ['pm2_5', 'air_temperature', 'air_humidity', 'T', 'P0', 'P', 'U', 'DD', 'Ff', 'VV', 'pm2_5_original', 'hour', 'season']\n",
    "        \n",
    "    models_to_add = [\n",
    "        (\"UniSeq2Seq LSTM Multi\", create_and_train_uniseq2seq_lstm_multi, direct_uniseq2seq_forecast_lstm_multi, \"uniseq2seq\", [0, 1, 2, 7, 8, 11, 12]),  #1\n",
    "        (\"UniSeq2Seq RF Multi\", create_and_train_uniseq2seq_rf_multi, direct_uniseq2seq_forecast_rf_multi, \"uniseq2seq\", [0, 1, 2, 7, 8, 11, 12]),        #2\n",
    "        (\"XGB Seq2Seq Multi\", create_and_train_xgb_seq2seq_multi, direct_seq2seq_forecast_xgb_multi, \"seq2seq\", [0, 1, 2, 7, 8, 11, 12]),                 #3\n",
    "        (\"UniSeq2Seq XGB Multi\", create_and_train_uniseq2seq_xgb, direct_uniseq2seq_forecast_xgb, \"uniseq2seq\", [0, 1, 2, 7, 8, 11, 12]),                 #4\n",
    "        (\"Seq2Seq RF Multi\", create_and_train_rf_seq2seq_multi, direct_seq2seq_forecast_rf_multi, \"seq2seq\", [0, 1, 2, 7, 8, 11, 12]),                    #5\n",
    "        (\"UniAR XGB Multi\", create_and_train_uniautoreg_xgb_multi, direct_uniautoreg_forecast_xgb_multi, \"uniautoreg\", [0, 1, 2, 7, 8, 11, 12]),          #6\n",
    "        (\"XGB Autoreg Multi\", create_and_train_uniautoreg_xgb_multi, direct_uniautoreg_forecast_xgb_multi, \"uniautoreg\", [0, 1, 2, 7, 8, 11, 12]),        #7\n",
    "        (\"UniSeq2Seq CNN Multi\", create_and_train_uniseq2seq_cnn_multi, direct_uniseq2seq_forecast_cnn_multi, \"uniseq2seq\", [0, 1, 2, 7, 8, 11, 12])      #8\n",
    "    ]\n",
    "    \n",
    "    # Set model numbers, for example, \"1-3,5,7-8\"\n",
    "    user_input = \"1-8\"  # Example input, can be changed to any\n",
    "    \n",
    "    selected_indices = parse_model_indices(user_input)\n",
    "    print(\"Selected models:\", selected_indices)\n",
    "\n",
    "    for idx, (model_name, train_func, forecast_func, forecast_type, feature_cols) in enumerate(models_to_add, start=1):\n",
    "        if idx in selected_indices:\n",
    "            combiner_multi.add_model(\n",
    "                model_name,\n",
    "                train_func,\n",
    "                forecast_func,\n",
    "                forecast_type=forecast_type,\n",
    "                feature_cols=feature_cols\n",
    "            )\n",
    "\n",
    "    combiner_multi.run_tests(gap_lengths=[5, 12, 24, 48, 72], n_runs=5)\n",
    "    combiner_multi.summarize_results(visualize=True, output_dir=\"plots_multi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[\"pm2_5\"].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Sample data from Tables 1 and 2\n",
    "gap_lengths = [5, 12, 24, 48, 72]\n",
    "mae_uni_seq2seq_xgb = [5.286, 7.279, 6.969, 7.030, 10.144]\n",
    "mae_multi_seq2seq_xgb = [5.180, 6.764, 6.537, 5.692, 9.177]\n",
    "mae_uni_xgb_seq2seq = [5.555, 7.679, 7.188, 6.971, 10.624]\n",
    "mae_multi_xgb_seq2seq = [5.507, 7.009, 6.697, 5.802, 8.528]\n",
    "mae_std_uni_seq2seq_xgb = [1.616, 2.199, 3.111, 3.987, 10.943]\n",
    "mae_std_multi_seq2seq_xgb = [1.500, 1.779, 3.142, 4.018, 12.750]\n",
    "mae_std_uni_xgb_seq2seq = [1.480, 2.179, 2.557, 3.126, 10.059]\n",
    "mae_std_multi_xgb_seq2seq = [1.798, 1.724, 3.009, 3.942, 10.796]\n",
    "\n",
    "# Calculate percentage reduction\n",
    "percent_reduction_seq2seq_xgb = [(u - m) / u * 100 for u, m in zip(mae_uni_seq2seq_xgb, mae_multi_seq2seq_xgb)]\n",
    "percent_reduction_xgb_seq2seq = [(u - m) / u * 100 for u, m in zip(mae_uni_xgb_seq2seq, mae_multi_xgb_seq2seq)]\n",
    "\n",
    "# Create bar chart\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "bar_width = 0.2\n",
    "index = np.arange(len(gap_lengths)) * 1.1  # Increased spacing between gap length groups\n",
    "\n",
    "# Plot bars with smaller offset within model types and larger offset between gap lengths\n",
    "bars1 = ax.bar([i - 0.35 for i in index], mae_uni_seq2seq_xgb, bar_width, label='UniSeq2Seq XGB', color='blue', yerr=mae_std_uni_seq2seq_xgb, capsize=3, ecolor=\"gray\")\n",
    "bars2 = ax.bar([i - 0.15 for i in index], mae_multi_seq2seq_xgb, bar_width, label='UniSeq2Seq XGB Multi', color='green', yerr=mae_std_multi_seq2seq_xgb, capsize=3, ecolor=\"gray\")\n",
    "bars3 = ax.bar([i + 0.15 for i in index], mae_uni_xgb_seq2seq, bar_width, label='XGB Seq2Seq', color='orange', yerr=mae_std_uni_xgb_seq2seq, capsize=3, ecolor=\"gray\")\n",
    "bars4 = ax.bar([i + 0.35 for i in index], mae_multi_xgb_seq2seq, bar_width, label='XGB Seq2Seq Multi', color='red', yerr=mae_std_multi_xgb_seq2seq, capsize=3, ecolor=\"gray\")\n",
    "\n",
    "# Add percentage reduction labels\n",
    "z = [-0.15, 0.95, 2.07, 3.19, 4.27]\n",
    "for i, (pct1, pct2) in enumerate(zip(percent_reduction_seq2seq_xgb, percent_reduction_xgb_seq2seq)):\n",
    "    \n",
    "    ax.text(z[i], max(mae_multi_seq2seq_xgb[i], mae_uni_seq2seq_xgb[i]) + 3, f'â–¼ {pct1:.1f}%', ha='center', rotation=0)\n",
    "    ax.text(z[i] + 0.5, max(mae_multi_xgb_seq2seq[i], mae_uni_xgb_seq2seq[i]) + 3, f'â–¼ {pct2:.1f}%', ha='center', rotation=0)\n",
    "\n",
    "\n",
    "# Customize plot\n",
    "ax.set_xlabel('Gap Length (hours)')\n",
    "ax.set_ylabel('MAE (Âµg/mÂ³)')\n",
    "# ax.set_title('MAE Comparison: Four Corresponding Models')\n",
    "ax.set_xticks(index)\n",
    "ax.set_xticklabels(gap_lengths)\n",
    "ax.grid(True, linestyle='--', alpha=0.7)\n",
    "ax.legend(loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(\"output_diagrams\", \"mae_comparison_uni_multi.png\"), dpi=600)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env_new (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
