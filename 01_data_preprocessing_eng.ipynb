{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Extraction from Database\n",
    "Fill short gaps (e.g., less than 5 minutes) using linear interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "# Station code name mapping\n",
    "station_mapping = {\n",
    "    \"16098828\": \"app_center\",\n",
    "    \"16101230\": \"app_2pavlodar\",\n",
    "    \"16101231\": \"app_pspu\",\n",
    "    \"16101232\": \"app_metallurg\",\n",
    "    \"16101233\": \"app_zaton\"\n",
    "}\n",
    "\n",
    "# ====================== Helper Functions ======================\n",
    "\n",
    "def prepare_station_data(df, station_name, tz='Asia/Tashkent', current=True):\n",
    "    \"\"\"\n",
    "    Prepares data for selected station:\n",
    "      - Converts dates and adds additional time columns,\n",
    "      - Applies station mapping,\n",
    "      - Filters data by station,\n",
    "      - Localizes timestamps and, if current=True, updates last_record_date to current time.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df['year']  = df['date'].dt.year\n",
    "    df['month'] = df['date'].dt.month\n",
    "    df['day']   = df['date'].dt.day\n",
    "    df['hour']  = df['date'].dt.hour\n",
    "    df['minute']= df['date'].dt.minute\n",
    "    df['station_name'] = df['app_id'].astype(str).map(station_mapping)\n",
    "    station_data = df[df['station_name'] == station_name]\n",
    "    \n",
    "    first_record_date = station_data['date'].min()\n",
    "    if first_record_date.tz is None:\n",
    "        first_record_date = first_record_date.tz_localize(tz)\n",
    "    last_record_date = station_data['date'].max()\n",
    "    if last_record_date.tz is None:\n",
    "        last_record_date = last_record_date.tz_localize(tz)\n",
    "    \n",
    "    if current:\n",
    "        analysis_end = pd.Timestamp.now(tz=tz).floor('min')\n",
    "        if analysis_end > last_record_date:\n",
    "            last_record_date = analysis_end\n",
    "\n",
    "    return station_data, first_record_date, last_record_date\n",
    "\n",
    "def get_months_ordered(first_record_date, last_record_date):\n",
    "    \"\"\"\n",
    "    Returns a list of months as datetime, starting from the first month (with adjustment if date is not 1st)\n",
    "    to the first day of the month following last_record_date.\n",
    "    \"\"\"\n",
    "    end_for_range = (last_record_date + pd.offsets.MonthBegin()).replace(day=1)\n",
    "    months_range = pd.date_range(start=first_record_date.replace(day=1),\n",
    "                                 end=end_for_range, freq=\"MS\").to_pydatetime().tolist()\n",
    "    if first_record_date.day != 1:\n",
    "        months_range[0] = first_record_date\n",
    "    return months_range\n",
    "\n",
    "def plot_month_fill_common(ax, station_name, month_data, month_name, total_minutes_in_month, first_record_date, last_record_date, parameter=None):\n",
    "    \"\"\"\n",
    "    Draws heatmap of data completeness for a month.\n",
    "      - If parameter is None, considers presence of records,\n",
    "      - Otherwise, if parameter value is not NaN – considers data present.\n",
    "    \"\"\"\n",
    "    days_in_month = pd.Timestamp(month_name).days_in_month\n",
    "    day_minutes = np.full((days_in_month, 24 * 60), 2)  # 2 = outside measurement period (white)\n",
    "    tz = last_record_date.tz\n",
    "    month_start = pd.Timestamp(f\"{month_name}-01\", tz=tz)\n",
    "    month_end   = pd.Timestamp(f\"{month_name}-{days_in_month}\", tz=tz) + pd.Timedelta(hours=23, minutes=59)\n",
    "    \n",
    "    if first_record_date and last_record_date:\n",
    "        if month_end < first_record_date or month_start > last_record_date:\n",
    "            completeness_percentage = 0\n",
    "        else:\n",
    "            for _, row in month_data.iterrows():\n",
    "                day_index = row['day'] - 1\n",
    "                minute_index = row['hour'] * 60 + row['minute']\n",
    "                if parameter is None:\n",
    "                    day_minutes[day_index, minute_index] = 1\n",
    "                else:\n",
    "                    if pd.notna(row[parameter]):\n",
    "                        day_minutes[day_index, minute_index] = 1\n",
    "\n",
    "            for day in range(days_in_month):\n",
    "                for minute in range(24 * 60):\n",
    "                    current_datetime = pd.Timestamp(f\"{month_name}-{day+1}\", tz=tz) + pd.Timedelta(minutes=minute)\n",
    "                    if first_record_date <= current_datetime <= last_record_date:\n",
    "                        if day_minutes[day, minute] == 2:\n",
    "                            day_minutes[day, minute] = 0\n",
    "\n",
    "            completeness_percentage = np.sum(day_minutes == 1) / total_minutes_in_month * 100\n",
    "    else:\n",
    "        completeness_percentage = 0\n",
    "\n",
    "    cmap = mcolors.ListedColormap([\"red\", \"green\", \"white\"])\n",
    "    bounds = [0, 1, 2, 3]\n",
    "    norm = mcolors.BoundaryNorm(bounds, cmap.N)\n",
    "    \n",
    "    ax.imshow(day_minutes, aspect='auto', cmap=cmap, norm=norm, interpolation='nearest')\n",
    "    title_text = f\"Data Completeness\" if parameter is None else f\"Completeness {parameter}\"\n",
    "    ax.set_title(f\"{month_name} ({station_name})\\n{title_text}: {completeness_percentage:.1f}%\", fontsize=10)\n",
    "    ax.set_xlabel(\"Hours of day\")\n",
    "    ax.set_ylabel(\"Day of month\")\n",
    "    ax.set_xticks(range(0, 24 * 60, 60))\n",
    "    ax.set_xticklabels(range(0, 24))\n",
    "    days_labels = [str(day) for day in range(1, days_in_month + 1)]\n",
    "    ax.set_yticks(range(0, days_in_month))\n",
    "    ax.set_yticklabels(days_labels)\n",
    "\n",
    "# ============== Main Plotting Functions ==============\n",
    "\n",
    "def plot_data_completeness(df, station_name, current=True):\n",
    "    station_data, first_record_date, last_record_date = prepare_station_data(df, station_name, current=current)\n",
    "    months_ordered = get_months_ordered(first_record_date, last_record_date)\n",
    "    \n",
    "    fig, axes = plt.subplots(nrows=3, ncols=4, figsize=(20, 15))\n",
    "    axes = axes.flatten()\n",
    "    for i, month_datetime in enumerate(months_ordered):\n",
    "        ax = axes[i]\n",
    "        month_data = station_data[\n",
    "            (station_data['year'] == month_datetime.year) &\n",
    "            (station_data['month'] == month_datetime.month)\n",
    "        ]\n",
    "        days_in_month = pd.Timestamp(month_datetime).days_in_month\n",
    "        total_minutes = days_in_month * 24 * 60\n",
    "        month_name = month_datetime.strftime(\"%Y-%m\")\n",
    "        plot_month_fill_common(ax, station_name, month_data, month_name, total_minutes, first_record_date, last_record_date, parameter=None)\n",
    "    for ax in axes[len(months_ordered):]:\n",
    "        ax.axis(\"off\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle(f\"Data Completeness for: {station_name}\", fontsize=20)\n",
    "    plt.subplots_adjust(top=0.9)\n",
    "    plt.show()\n",
    "\n",
    "def plot_param_completeness(df, station_name, parameter, current=True):\n",
    "    station_data, first_record_date, last_record_date = prepare_station_data(df, station_name, current=current)\n",
    "    months_ordered = get_months_ordered(first_record_date, last_record_date)\n",
    "    \n",
    "    fig, axes = plt.subplots(nrows=3, ncols=4, figsize=(20, 15))\n",
    "    axes = axes.flatten()\n",
    "    for i, month_datetime in enumerate(months_ordered):\n",
    "        ax = axes[i]\n",
    "        month_data = station_data[\n",
    "            (station_data['year'] == month_datetime.year) &\n",
    "            (station_data['month'] == month_datetime.month)\n",
    "        ]\n",
    "        days_in_month = pd.Timestamp(month_datetime).days_in_month\n",
    "        total_minutes = days_in_month * 24 * 60\n",
    "        month_name = month_datetime.strftime(\"%Y-%m\")\n",
    "        plot_month_fill_common(ax, station_name, month_data, month_name, total_minutes, first_record_date, last_record_date, parameter=parameter)\n",
    "    for ax in axes[len(months_ordered):]:\n",
    "        ax.axis(\"off\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle(f\"Completeness of {parameter} for station: {station_name}\", fontsize=20)\n",
    "    plt.subplots_adjust(top=0.9)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database connection\n",
    "db_path = \"air_quality_data.sqlite\"\n",
    "conn = sqlite3.connect(db_path)\n",
    "\n",
    "# Set station\n",
    "station = \"16101231\"\n",
    "\n",
    "# Set dates with time.\n",
    "\n",
    "# ---Research dates---\n",
    "start_date = \"2024-05-23 00:00:00\"  # Or False for automatic substitution\n",
    "end_date = \"2025-01-19 23:59:59\"      # Or False for automatic substitution\n",
    "# ----------------------------\n",
    "\n",
    "if start_date is False or end_date is False:\n",
    "    if start_date is False:\n",
    "        query_min = f\"SELECT MIN(date) as min_date FROM air_quality WHERE app_id = {station}\"\n",
    "        start_date = pd.read_sql_query(query_min, conn).iloc[0]['min_date']\n",
    "    if end_date is False:\n",
    "        query_max = f\"SELECT MAX(date) as max_date FROM air_quality WHERE app_id = {station}\"\n",
    "        end_date = pd.read_sql_query(query_max, conn).iloc[0]['max_date']\n",
    "\n",
    "query = f\"\"\"\n",
    "SELECT app_id, date, air_temperature, air_humidity, pm2_5 \n",
    "FROM air_quality \n",
    "WHERE app_id = {station} \n",
    "  AND date BETWEEN '{start_date}' AND '{end_date}'\n",
    "ORDER BY date\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_sql_query(query, conn)\n",
    "conn.close()\n",
    "\n",
    "print(f\"Number of rows in dataframe: {len(df)}\")\n",
    "df['date'] = pd.to_datetime(df['date']).dt.floor('min')\n",
    "# print(df.head())\n",
    "print(df.tail())\n",
    "\n",
    "# ====================== Function Call Examples ======================\n",
    "# plot_data_completeness(df, station_mapping[station], current=False)\n",
    "plot_param_completeness(df, station_mapping[station], \"pm2_5\", current=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Outlier Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns)\n",
    "\n",
    "df_init = df.copy()  # Create a copy to avoid modifying the original dataset\n",
    "\n",
    "nan_rows_ = df_init[df_init[\"pm2_5\"].isna()]\n",
    "print(nan_rows_.head())\n",
    "\n",
    "df_init = df_init.dropna(subset=[\"pm2_5\"])  # Remove rows where pm2_5 == NaN\n",
    "\n",
    "nan_rows_ = df_init[df_init[\"pm2_5\"].isna()]\n",
    "print(nan_rows_.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "\n",
    "# Function for primary outlier detection\n",
    "def primary_outlier_detection(df, column=\"pm2_5\"):\n",
    "    df[\"is_primary_outlier\"] = (df[column] > 200) & (df[column] > 3 * df[column].shift(1))\n",
    "    return df\n",
    "\n",
    "# Function for outlier detection using IQR method\n",
    "def detect_outliers_iqr(df, column=\"pm2_5\"):\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    lower_bound = Q1 - 10 * IQR\n",
    "    upper_bound = Q3 + 10 * IQR\n",
    "\n",
    "    # Outlier mask\n",
    "    df[\"is_outlier\"] = (df[column] < lower_bound) | (df[column] > upper_bound)\n",
    "\n",
    "    return df, lower_bound, upper_bound\n",
    "\n",
    "# Apply primary outlier detection method to data\n",
    "df_init = primary_outlier_detection(df_init, column=\"pm2_5\")\n",
    "\n",
    "# Visualize primary outliers\n",
    "fig = go.Figure()\n",
    "\n",
    "# Line with all values\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=df_init[\"date\"], \n",
    "    y=df_init[\"pm2_5\"],\n",
    "    mode=\"lines\",\n",
    "    # name=\"All data\",\n",
    "    line=dict(color=\"blue\")\n",
    "))\n",
    "\n",
    "# Primary outlier points\n",
    "primary_outliers = df_init[df_init[\"is_primary_outlier\"]]\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=primary_outliers[\"date\"], \n",
    "    y=primary_outliers[\"pm2_5\"],\n",
    "    mode=\"markers\",\n",
    "    # name=\"Primary outliers\",\n",
    "    marker=dict(color=\"red\", size=6, symbol=\"circle-open\")\n",
    "))\n",
    "\n",
    "# Configure axes and display\n",
    "fig.update_layout(\n",
    "    # title=\"Interactive PM2.5 plot with primary outliers\",\n",
    "    xaxis_title=\"Datetime\",\n",
    "    yaxis_title=\"PM2.5 (µg/m³)\",\n",
    "    xaxis=dict(rangeslider=dict(visible=True), type=\"date\"),\n",
    "    legend=dict(x=0, y=1)\n",
    ")\n",
    "\n",
    "# Display plot\n",
    "fig.show()\n",
    "\n",
    "# Remove primary outliers\n",
    "df_no_primary_outliers = df_init[~df_init[\"is_primary_outlier\"]]\n",
    "\n",
    "# Visualize data without primary outliers\n",
    "fig = go.Figure()\n",
    "\n",
    "# Line without primary outliers\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=df_no_primary_outliers[\"date\"], \n",
    "    y=df_no_primary_outliers[\"pm2_5\"],\n",
    "    mode=\"lines\",\n",
    "    # name=\"Data without primary outliers\",\n",
    "    line=dict(color=\"blue\")\n",
    "))\n",
    "\n",
    "# Configure axes and display\n",
    "fig.update_layout(\n",
    "    # title=\"Interactive PM2.5 plot without primary outliers\",\n",
    "    xaxis_title=\"Datetime\",\n",
    "    yaxis_title=\"PM2.5 (µg/m³)\",\n",
    "    xaxis=dict(rangeslider=dict(visible=True), type=\"date\"),\n",
    "    legend=dict(x=0, y=1)\n",
    ")\n",
    "\n",
    "# Display plot\n",
    "fig.show()\n",
    "\n",
    "# Apply IQR method to data without primary outliers\n",
    "df_no_primary_outliers, lower_bound, upper_bound = detect_outliers_iqr(df_no_primary_outliers, column=\"pm2_5\")\n",
    "\n",
    "# Visualize outliers after IQR method\n",
    "fig = go.Figure()\n",
    "\n",
    "# Line with all values after primary screening\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=df_no_primary_outliers[\"date\"], \n",
    "    y=df_no_primary_outliers[\"pm2_5\"],\n",
    "    mode=\"lines\",\n",
    "    name=\"All data\",\n",
    "    line=dict(color=\"blue\")\n",
    "))\n",
    "\n",
    "# Outlier points after IQR method\n",
    "iqr_outliers = df_no_primary_outliers[df_no_primary_outliers[\"is_outlier\"]]\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=iqr_outliers[\"date\"], \n",
    "    y=iqr_outliers[\"pm2_5\"],\n",
    "    mode=\"markers\",\n",
    "    name=\"Outliers (after IQR)\",\n",
    "    marker=dict(color=\"red\", size=6, symbol=\"circle-open\")\n",
    "))\n",
    "\n",
    "# Configure axes and display\n",
    "fig.update_layout(\n",
    "    # title=\"Interactive PM2.5 plot with outliers (IQR method)\",\n",
    "    xaxis_title=\"Datetime\",\n",
    "    yaxis_title=\"PM2.5 (µg/m³)\",\n",
    "    xaxis=dict(rangeslider=dict(visible=False), type=\"date\"),\n",
    "    legend=dict(\n",
    "        x=1,\n",
    "        y=1,\n",
    "        xanchor=\"right\",\n",
    "        yanchor=\"top\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Display plot\n",
    "fig.show()\n",
    "\n",
    "# Save plot in high resolution\n",
    "# pio.write_image(fig, 'high_res_plot.png', width=1000, height=600, scale=6)\n",
    "\n",
    "# Remove outliers after IQR method\n",
    "df_no_outliers = df_no_primary_outliers[~df_no_primary_outliers[\"is_outlier\"]]\n",
    "\n",
    "# Visualize data without outliers after IQR method\n",
    "fig = go.Figure()\n",
    "\n",
    "# Line without outliers after IQR method\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=df_no_outliers[\"date\"], \n",
    "    y=df_no_outliers[\"pm2_5\"],\n",
    "    mode=\"lines\",\n",
    "    # name=\"Data without outliers (IQR method)\",\n",
    "    line=dict(color=\"blue\")\n",
    "))\n",
    "\n",
    "# Configure axes and display\n",
    "fig.update_layout(\n",
    "    # title=\"Interactive PM2.5 plot without outliers (IQR method)\",\n",
    "    xaxis_title=\"Datetime\",\n",
    "    yaxis_title=\"PM2.5 (µg/m³)\",\n",
    "    xaxis=dict(rangeslider=dict(visible=False), type=\"date\"),\n",
    "    legend=dict(x=0, y=1)\n",
    ")\n",
    "\n",
    "# Display plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace outliers with NaN instead of removing rows\n",
    "df_cleaned = df_no_outliers.copy()\n",
    "df_cleaned.loc[df_cleaned[\"is_outlier\"], \"pm2_5\"] = np.nan\n",
    "df_cleaned.loc[df_cleaned[\"is_primary_outlier\"], \"pm2_5\"] = np.nan\n",
    "# Remove helper column \"is_outlier\" if no longer needed\n",
    "df_cleaned = df_cleaned.drop(columns=[\"is_outlier\"])\n",
    "df_cleaned = df_cleaned.drop(columns=[\"is_primary_outlier\"])\n",
    "\n",
    "# Output number of rows before and after outlier processing\n",
    "print(f\"Number of rows BEFORE outlier processing: {len(df_no_outliers)}\")\n",
    "print(f\"Number of rows AFTER outlier processing (should be the same): {len(df_cleaned)}\")\n",
    "\n",
    "# Check that outliers were replaced with NaN\n",
    "print(df_cleaned.head())\n",
    "print(f\"Number of NaN in 'pm2_5' after outlier replacement: {df_cleaned['pm2_5'].isna().sum()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fill Missing Minutes and Round Time to Minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fill_missing_minutes(df, time_column=\"date\", station_column=\"app_id\", parameters=None):\n",
    "    \"\"\"\n",
    "    Fills missing minutes in data, adding them with NaN values for parameters.\n",
    "    Automatically adds columns year, month, day, hour, minute and station_name.\n",
    "\n",
    "    :param df: Original DataFrame.\n",
    "    :param time_column: Name of date and time column.\n",
    "    :param station_column: Name of station identifier column (e.g., \"app_id\").\n",
    "    :param parameters: List of parameters to fill (if None, uses all numeric columns).\n",
    "    :return: DataFrame with filled minutes.\n",
    "    \"\"\"\n",
    "    df = df.copy()  # Avoid modifying the original dataframe\n",
    "    df[time_column] = pd.to_datetime(df[time_column])  # Convert time to datetime\n",
    "    df[station_column] = df[station_column].astype(str)  # Convert \"app_id\" to string\n",
    "\n",
    "    if parameters is None:\n",
    "        parameters = df.select_dtypes(include=[float, int]).columns.tolist()\n",
    "\n",
    "    # Determine minimum and maximum date in data\n",
    "    min_date, max_date = df[time_column].min(), df[time_column].max()\n",
    "\n",
    "    # Create full time range with 1-minute step\n",
    "    full_time_index = pd.date_range(start=min_date, end=max_date, freq=\"min\")\n",
    "\n",
    "    # Create new dataframe with all stations and full time index\n",
    "    all_stations = df[station_column].unique()  # Take unique app_id (already strings)\n",
    "    full_index = pd.MultiIndex.from_product([all_stations, full_time_index], names=[station_column, time_column])\n",
    "    filled_df = pd.DataFrame(index=full_index).reset_index()\n",
    "\n",
    "    # Merge with original data, filling missing rows with NaN\n",
    "    df = pd.merge(filled_df, df, on=[station_column, time_column], how=\"left\")\n",
    "\n",
    "    # **Automatically add time components**\n",
    "    df[\"year\"] = df[time_column].dt.year\n",
    "    df[\"month\"] = df[time_column].dt.month\n",
    "    df[\"day\"] = df[time_column].dt.day\n",
    "    df[\"hour\"] = df[time_column].dt.hour\n",
    "    df[\"minute\"] = df[time_column].dt.minute\n",
    "\n",
    "    # **Automatically add station_name**\n",
    "    station_mapping = {\n",
    "        \"16098828\": \"app_center\",\n",
    "        \"16101230\": \"app_2pavlodar\",\n",
    "        \"16101231\": \"app_pspu\",\n",
    "        \"16101232\": \"app_metallurg\",\n",
    "        \"16101233\": \"app_zaton\"\n",
    "    }\n",
    "    df[\"station_name\"] = df[station_column].map(station_mapping)\n",
    "\n",
    "    return df\n",
    "\n",
    "df_filled = df_cleaned.copy()  # Create copy to avoid changing original data\n",
    "\n",
    "print(df_filled.head())\n",
    "\n",
    "plot_param_completeness(df_filled, station_name=station_mapping[station], parameter=\"pm2_5\", current=False)\n",
    "\n",
    "# Apply function to dataframe. Fill missing minutes in data, adding them with NaN values for parameters.\n",
    "df_filled = fill_missing_minutes(df_filled, time_column=\"date\", station_column=\"app_id\", parameters=[\"pm2_5\", \"air_temperature\",  \"air_humidity\"])\n",
    "\n",
    "# Output first rows for verification\n",
    "print(df_filled.head())\n",
    "\n",
    "print(f\"Number of rows in dataframe: {len(df_filled)}\")\n",
    "\n",
    "nan_rows = df_filled[df_filled[\"pm2_5\"].isna()]\n",
    "print(nan_rows.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import missingno as msno\n",
    "msno.matrix(df_filled)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correct Artificial Gaps (Due to Delayed Data Recording)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_late_measurements(df, time_column=\"date\", station_column=\"station_name\", threshold_seconds=15):\n",
    "    \"\"\"\n",
    "    Corrects data delays when measurements are recorded with delay in the next minute.\n",
    "    \n",
    "    :param df: Original DataFrame.\n",
    "    :param time_column: Name of date column.\n",
    "    :param station_column: Name of station identifier column.\n",
    "    :param threshold_seconds: Maximum number of seconds to assign measurement to previous minute.\n",
    "    :return: DataFrame with corrected minutes.\n",
    "    \"\"\"\n",
    "\n",
    "    df = df.copy()\n",
    "    df[\"original_date\"] = df[time_column]  # Save original date\n",
    "    df[\"rounded_date\"] = df[time_column].dt.floor(\"min\")  # Round to minutes\n",
    "    df[\"seconds\"] = df[time_column].dt.second  # Extract seconds\n",
    "\n",
    "    # Sort data by station and time\n",
    "    df = df.sort_values(by=[station_column, time_column])\n",
    "\n",
    "    # Create list for new records\n",
    "    corrected_rows = []\n",
    "\n",
    "    for station in df[station_column].unique():\n",
    "        station_data = df[df[station_column] == station].copy()\n",
    "        station_data = station_data.sort_values(by=time_column)\n",
    "\n",
    "        # Go through each row\n",
    "        for i in range(1, len(station_data)):\n",
    "            prev_row = station_data.iloc[i - 1]\n",
    "            curr_row = station_data.iloc[i]\n",
    "\n",
    "            # If previous and current record have the same minute\n",
    "            if prev_row[\"rounded_date\"] == curr_row[\"rounded_date\"]:\n",
    "                continue  # Duplicates of same minute (skip)\n",
    "\n",
    "            # If difference between records is more than 1 minute — potential gap\n",
    "            diff_minutes = (curr_row[\"rounded_date\"] - prev_row[\"rounded_date\"]).seconds // 60\n",
    "\n",
    "            if diff_minutes > 1 and curr_row[\"seconds\"] < threshold_seconds:\n",
    "                # Record new row belonging to missing minute\n",
    "                missing_time = prev_row[\"rounded_date\"] + pd.Timedelta(minutes=1)\n",
    "                corrected_row = curr_row.copy()\n",
    "                corrected_row[\"rounded_date\"] = missing_time\n",
    "                corrected_row[\"corrected\"] = True  # Mark as corrected value\n",
    "                corrected_rows.append(corrected_row)\n",
    "\n",
    "    # Add corrected rows\n",
    "    if corrected_rows:\n",
    "        corrected_df = pd.DataFrame(corrected_rows)\n",
    "        df = pd.concat([df, corrected_df], ignore_index=True)\n",
    "\n",
    "    # Clean duplicates after corrections\n",
    "    df = df.drop_duplicates(subset=[station_column, \"rounded_date\"], keep=\"first\")\n",
    "\n",
    "    # Return corrected dataframe with updated time_column\n",
    "    df[time_column] = df[\"rounded_date\"]\n",
    "    df = df.drop(columns=[\"rounded_date\", \"seconds\"], errors=\"ignore\")\n",
    "    return df\n",
    "\n",
    "df_corrected_data = df_filled.copy()\n",
    "\n",
    "# Count duplicates by station_name and date\n",
    "def count_duplicates(df, time_column=\"date\", station_column=\"station_name\"):\n",
    "    return df.duplicated(subset=[station_column, time_column], keep=False).sum()\n",
    "\n",
    "# Output number of duplicates before correction\n",
    "print(f\"Number of time duplicates BEFORE correction: {count_duplicates(df_corrected_data)}\")\n",
    "\n",
    "# Apply correction\n",
    "df_corrected = correct_late_measurements(df_corrected_data)\n",
    "\n",
    "# Output number of duplicates after correction\n",
    "print(f\"Number of time duplicates AFTER correction: {count_duplicates(df_corrected)}\")\n",
    "\n",
    "print(df_corrected.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_param_over_time(df, param = \"pm2_5\"):\n",
    "    \"\"\"\n",
    "    Plots PM2.5 changes over time.\n",
    "\n",
    "    :param df: DataFrame with 'date' and 'pm2_5' columns\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(20, 6))\n",
    "    plt.plot(df[\"date\"], df[param], marker=\"o\", linestyle=\"\", color=\"blue\", markersize=3, label = param)\n",
    "\n",
    "    plt.xlabel(\"Date and time\")\n",
    "    plt.ylabel(f\"{param} concentration\")\n",
    "    plt.title(f\"{param} changes over time (after outlier removal)\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Function call for visualization\n",
    "plot_param_over_time(df_corrected, param = \"pm2_5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_gaps(df, time_column=\"date\", value_column=\"pm2_5\", station_column=\"station_name\"):\n",
    "    \"\"\"\n",
    "    Detects gaps in data for specified parameter.\n",
    "\n",
    "    :param df: DataFrame with data\n",
    "    :param time_column: Name of timestamp column\n",
    "    :param value_column: Name of analyzed parameter column\n",
    "    :param station_column: Name of station identifier column\n",
    "    :return: DataFrame with gap characteristics (start_time, end_time, duration_minutes, gap_type)\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df[time_column] = pd.to_datetime(df[time_column])\n",
    "\n",
    "    # Filter only needed columns and sort\n",
    "    df = df[[station_column, time_column, value_column]].sort_values(by=[station_column, time_column])\n",
    "\n",
    "    # Remove time duplicates\n",
    "    df = df.drop_duplicates(subset=[station_column, time_column])\n",
    "\n",
    "    # Define rows with NaN\n",
    "    df[\"is_nan\"] = df[value_column].isna()\n",
    "\n",
    "    # Define gap start and end\n",
    "    df[\"gap_start\"] = df[\"is_nan\"] & ~df[\"is_nan\"].shift(1, fill_value=False)\n",
    "    df[\"gap_end\"] = df[\"is_nan\"] & ~df[\"is_nan\"].shift(-1, fill_value=False)\n",
    "\n",
    "    # Form lists of start and end points of gaps\n",
    "    gap_starts = df[df[\"gap_start\"]][[station_column, time_column]].rename(columns={time_column: \"start_time\"})\n",
    "    gap_ends = df[df[\"gap_end\"]][[station_column, time_column]].rename(columns={time_column: \"end_time\"})\n",
    "\n",
    "    # Ensure equal number of start and end points\n",
    "    if len(gap_starts) != len(gap_ends):\n",
    "        print(\"⚠️ Detected mismatch in number of gap start and end points!\")\n",
    "\n",
    "    # Combine gap start and end using indices\n",
    "    gaps = pd.concat([gap_starts.reset_index(drop=True), gap_ends.reset_index(drop=True)], axis=1)\n",
    "\n",
    "    # Calculate gap duration\n",
    "    gaps[\"duration_minutes\"] = (gaps[\"end_time\"] - gaps[\"start_time\"]).dt.total_seconds() / 60 + 1\n",
    "\n",
    "    # Remove gaps with 0 minutes duration (if start_time == end_time)\n",
    "    gaps = gaps[gaps[\"duration_minutes\"] > 0]\n",
    "\n",
    "    # Categorize gaps by duration\n",
    "    bins = [0, 5, 30, np.inf]\n",
    "    labels = [\"short\", \"medium\", \"long\"]\n",
    "    gaps[\"gap_type\"] = pd.cut(gaps[\"duration_minutes\"], bins=bins, labels=labels, right=False)\n",
    "\n",
    "    # Create index without duplicates for mapping neighboring values\n",
    "    df_unique = df.drop_duplicates(subset=[time_column]).set_index(time_column)\n",
    "\n",
    "    # Add neighboring values before and after gap\n",
    "    gaps[\"before_gap\"] = gaps[\"start_time\"].map(df_unique[value_column].ffill())\n",
    "    gaps[\"after_gap\"] = gaps[\"end_time\"].map(df_unique[value_column].bfill())\n",
    "\n",
    "    return gaps.reset_index(drop=True)\n",
    "\n",
    "gap_df_pm2_5 = detect_gaps(df_corrected, time_column=\"date\", value_column=\"pm2_5\", station_column=\"station_name\")\n",
    "\n",
    "gap_df_air_t = detect_gaps(df_corrected, time_column=\"date\", value_column=\"air_temperature\", station_column=\"station_name\")\n",
    "\n",
    "gap_df_air_h = detect_gaps(df_corrected, time_column=\"date\", value_column=\"air_humidity\", station_column=\"station_name\")\n",
    "\n",
    "# Output first 10 gaps\n",
    "print(gap_df_pm2_5.head())  # Output first rows\n",
    "\n",
    "# df_corrected[df_corrected.duplicated(subset=[\"date\"], keep=False)]\n",
    "\n",
    "# Check - output gaps with zero duration\n",
    "# print(gap_df[gap_df[\"duration_minutes\"] == 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Interpolation of Short Gaps (up to 5 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_short_gaps(df, time_column=\"date\", value_column=\"pm2_5\", max_gap=5):\n",
    "    \"\"\"\n",
    "    Interpolates only short-term gaps (no more than max_gap minutes).\n",
    "\n",
    "    :param df: DataFrame with time data.\n",
    "    :param time_column: Name of date column.\n",
    "    :param value_column: Name of parameter to fill gaps in.\n",
    "    :param max_gap: Maximum gap length (in minutes) for interpolation.\n",
    "    :return: DataFrame with partially filled gaps.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df[time_column] = pd.to_datetime(df[time_column])  # Ensure datetime format\n",
    "\n",
    "    # Define gaps\n",
    "    df[\"is_nan\"] = df[value_column].isna()\n",
    "\n",
    "    # Define gap start and end\n",
    "    df[\"gap_start\"] = df[\"is_nan\"] & ~df[\"is_nan\"].shift(1, fill_value=False)\n",
    "    df[\"gap_end\"] = df[\"is_nan\"] & ~df[\"is_nan\"].shift(-1, fill_value=False)\n",
    "\n",
    "    # Form lists of start and end points of gaps\n",
    "    gap_starts = df[df[\"gap_start\"]][[time_column]].rename(columns={time_column: \"start_time\"})\n",
    "    gap_ends = df[df[\"gap_end\"]][[time_column]].rename(columns={time_column: \"end_time\"})\n",
    "\n",
    "    # Combine gap start and end\n",
    "    gaps = pd.concat([gap_starts.reset_index(drop=True), gap_ends.reset_index(drop=True)], axis=1)\n",
    "\n",
    "    # Calculate gap duration in minutes\n",
    "    gaps[\"duration_minutes\"] = (gaps[\"end_time\"] - gaps[\"start_time\"]).dt.total_seconds() / 60 + 1\n",
    "\n",
    "    # Keep only short gaps (≤ max_gap minutes)\n",
    "    short_gaps = gaps[gaps[\"duration_minutes\"] <= max_gap]\n",
    "\n",
    "    # Interpolate only short gaps\n",
    "    for _, row in short_gaps.iterrows():\n",
    "        start, end = row[\"start_time\"], row[\"end_time\"]\n",
    "        mask = (df[time_column] >= start) & (df[time_column] <= end)\n",
    "        df.loc[mask, value_column] = df[value_column].interpolate(method=\"linear\", limit_direction=\"both\")\n",
    "\n",
    "    # Remove helper columns\n",
    "    df = df.drop(columns=[\"is_nan\", \"gap_start\", \"gap_end\"], errors=\"ignore\")\n",
    "\n",
    "    return df\n",
    "\n",
    "# Fast interpolation function for only short gaps (up to max_gap minutes) with guaranteed capture of all minutes\n",
    "\n",
    "def fast_interpolate_short_gaps(df, gap_df, time_column=\"date\", value_column=\"pm2_5\", max_gap=5):\n",
    "    \"\"\"\n",
    "    Fast vectorized interpolation of short-term gaps with capture of all minutes inside gaps.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df[time_column] = pd.to_datetime(df[time_column])\n",
    "\n",
    "    # Select only short gaps\n",
    "    short_gaps = gap_df[gap_df[\"duration_minutes\"] <= max_gap]\n",
    "\n",
    "    # Form **vectorized** mask for all rows falling into found gaps\n",
    "    mask = df[time_column].apply(lambda t: any((t >= row[\"start_time\"]) & (t <= row[\"end_time\"]) for _, row in short_gaps.iterrows()))\n",
    "\n",
    "    # Apply interpolation to found gaps\n",
    "    df.loc[mask, value_column] = df[value_column].interpolate(method=\"linear\", limit_direction=\"both\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create copy of original dataframe\n",
    "df_interp = df_corrected.copy()\n",
    "\n",
    "# 1. Output number of gaps BEFORE interpolation\n",
    "for param in [\"pm2_5\", \"air_temperature\", \"air_humidity\"]:\n",
    "    print(f\"Gaps in '{param}' BEFORE interpolation:\", df_interp[param].isna().sum())\n",
    "\n",
    "# 2. Visualize data completeness before interpolation\n",
    "# plot_param_completeness(df_filled, station_name=\"app_pspu\", parameter=\"pm2_5\")\n",
    "\n",
    "# 3. Apply interpolation only to short gaps\n",
    "for param in [\"pm2_5\", \"air_temperature\", \"air_humidity\"]:\n",
    "    df_interp = interpolate_short_gaps(df_interp, value_column=param, max_gap=5)\n",
    "\n",
    "# 4. Output number of gaps AFTER interpolation\n",
    "print(\"\\nResults after interpolation:\")\n",
    "for param in [\"pm2_5\", \"air_temperature\", \"air_humidity\"]:\n",
    "    print(f\"Gaps in '{param}' AFTER interpolation:\", df_interp[param].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msno.matrix(df_interp)\n",
    "\n",
    "print(df_interp.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gap_visualization(df, gap_df, gap_index=0, param=\"pm2_5\", wingspan=10):\n",
    "    \"\"\"\n",
    "    Visualizes extracted gap in data, showing 10 points before and after the gap.\n",
    "    \n",
    "    :param df: DataFrame with data\n",
    "    :param gap_df: DataFrame with detected gaps (must contain start_time and end_time)\n",
    "    :param gap_index: Gap index from gap_df to visualize\n",
    "    :param param: Parameter name for analysis (e.g., \"pm2_5\")\n",
    "    \"\"\"\n",
    "\n",
    "    if gap_df.empty:\n",
    "        print(\"No data in gaps table!\")\n",
    "        return\n",
    "    \n",
    "    # Get gap data\n",
    "    first_gap = gap_df.iloc[gap_index]  # Gap by specified index\n",
    "    start_time, end_time, duration = first_gap[\"start_time\"], first_gap[\"end_time\"], first_gap[\"duration_minutes\"]\n",
    "\n",
    "    # Output gap length information\n",
    "    print(f\"📌 Gap length - {duration} minutes (from {start_time} to {end_time})\")\n",
    "\n",
    "    # Select 10 points before and after gap\n",
    "    before_gap = df[df[\"date\"] < start_time].tail(wingspan)  # 10 points BEFORE gap\n",
    "    after_gap = df[df[\"date\"] > end_time].head(wingspan)  # 10 points AFTER gap\n",
    "    gap_points = df[(df[\"date\"] >= start_time) & (df[\"date\"] <= end_time)]  # The gap itself\n",
    "    \n",
    "    # Combine data for lines\n",
    "    combined_df = pd.concat([before_gap, gap_points, after_gap]).sort_values(by=\"date\")\n",
    "\n",
    "    # Plot construction\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    \n",
    "    # Line connects all points, including missing ones\n",
    "    plt.plot(combined_df[\"date\"], combined_df[param], linestyle=\"-\", color=\"gray\", alpha=0.5)\n",
    "\n",
    "    # Original points (blue)\n",
    "    plt.scatter(before_gap[\"date\"], before_gap[param], color=\"blue\", label=\"Original points\")\n",
    "    plt.scatter(after_gap[\"date\"], after_gap[param], color=\"blue\")\n",
    "\n",
    "    # Filled points (red)\n",
    "    plt.scatter(gap_points[\"date\"], gap_points[param], color=\"red\", label=\"Filled points\")\n",
    "\n",
    "    # Plot formatting\n",
    "    plt.xlabel(\"Date and time\")\n",
    "    plt.ylabel(param)\n",
    "    plt.title(f\"Gap #{gap_index} visualization ({param})\")\n",
    "    plt.legend()\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid()\n",
    "\n",
    "    # Show plot\n",
    "    plt.show()\n",
    "\n",
    "# plot_gap_visualization(df_interp, gap_df_pm2_5, gap_index=1, param=\"pm2_5\", wingspan=20)\n",
    "\n",
    "plot_gap_visualization(df_interp, gap_df_air_t, gap_index=46, param=\"air_temperature\", wingspan=30) # example of wide gap gap_index=46"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hourly Data Aggregation\n",
    "Average PM2.5, temperature and humidity within each hour.\n",
    "Check that each hour has sufficient data (≥40 minutes).\n",
    "If data is insufficient, the hourly interval remains empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df_for_agregation = df_interp.copy()\n",
    "\n",
    "# Ensure 'date' is in datetime format\n",
    "df_for_agregation[\"date\"] = pd.to_datetime(df_for_agregation[\"date\"])\n",
    "\n",
    "# Set 'date' as index\n",
    "df_for_agregation.set_index(\"date\", inplace=True)\n",
    "\n",
    "# Parameters for averaging\n",
    "params = [\"pm2_5\", \"air_temperature\", \"air_humidity\"]\n",
    "min_count = 40  # Minimum number of points per hour\n",
    "\n",
    "# Create empty DataFrame for hourly data with time axis\n",
    "df_hourly = pd.DataFrame(index=df_for_agregation.resample(\"h\").mean(numeric_only=True).index)\n",
    "\n",
    "# Average by each parameter considering minimum number of measurements\n",
    "for param in params:\n",
    "    hourly_counts = df_for_agregation.resample(\"h\")[param].count()  # Number of points in each hour\n",
    "    valid_hours = hourly_counts[hourly_counts >= min_count].index  # Only hours with ≥40 measurements\n",
    "    \n",
    "    # Average and leave NaN where there's insufficient data\n",
    "    df_hourly[param] = df_for_agregation.resample(\"h\")[param].mean(numeric_only=True)\n",
    "    df_hourly.loc[~df_hourly.index.isin(valid_hours), param] = np.nan\n",
    "\n",
    "# Reset index, return 'date' as column\n",
    "df_hourly.reset_index(inplace=True)\n",
    "\n",
    "# Output result\n",
    "from IPython.display import display\n",
    "display(df_hourly)\n",
    "\n",
    "print(df_hourly.head(75))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming df_hourly is already created and contains \"date\" and \"pm2_5\" columns\n",
    "# Sort data by date and create copy for work\n",
    "df_out = df_hourly.copy().sort_values(\"date\").reset_index(drop=True)\n",
    "\n",
    "# Drop outliers for cleaner analysis (optional)\n",
    "df_clean = df_out[~df_out[\"pm2_5\"].isna()]  # remove rows with NaN in pm2.5\n",
    "\n",
    "percentile = (df_clean[\"pm2_5\"] <= 270).mean() * 100\n",
    "print(f\"Value 270 µg/m³ corresponds to {percentile:.2f}th percentile\")\n",
    "# Can also calculate other distribution characteristics\n",
    "p_99 = np.percentile(df_clean[\"pm2_5\"], 99)\n",
    "p_99_6 = np.percentile(df_clean[\"pm2_5\"], 99.6)\n",
    "p_99_9 = np.percentile(df_clean[\"pm2_5\"], 99.9)\n",
    "\n",
    "print(f\"99th percentile: {p_99:.2f} µg/m³\")\n",
    "print(f\"99.6th percentile: {p_99_6:.2f} µg/m³\")\n",
    "print(f\"99.9th percentile: {p_99_9:.2f} µg/m³\")\n",
    "\n",
    "# Initialize column for outlier marking\n",
    "df_out[\"is_primary_outlier\"] = False\n",
    "\n",
    "# Primary outlier screening logic:\n",
    "# A point is considered an outlier if:\n",
    "# 1. Its value is greater than 270, or\n",
    "# 2. Its value > 200 and more than 3 times higher than the previous point value.\n",
    "# If subsequent points follow that satisfy the condition:\n",
    "#    - value greater than 270, or\n",
    "#    - value > 200 and doesn't drop sharply (not below 90% of previous value from group),\n",
    "# then they are also marked as outliers until the first sharp drop.\n",
    "N = len(df_out)\n",
    "i = 1\n",
    "while i < N:\n",
    "    current_val = df_out.loc[i, \"pm2_5\"]\n",
    "    previous_val = df_out.loc[i-1, \"pm2_5\"]\n",
    "    \n",
    "    # Check condition for detecting first outlier in group\n",
    "    if (current_val > 270) or ((current_val > 200) and (current_val > 3 * previous_val)):\n",
    "        df_out.loc[i, \"is_primary_outlier\"] = True\n",
    "        group_val = current_val\n",
    "        j = i + 1\n",
    "        # Mark subsequent points while they satisfy conditions\n",
    "        while j < N:\n",
    "            next_val = df_out.loc[j, \"pm2_5\"]\n",
    "            if (next_val > 270) or ((next_val > 200) and (next_val >= 0.9 * group_val)):\n",
    "                df_out.loc[j, \"is_primary_outlier\"] = True\n",
    "                group_val = next_val  # update group value if next point is greater or approximately equal to previous from group\n",
    "                j += 1\n",
    "            else:\n",
    "                break\n",
    "        i = j  # move to next point after outlier group\n",
    "    else:\n",
    "        i += 1\n",
    "\n",
    "# Build first plot: all points blue, outliers (marked) red\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(df_out[\"date\"], df_out[\"pm2_5\"], color=\"blue\", label=\"All data\")\n",
    "plt.scatter(df_out.loc[df_out[\"is_primary_outlier\"], \"date\"],\n",
    "            df_out.loc[df_out[\"is_primary_outlier\"], \"pm2_5\"],\n",
    "            color=\"red\", label=\"Outliers\", zorder=5)\n",
    "plt.xlabel(\"Datetime\")\n",
    "plt.ylabel(\"PM2.5 (µg/m³)\")\n",
    "# plt.title(\"PM2.5 plot with primary outlier screening\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(os.path.join(\"output_diagrams\", f\"secondary_outliers_hourly.png\"), dpi=600)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Create DataFrame only with points not marked as outliers\n",
    "df_hourly_no_outliers = df_out[~df_out[\"is_primary_outlier\"]]\n",
    "\n",
    "# Build second plot: only remaining points (without outliers)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(df_hourly_no_outliers[\"date\"], df_hourly_no_outliers[\"pm2_5\"], color=\"blue\", label=\"Data without outliers\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"PM2.5\")\n",
    "plt.title(\"PM2.5 plot after primary outlier removal\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "def plot_hourly_param_completeness(df_hourly, parameter=\"pm2_5\"):\n",
    "    \"\"\"\n",
    "    Visualization of hourly data completeness for specified parameter.\n",
    "\n",
    "    :param df_hourly: DataFrame with hourly average values.\n",
    "    :param parameter: Parameter name (e.g., \"pm2_5\", \"air_temperature\", \"air_humidity\").\n",
    "    \"\"\"\n",
    "    df_hourly[\"date\"] = pd.to_datetime(df_hourly[\"date\"])  # Ensure datetime format\n",
    "\n",
    "    # Define full range of monitoring months\n",
    "    first_record_date = df_hourly[\"date\"].min()\n",
    "    last_record_date = df_hourly[\"date\"].max()\n",
    "\n",
    "    # Ensure month boundaries are considered\n",
    "    first_record_month_start = first_record_date.replace(day=1, hour=0, minute=0, second=0)\n",
    "    last_record_month_start = last_record_date.replace(day=1, hour=0, minute=0, second=0)\n",
    "\n",
    "    # Generate full list of months\n",
    "    month_range = pd.date_range(start=first_record_month_start, end=last_record_month_start, freq=\"MS\").to_pydatetime().tolist()\n",
    "\n",
    "    # Function for plotting monthly data completeness\n",
    "    def plot_month_fill(ax, month_data, month_name, first_record_date, last_record_date):\n",
    "        days_in_month = pd.Timestamp(month_name).days_in_month\n",
    "        hours_in_month = days_in_month * 24\n",
    "        hourly_completeness = np.full(hours_in_month, 2)  # Default 2 (outside measurement range)\n",
    "\n",
    "        # Define month boundaries\n",
    "        month_start = pd.Timestamp(f\"{month_name}-01\")\n",
    "        month_end = month_start + pd.Timedelta(days=days_in_month, hours=23)\n",
    "\n",
    "        if first_record_date and last_record_date:\n",
    "            if month_end < first_record_date or month_start > last_record_date:\n",
    "                completeness_percentage = 0\n",
    "            else:\n",
    "                # Fill array (1 - data present, 0 - no data)\n",
    "                for _, row in month_data.iterrows():\n",
    "                    day_index = row[\"date\"].day - 1\n",
    "                    hour_index = row[\"date\"].hour\n",
    "                    index = day_index * 24 + hour_index\n",
    "                    if pd.notna(row[parameter]):\n",
    "                        hourly_completeness[index] = 1  # Data present\n",
    "\n",
    "                # Update color map for hours within measurement period\n",
    "                for hour in range(hours_in_month):\n",
    "                    current_datetime = month_start + pd.Timedelta(hours=hour)\n",
    "                    if first_record_date <= current_datetime <= last_record_date:\n",
    "                        if hourly_completeness[hour] == 2:\n",
    "                            hourly_completeness[hour] = 0  # No data\n",
    "                    elif current_datetime < first_record_date:\n",
    "                        hourly_completeness[hour] = 2  # Outside measurement period (white)\n",
    "\n",
    "                # Calculate completeness percentage\n",
    "                completeness_percentage = np.sum(hourly_completeness == 1) / hours_in_month * 100\n",
    "        else:\n",
    "            completeness_percentage = 0  # If no data at all\n",
    "\n",
    "        # Create color map\n",
    "        cmap = mcolors.ListedColormap([\"red\", \"green\", \"white\"])\n",
    "        bounds = [0, 1, 2, 3]\n",
    "        norm = mcolors.BoundaryNorm(bounds, cmap.N)\n",
    "\n",
    "        # Build heatmap\n",
    "        ax.imshow(hourly_completeness.reshape((days_in_month, 24)), aspect=\"auto\", cmap=cmap, norm=norm, interpolation=\"nearest\")\n",
    "        ax.set_title(f\"{month_name} ({parameter})\\ncompleteness: {completeness_percentage:.1f}%\", fontsize=10)\n",
    "        ax.set_xlabel(\"Hours\")\n",
    "        ax.set_ylabel(\"Day of the month\")\n",
    "\n",
    "        # Configure X axis (hours)\n",
    "        ax.set_xticks(np.arange(0, 24))  # Main hour marks\n",
    "        ax.set_xticklabels(range(0, 24))  # Hours 0–23\n",
    "        ax.set_xticks(np.arange(-0.5, 24, 1), minor=True)  # Intermediate divisions between hours\n",
    "\n",
    "        # Configure Y axis (days of month)\n",
    "        days_labels = [f\"{day}\" for day in range(1, days_in_month + 1)]\n",
    "        ax.set_yticks(range(0, days_in_month))  # Main day marks\n",
    "        ax.set_yticklabels(days_labels)\n",
    "        ax.set_yticks(np.arange(-0.5, days_in_month, 1), minor=True)  # Intermediate divisions between days\n",
    "\n",
    "        # Enable grid **only on intermediate divisions**\n",
    "        ax.grid(which=\"minor\", linestyle=\":\", linewidth=0.5, alpha=0.7)  # Keep only grid on intermediate divisions\n",
    "\n",
    "    # Build calendar\n",
    "    fig, axes = plt.subplots(nrows=3, ncols=4, figsize=(20, 15))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, month_datetime in enumerate(month_range):\n",
    "        ax = axes[i]\n",
    "        month_data = df_hourly[\n",
    "            (df_hourly[\"date\"].dt.year == month_datetime.year) &\n",
    "            (df_hourly[\"date\"].dt.month == month_datetime.month)\n",
    "        ]\n",
    "        month_name = month_datetime.strftime(\"%Y-%m\")\n",
    "        plot_month_fill(ax, month_data, month_name, first_record_date, last_record_date)\n",
    "\n",
    "    # Remove extra axes\n",
    "    for ax in axes[len(month_range):]:\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    # plt.suptitle(f\"Hourly data completeness, parameter: {parameter}\", fontsize=20)\n",
    "    plt.subplots_adjust(top=0.9)\n",
    "    plt.savefig(os.path.join(\"output_diagrams\", f\"hourly_data_completeness.png\"), dpi=600)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hourly_param_completeness(df_hourly_no_outliers, parameter=\"pm2_5\")\n",
    "# plot_hourly_param_completeness(df_hourly, parameter=\"air_temperature\")\n",
    "# plot_hourly_param_completeness(df_hourly, parameter=\"air_humidity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_hourly_gaps(df, time_column=\"date\", value_column=\"pm2_5\"):\n",
    "    \"\"\"\n",
    "    Detects gaps in hourly DataFrame for specified parameter.\n",
    "\n",
    "    :param df: DataFrame with hourly data.\n",
    "    :param time_column: Name of timestamp column.\n",
    "    :param value_column: Name of analyzed parameter column.\n",
    "    :return: DataFrame with gap characteristics (start_time, end_time, duration_hours, gap_type).\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df[time_column] = pd.to_datetime(df[time_column])  # Ensure datetime format\n",
    "\n",
    "    # Filter needed columns and sort\n",
    "    df = df[[time_column, value_column]].sort_values(by=time_column)\n",
    "\n",
    "    # Remove time duplicates\n",
    "    df = df.drop_duplicates(subset=[time_column])\n",
    "\n",
    "    # Define rows with NaN\n",
    "    df[\"is_nan\"] = df[value_column].isna()\n",
    "\n",
    "    # Define gap start and end\n",
    "    df[\"gap_start\"] = df[\"is_nan\"] & ~df[\"is_nan\"].shift(1, fill_value=False)\n",
    "    df[\"gap_end\"] = df[\"is_nan\"] & ~df[\"is_nan\"].shift(-1, fill_value=False)\n",
    "\n",
    "    # Form lists of start and end points of gaps\n",
    "    gap_starts = df[df[\"gap_start\"]][[time_column]].rename(columns={time_column: \"start_time\"})\n",
    "    gap_ends = df[df[\"gap_end\"]][[time_column]].rename(columns={time_column: \"end_time\"})\n",
    "\n",
    "    # Ensure equal number of start and end points\n",
    "    if len(gap_starts) != len(gap_ends):\n",
    "        print(\"⚠️ Detected mismatch in number of gap start and end points!\")\n",
    "\n",
    "    # Combine gap start and end\n",
    "    gaps = pd.concat([gap_starts.reset_index(drop=True), gap_ends.reset_index(drop=True)], axis=1)\n",
    "\n",
    "    # Calculate gap duration in hours (add 1 hour to include both ends)\n",
    "    gaps[\"duration_hours\"] = (gaps[\"end_time\"] - gaps[\"start_time\"]).dt.total_seconds() / 3600 + 1\n",
    "\n",
    "    # Remove gaps with 0 hours duration (if start_time == end_time, they will be 1 hour)\n",
    "    gaps = gaps[gaps[\"duration_hours\"] > 0]\n",
    "\n",
    "    # Categorize gaps by duration\n",
    "    bins = [0, 2, 12, np.inf]  # <= 2 hours - short, <= 12 - medium, > 12 - long\n",
    "    labels = [\"short\", \"medium\", \"long\"]\n",
    "    gaps[\"gap_type\"] = pd.cut(gaps[\"duration_hours\"], bins=bins, labels=labels, right=False)\n",
    "\n",
    "    # Create index without duplicates for mapping neighboring values\n",
    "    df_unique = df.drop_duplicates(subset=[time_column]).set_index(time_column)\n",
    "\n",
    "    # Add neighboring values before and after gap\n",
    "    gaps[\"before_gap\"] = gaps[\"start_time\"].map(df_unique[value_column].ffill())\n",
    "    gaps[\"after_gap\"] = gaps[\"end_time\"].map(df_unique[value_column].bfill())\n",
    "\n",
    "    return gaps.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_gaps_pm2_5 = detect_hourly_gaps(df_hourly_no_outliers, value_column=\"pm2_5\")\n",
    "hourly_gaps_temperature = detect_hourly_gaps(df_hourly_no_outliers, value_column=\"air_temperature\")\n",
    "hourly_gaps_humidity = detect_hourly_gaps(df_hourly_no_outliers, value_column=\"air_humidity\")\n",
    "\n",
    "# Output found gaps\n",
    "print(hourly_gaps_pm2_5.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Select only numeric columns\n",
    "numeric_columns = [\"pm2_5\", \"air_temperature\", \"air_humidity\"]\n",
    "df_corr = df_hourly_no_outliers[numeric_columns].corr()\n",
    "\n",
    "# Visualization using Seaborn\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(df_corr, annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=0.5)\n",
    "plt.title(\"Correlation matrix of hourly data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize PM2.5 relationship with temperature and humidity\n",
    "\n",
    "# PM2.5 vs Temperature scatter plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x=df_hourly_no_outliers[\"air_temperature\"], y=df_hourly_no_outliers[\"pm2_5\"], alpha=0.5)\n",
    "plt.xlabel(\"Temperature, °C\")\n",
    "plt.ylabel(\"PM2.5, µg/m³\")\n",
    "plt.title(\"PM2.5 vs Temperature\")\n",
    "plt.show()\n",
    "\n",
    "# PM2.5 vs Humidity scatter plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x=df_hourly_no_outliers[\"air_humidity\"], y=df_hourly_no_outliers[\"pm2_5\"], alpha=0.5)\n",
    "plt.xlabel(\"Humidity, %\")\n",
    "plt.ylabel(\"PM2.5, µg/m³\")\n",
    "plt.title(\"PM2.5 vs Humidity\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot PM2.5 distribution at different temperatures and humidity levels\n",
    "\n",
    "df_hourly_no_outliers[\"temp_bins\"] = pd.cut(df_hourly_no_outliers[\"air_temperature\"], bins=10)\n",
    "df_hourly_no_outliers[\"hum_bins\"] = pd.cut(df_hourly_no_outliers[\"air_humidity\"], bins=10)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x=df_hourly_no_outliers[\"temp_bins\"], y=df_hourly_no_outliers[\"pm2_5\"])\n",
    "plt.xticks(rotation=45)\n",
    "plt.xlabel(\"Temperature ranges\")\n",
    "plt.ylabel(\"PM2.5\")\n",
    "plt.title(\"PM2.5 distribution by temperature\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x=df_hourly_no_outliers[\"hum_bins\"], y=df_hourly_no_outliers[\"pm2_5\"])\n",
    "plt.xticks(rotation=45)\n",
    "plt.xlabel(\"Humidity ranges\")\n",
    "plt.ylabel(\"PM2.5\")\n",
    "plt.title(\"PM2.5 distribution by humidity\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weather Data Import\n",
    "1 Data import – Load CSV with parameters: time, temperature, pressure, humidity, wind direction and speed, visibility.\n",
    "\n",
    "2 Time conversion – Convert time to datetime format.\n",
    "\n",
    "3 Numeric data cleaning – Convert T, P0, P, U, Ff, VV to float, replace gaps with median.\n",
    "\n",
    "4 Categorical variable processing – Encode DD (wind direction) with numbers, create decoding dictionary.\n",
    "\n",
    "5 Visibility cleaning (VV) – Remove text part from values (\"10.0 and more\" → 10.0).\n",
    "\n",
    "time (time), T (temperature), P0 (station level pressure), P (sea level adjusted pressure), U (humidity), DD (wind direction), Ff (wind speed), VV (visibility)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# File path\n",
    "file_path = \"weather_archive_utf8.csv\"\n",
    "\n",
    "# Select only needed columns (remove \"c\")\n",
    "columns_to_import = [\"time\", \"T\", \"P0\", \"P\", \"U\", \"DD\", \"Ff\", \"VV\"]\n",
    "\n",
    "# Data import\n",
    "df_weather = pd.read_csv(file_path, delimiter=\";\", usecols=columns_to_import, encoding=\"utf-8\")\n",
    "\n",
    "# Convert \"time\" column to datetime\n",
    "df_weather[\"time\"] = pd.to_datetime(df_weather[\"time\"], format=\"%d.%m.%Y %H:%M\")\n",
    "\n",
    "# Process numeric data, replacing commas with dots (if any)\n",
    "numeric_columns = [\"T\", \"P0\", \"P\", \"U\", \"Ff\"]\n",
    "df_weather[numeric_columns] = df_weather[numeric_columns].replace(\",\", \".\", regex=True).astype(float)\n",
    "\n",
    "# Fill missing values with median (if any)\n",
    "df_weather[numeric_columns] = df_weather[numeric_columns].fillna(df_weather[numeric_columns].median())\n",
    "\n",
    "# Categorical variables: wind direction (DD) → label encoding\n",
    "df_weather[\"DD\"] = df_weather[\"DD\"].astype(str)\n",
    "wind_mapping = {direction: idx for idx, direction in enumerate(df_weather[\"DD\"].unique())}\n",
    "df_weather[\"DD\"] = df_weather[\"DD\"].map(wind_mapping)\n",
    "\n",
    "# **Process VV (visibility)**\n",
    "# Keep only number, replace \"10.0 and more\" with 10.0\n",
    "df_weather[\"VV\"] = df_weather[\"VV\"].astype(str).str.extract(r'(\\d+\\.?\\d*)')[0].astype(float)\n",
    "\n",
    "# Check result\n",
    "print(df_weather.info())\n",
    "print(df_weather.head())\n",
    "\n",
    "# Dictionary for decoding wind direction codes\n",
    "print(\"\\nEncoded wind directions:\")\n",
    "print(wind_mapping)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Merge dataframes\n",
    "# Output original DataFrame structure\n",
    "\n",
    "\n",
    "df_merged = df_hourly_no_outliers.copy()\n",
    "df_merged = df_merged.drop(columns=[\"temp_bins\", \"hum_bins\"], errors=\"ignore\")\n",
    "print(\"Original df_merged DataFrame structure:\")\n",
    "print(df_merged.info())\n",
    "\n",
    "# Convert time column in df_weather to datetime format (if not done yet)\n",
    "df_weather[\"time\"] = pd.to_datetime(df_weather[\"time\"])\n",
    "\n",
    "# Convert date column in df_merged to datetime format (if not done yet)\n",
    "df_merged[\"date\"] = pd.to_datetime(df_merged[\"date\"])\n",
    "\n",
    "# Filter values where minutes = 00 in df_weather (full hours)\n",
    "df_weather_hourly = df_weather[df_weather[\"time\"].dt.minute == 0]\n",
    "\n",
    "# Filter values where minutes = 30 in df_weather (backup values)\n",
    "df_weather_half_hour = df_weather[df_weather[\"time\"].dt.minute == 30]\n",
    "\n",
    "# Merge by exact hourly correspondence (00 minutes)\n",
    "df_merged = pd.merge(\n",
    "    df_merged, \n",
    "    df_weather_hourly, \n",
    "    left_on=\"date\", \n",
    "    right_on=\"time\", \n",
    "    how=\"left\", \n",
    "    suffixes=(\"\", \"_hourly\")  # Add suffix for columns from df_weather_hourly\n",
    ")\n",
    "\n",
    "# Remove extra time column (after first merge)\n",
    "df_merged.drop(columns=[\"time\"], inplace=True)\n",
    "\n",
    "# Merge with 30-minute data (if 00-minute data is missing)\n",
    "df_merged = pd.merge(\n",
    "    df_merged, \n",
    "    df_weather_half_hour, \n",
    "    left_on=\"date\", \n",
    "    right_on=\"time\", \n",
    "    how=\"left\", \n",
    "    suffixes=(\"\", \"_half_hour\")  # Add suffix for columns from df_weather_half_hour\n",
    ")\n",
    "\n",
    "# Fill gaps with 30-minute values\n",
    "weather_cols = [\"T\", \"P0\", \"P\", \"U\", \"DD\", \"Ff\", \"VV\"]\n",
    "for col in weather_cols:\n",
    "    df_merged[col] = df_merged[col].fillna(df_merged[f\"{col}_half_hour\"])\n",
    "\n",
    "# Remove temporary columns and duplicates\n",
    "df_merged.drop(columns=[\"time\"] + [f\"{col}_half_hour\" for col in weather_cols], inplace=True, errors=\"ignore\")\n",
    "\n",
    "# Output final DataFrame structure\n",
    "print(\"Merged DataFrame structure:\")\n",
    "print(df_merged.info())\n",
    "\n",
    "# Show first rows\n",
    "print(df_merged.head())\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hourly_param_completeness(df_merged, parameter=\"P\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Interpolation of Short Gaps (no more than 2 hours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def interpolate_short_gaps(df, value_column, time_column=\"date\", max_gap=1):\n",
    "    \"\"\"\n",
    "    Interpolates only short gaps (no more than max_gap hours).\n",
    "    \n",
    "    :param df: DataFrame with data\n",
    "    :param value_column: Name of data column for interpolation\n",
    "    :param time_column: Name of time column\n",
    "    :param max_gap: Maximum gap length for interpolation (in hours)\n",
    "    :return: DataFrame with interpolated values (only for short gaps)\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df[time_column] = pd.to_datetime(df[time_column])  # Convert to datetime\n",
    "    df = df.sort_values(time_column)  # Sort by time\n",
    "\n",
    "    # Define gaps\n",
    "    df[\"gap\"] = df[value_column].isna()\n",
    "\n",
    "    # Calculate gap duration\n",
    "    df[\"gap_group\"] = (df[\"gap\"] != df[\"gap\"].shift()).cumsum()  # Group gaps\n",
    "    gap_sizes = df.groupby(\"gap_group\")[\"gap\"].transform(\"sum\")  # Size of each gap\n",
    "\n",
    "    # Keep only short gaps\n",
    "    df[value_column] = df[value_column].interpolate(method=\"linear\", limit=max_gap)\n",
    "\n",
    "    # If gap size is larger than max_gap → keep NaN\n",
    "    df.loc[gap_sizes > max_gap, value_column] = np.nan\n",
    "\n",
    "    # Remove helper columns\n",
    "    df.drop(columns=[\"gap\", \"gap_group\"], inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "# 📌 Apply interpolation only for gaps ≤ 2 hours\n",
    "df_interpolated = df_merged.copy()\n",
    "for param in [\"pm2_5\", \"air_temperature\", \"air_humidity\", \"T\", \"P0\", \"P\", \"U\", \"DD\",\"Ff\", \"VV\"]:\n",
    "    df_interpolated = interpolate_short_gaps(df_interpolated, value_column=param, max_gap=2)\n",
    "\n",
    "# 📊 Output number of remaining gaps\n",
    "print(\"\\n📊 Number of gaps AFTER interpolation (only short ≤2 hours):\")\n",
    "print(df_interpolated.isna().sum())\n",
    "\n",
    "# 🔍 Visualize result (NaN heatmap)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(df_interpolated.isna(), cmap=\"coolwarm\", cbar=False)\n",
    "plt.title(\"Heatmap of gaps after interpolation (≤2 hours)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hourly_param_completeness(df_interpolated, parameter=\"pm2_5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_interpolated.to_csv(\"df_data_prepared.csv\", index=False, encoding=\"utf-8\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
