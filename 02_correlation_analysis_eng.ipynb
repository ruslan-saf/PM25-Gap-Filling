
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation analysis between all parameters, including: PM2.5, Temperature and humidity (measured and from open sources), Pressure, wind, visibility (from open sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import pearsonr\n",
    "import os\n",
    "\n",
    "# Import the dataframe\n",
    "df_interpolated = pd.read_csv(\"df_data_prepared.csv\", encoding=\"utf-8\", parse_dates=[\"date\"])\n",
    "df_corr = df_interpolated.copy()\n",
    "\n",
    "# Select only numeric columns for correlation analysis\n",
    "numeric_columns = df_corr.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "# Drop rows with NaN values across all numeric columns to ensure consistent lengths\n",
    "df_clean = df_corr[numeric_columns].dropna()\n",
    "\n",
    "# Calculate the correlation matrix\n",
    "corr_matrix = df_clean.corr()\n",
    "\n",
    "# Calculate p-values for statistical significance\n",
    "p_matrix = pd.DataFrame(index=numeric_columns, columns=numeric_columns)\n",
    "for col1 in numeric_columns:\n",
    "    for col2 in numeric_columns:\n",
    "        if col1 != col2:\n",
    "            corr, p_val = pearsonr(df_clean[col1], df_clean[col2])\n",
    "            p_matrix.loc[col1, col2] = p_val\n",
    "        else:\n",
    "            p_matrix.loc[col1, col2] = 1.0  # Diagonal elements (self-correlation) have p-value = 1\n",
    "\n",
    "# Rename columns for display\n",
    "new_labels = {\n",
    "    \"pm2_5\": \"PM2.5\",\n",
    "    \"air_temperature\": \"T'\",\n",
    "    \"air_humidity\": \"U'\"\n",
    "}\n",
    "corr_matrix_renamed = corr_matrix.rename(columns=new_labels, index=new_labels)\n",
    "\n",
    "# Visualize the correlation matrix\n",
    "plt.figure(figsize=(12, 8))\n",
    "ax = sns.heatmap(corr_matrix_renamed, annot=False, cmap=\"coolwarm\", \n",
    "                 linewidths=0.5, vmin=-1, vmax=1)  # annot=False to avoid default annotations\n",
    "\n",
    "# Add custom annotations: correlation coefficients and asterisks below\n",
    "for i, row in enumerate(corr_matrix_renamed.index):\n",
    "    for j, col in enumerate(corr_matrix_renamed.columns):\n",
    "        if not pd.isna(corr_matrix_renamed.loc[row, col]):  # Skip NaN values\n",
    "            corr_val = corr_matrix_renamed.loc[row, col]\n",
    "            text = f\"{corr_val:.2f}\"\n",
    "            # Place correlation coefficient\n",
    "            ax.text(j + 0.5, i + 0.35, text, ha=\"center\", va=\"center\", fontsize=10)\n",
    "            # Determine significance stars\n",
    "            if row != col:  # Skip diagonal\n",
    "                p_val = p_matrix.loc[numeric_columns[i], numeric_columns[j]]\n",
    "                stars = \"\"\n",
    "                if p_val < 0.001:\n",
    "                    stars = \"***\"\n",
    "                elif p_val < 0.01:\n",
    "                    stars = \"**\"\n",
    "                elif p_val < 0.05:\n",
    "                    stars = \"*\"\n",
    "                # Place stars below the coefficient\n",
    "                ax.text(j + 0.5, i + 0.65, stars, ha=\"center\", va=\"center\", fontsize=10)\n",
    "\n",
    "# Rotate axis labels to horizontal\n",
    "plt.xticks(rotation=0)\n",
    "plt.yticks(rotation=0)\n",
    "\n",
    "# Ensure the output directory exists\n",
    "output_dir = \"output_diagrams\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save the plot with specified resolution\n",
    "plt.savefig(os.path.join(output_dir, \"correlation_matrix_params.png\"), dpi=600, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "# Display significant correlations (|corr| > 0.3)\n",
    "print(\"\\nðŸ“Œ Most significant correlations (|corr| > 0.3):\")\n",
    "strong_correlations = corr_matrix[abs(corr_matrix) > 0.3]\n",
    "print(strong_correlations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking for non-linear dependencies of PM2.5 with weather parameters using logarithmic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 1ï¸âƒ£ Data Preparation\n",
    "df_regression = df_interpolated.copy()\n",
    "\n",
    "# Keep only the necessary columns\n",
    "features = [\"air_temperature\", \"air_humidity\", \"P\", \"DD\", \"Ff\", \"VV\"]\n",
    "df_regression = df_regression[[\"pm2_5\"] + features]\n",
    "\n",
    "# Remove rows with missing values\n",
    "df_regression = df_regression.dropna()\n",
    "\n",
    "# Logarithmic transformation of PM2.5\n",
    "df_regression[\"log_pm2_5\"] = np.log1p(df_regression[\"pm2_5\"])  # log(1 + x) to avoid log(0)\n",
    "\n",
    "# 2ï¸âƒ£ Logarithmic transformation of independent variables (only positive ones)\n",
    "for col in features:\n",
    "    if (df_regression[col] > 0).all():  # For positive values only\n",
    "        df_regression[f\"log_{col}\"] = np.log1p(df_regression[col])\n",
    "\n",
    "# 3ï¸âƒ£ Training the logarithmic regression\n",
    "X = df_regression[[f\"log_{col}\" if f\"log_{col}\" in df_regression.columns else col for col in features]]\n",
    "y = df_regression[\"log_pm2_5\"]\n",
    "\n",
    "X = sm.add_constant(X)  # Add a constant for the intercept\n",
    "\n",
    "model = sm.OLS(y, X).fit()\n",
    "\n",
    "# 4ï¸âƒ£ Display the results\n",
    "print(model.summary())\n",
    "\n",
    "# 5ï¸âƒ£ Plot of actual vs. predicted values\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.scatter(y, model.predict(X), alpha=0.5)\n",
    "plt.xlabel(\"Actual log(PM2.5)\")\n",
    "plt.ylabel(\"Predicted log(PM2.5)\")\n",
    "# plt.title(\"Actual vs. Predicted values\")\n",
    "plt.grid()\n",
    "plt.savefig(os.path.join(output_dir, \"Log_reg_predictions.png\"), dpi=600, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "# 6ï¸âƒ£ Histogram of model residuals\n",
    "residuals = y - model.predict(X)\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.histplot(residuals, bins=30, kde=True)\n",
    "plt.xlabel(\"Residuals\")\n",
    "# plt.title(\"Distribution of residuals\")\n",
    "plt.grid()\n",
    "plt.savefig(os.path.join(output_dir, \"Log_reg_residuals.png\"), dpi=600, bbox_inches=\"tight\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "# 1ï¸âƒ£ Data Preparation\n",
    "df_regression = df_interpolated.copy()\n",
    "\n",
    "# Keep only the necessary columns\n",
    "features = [\"air_temperature\", \"air_humidity\", \"P\", \"DD\", \"Ff\", \"VV\"]\n",
    "df_regression = df_regression[[\"pm2_5\"] + features]\n",
    "\n",
    "# Remove rows with missing values\n",
    "df_regression = df_regression.dropna()\n",
    "\n",
    "# Logarithmic transformation of PM2.5\n",
    "df_regression[\"log_pm2_5\"] = np.log1p(df_regression[\"pm2_5\"])  # log(1 + x) to avoid log(0)\n",
    "\n",
    "# 2ï¸âƒ£ Logarithmic transformation of independent variables\n",
    "# For wind speed, add a small constant before taking the logarithm\n",
    "df_regression[\"log_Ff\"] = np.log(df_regression[\"Ff\"] + 0.1)  # Add 0.1 to handle possible zero values\n",
    "\n",
    "# For the other variables, apply the standard transformation\n",
    "for col in [c for c in features if c != \"Ff\"]:\n",
    "    if (df_regression[col] > 0).all():  # For positive values only\n",
    "        df_regression[f\"log_{col}\"] = np.log1p(df_regression[col])\n",
    "\n",
    "# 3ï¸âƒ£ Training the logarithmic regression with modified features\n",
    "# Use log_Ff instead of Ff\n",
    "modified_features = []\n",
    "for col in features:\n",
    "    if col == \"Ff\":\n",
    "        modified_features.append(\"log_Ff\")\n",
    "    else:\n",
    "        modified_features.append(f\"log_{col}\" if f\"log_{col}\" in df_regression.columns else col)\n",
    "\n",
    "X = df_regression[modified_features]\n",
    "y = df_regression[\"log_pm2_5\"]\n",
    "\n",
    "X = sm.add_constant(X)  # Add a constant for the intercept\n",
    "\n",
    "model = sm.OLS(y, X).fit()\n",
    "\n",
    "# 4ï¸âƒ£ Display the results\n",
    "print(model.summary())\n",
    "\n",
    "# 5ï¸âƒ£ Plot of actual vs. predicted values\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.scatter(y, model.predict(X), alpha=0.5)\n",
    "plt.xlabel(\"Actual log(PM2.5)\")\n",
    "plt.ylabel(\"Predicted log(PM2.5)\")\n",
    "plt.grid()\n",
    "# Create the output directory if it doesn't exist\n",
    "os.makedirs(\"output_dir\", exist_ok=True)\n",
    "plt.savefig(os.path.join(\"output_dir\", \"Log_reg_predictions.png\"), dpi=600, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "# 6ï¸âƒ£ Histogram of model residuals\n",
    "residuals = y - model.predict(X)\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.histplot(residuals, bins=30, kde=True)\n",
    "plt.xlabel(\"Residuals\")\n",
    "plt.grid()\n",
    "plt.savefig(os.path.join(\"output_dir\", \"Log_reg_residuals.png\"), dpi=600, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Load prepared data\n",
    "df = pd.read_csv('df_data_prepared.csv', parse_dates=['date'])\n",
    "\n",
    "# Create a figure with 4 subplots\n",
    "fig, axs = plt.subplots(2, 2, figsize=(12, 10))\n",
    "# fig.suptitle('Relationships between PM2.5 and Meteorological Variables', fontsize=16)\n",
    "\n",
    "# (a) PM2.5 vs air temperature\n",
    "ax = axs[0, 0]\n",
    "x = df['air_temperature'].values\n",
    "y = df['pm2_5'].values\n",
    "mask = ~np.isnan(x) & ~np.isnan(y)\n",
    "ax.scatter(x[mask], y[mask], alpha=0.5, color='blue', s=10)\n",
    "# Trend line\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(x[mask], y[mask])\n",
    "x_line = np.linspace(min(x[mask]), max(x[mask]), 100)\n",
    "y_line = slope * x_line + intercept\n",
    "ax.plot(x_line, y_line, color='red', lw=2)\n",
    "ax.set_xlabel('Air Temperature (Â°C)')\n",
    "ax.set_ylabel('PM2.5 (Âµg/mÂ³)')\n",
    "ax.set_title('(a) PM2.5 vs Air Temperature')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# (b) PM2.5 vs relative humidity\n",
    "ax = axs[0, 1]\n",
    "x = df['air_humidity'].values\n",
    "y = df['pm2_5'].values\n",
    "mask = ~np.isnan(x) & ~np.isnan(y)\n",
    "ax.scatter(x[mask], y[mask], alpha=0.5, color='blue', s=10)\n",
    "# Trend line\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(x[mask], y[mask])\n",
    "x_line = np.linspace(min(x[mask]), max(x[mask]), 100)\n",
    "y_line = slope * x_line + intercept\n",
    "ax.plot(x_line, y_line, color='red', lw=2)\n",
    "ax.set_xlabel('Relative Humidity (%)')\n",
    "ax.set_ylabel('PM2.5 (Âµg/mÂ³)')\n",
    "ax.set_title('(b) PM2.5 vs Relative Humidity')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# (c) PM2.5 vs wind speed (Ff)\n",
    "ax = axs[1, 0]\n",
    "x = df['Ff'].values\n",
    "y = df['pm2_5'].values\n",
    "mask = ~np.isnan(x) & ~np.isnan(y)\n",
    "ax.scatter(x[mask], y[mask], alpha=0.5, color='blue', s=10)\n",
    "# Trend line\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(x[mask], y[mask])\n",
    "x_line = np.linspace(min(x[mask]), max(x[mask]), 100)\n",
    "y_line = slope * x_line + intercept\n",
    "ax.plot(x_line, y_line, color='red', lw=2)\n",
    "ax.set_xlabel('Wind Speed (m/s)')\n",
    "ax.set_ylabel('PM2.5 (Âµg/mÂ³)')\n",
    "ax.set_title('(c) PM2.5 vs Wind Speed')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# (d) PM2.5 vs visibility (VV)\n",
    "ax = axs[1, 1]\n",
    "x = df['VV'].values\n",
    "y = df['pm2_5'].values\n",
    "mask = ~np.isnan(x) & ~np.isnan(y)\n",
    "ax.scatter(x[mask], y[mask], alpha=0.5, color='blue', s=10)\n",
    "# Trend line\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(x[mask], y[mask])\n",
    "x_line = np.linspace(min(x[mask]), max(x[mask]), 100)\n",
    "y_line = slope * x_line + intercept\n",
    "ax.plot(x_line, y_line, color='red', lw=2)\n",
    "ax.set_xlabel('Visibility (km)')\n",
    "ax.set_ylabel('PM2.5 (Âµg/mÂ³)')\n",
    "ax.set_title('(d) PM2.5 vs Visibility')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.savefig(os.path.join(output_dir, 'pm25_meteo_relationships.png'), dpi=600, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('df_data_prepared.csv', parse_dates=['date'])\n",
    "\n",
    "# Load weather data for wind direction mapping\n",
    "df_weather = pd.read_csv('weather_archive_utf8.csv', delimiter=\";\", encoding=\"utf-8\")\n",
    "\n",
    "# Create categorical variables\n",
    "# For temperature: create bins from -25 to 40 with a step of 5\n",
    "temp_bins = np.arange(-25, 41, 5)\n",
    "temp_labels = [f\"{temp_bins[i]} to {temp_bins[i+1]}\" for i in range(len(temp_bins)-1)]\n",
    "df['temp_category'] = pd.cut(df['air_temperature'], bins=temp_bins, labels=temp_labels)\n",
    "\n",
    "# For humidity: create bins from 20 to 100 with a step of 10\n",
    "humidity_bins = np.arange(20, 101, 10)\n",
    "humidity_labels = [f\"{humidity_bins[i]} to {humidity_bins[i+1]}\" for i in range(len(humidity_bins)-1)]\n",
    "df['humidity_category'] = pd.cut(df['air_humidity'], bins=humidity_bins, labels=humidity_labels)\n",
    "\n",
    "# For wind speed: create bins from 0 to 18 with a step of 2\n",
    "wind_bins = np.arange(0, 19, 2)\n",
    "wind_labels = [f\"{wind_bins[i]} to {wind_bins[i+1]}\" for i in range(len(wind_bins)-1)]\n",
    "df['wind_category'] = pd.cut(df['Ff'], bins=wind_bins, labels=wind_labels)\n",
    "\n",
    "# Create wind direction mapping\n",
    "wind_compass_mapping = {\n",
    "    'Wind from the North': 'N', \n",
    "    'Wind from the North-Northeast': 'NNE',\n",
    "    'Wind from the Northeast': 'NE', \n",
    "    'Wind from the East-Northeast': 'ENE',\n",
    "    'Wind from the East': 'E', \n",
    "    'Wind from the East-Southeast': 'ESE',\n",
    "    'Wind from the Southeast': 'SE', \n",
    "    'Wind from the South-Southeast': 'SSE',\n",
    "    'Wind from the South': 'S', \n",
    "    'Wind from the South-Southwest': 'SSW',\n",
    "    'Wind from the Southwest': 'SW', \n",
    "    'Wind from the West-Southwest': 'WSW',\n",
    "    'Wind from the West': 'W', \n",
    "    'Wind from the West-Northwest': 'WNW',\n",
    "    'Wind from the Northwest': 'NW', \n",
    "    'Wind from the North-Northwest': 'NNW',\n",
    "    'Variable direction': 'Variable',\n",
    "    'Calm, no wind': 'Calm'\n",
    "}\n",
    "\n",
    "# Inverted mapping\n",
    "wind_reverse_mapping = {idx: wind_compass_mapping.get(direction, 'Unknown') \n",
    "                        for idx, direction in enumerate(sorted(df_weather[\"DD\"].unique()))}\n",
    "\n",
    "# Apply mapping to the data\n",
    "df['wind_direction_compass'] = df['DD'].map(wind_reverse_mapping)\n",
    "\n",
    "# Clockwise order of directions (excluding special states)\n",
    "direction_order = ['N', 'NNE', 'NE', 'ENE', \n",
    "                   'E', 'ESE', 'SE', 'SSE', \n",
    "                   'S', 'SSW', 'SW', 'WSW', \n",
    "                   'W', 'WNW', 'NW', 'NNW']\n",
    "\n",
    "# Filter data, including only standard directions\n",
    "df_filtered = df[df['wind_direction_compass'].isin(direction_order)]\n",
    "\n",
    "# Figure setup\n",
    "fig, axs = plt.subplots(2, 2, figsize=(14, 12))\n",
    "# fig.suptitle('Distribution of PM2.5 across Meteorological Parameters', fontsize=16)\n",
    "\n",
    "# Calculate the upper limit with a small margin\n",
    "y_max = df['pm2_5'].quantile(0.99)  # Increased to the 99th percentile\n",
    "y_max_with_margin = y_max * 1.1  # Add a 10% margin\n",
    "\n",
    "# (a) PM2.5 by temperature ranges\n",
    "sns.boxplot(x='temp_category', y='pm2_5', data=df, ax=axs[0, 0], color='skyblue')\n",
    "axs[0, 0].set_xlabel('Air Temperature Range (Â°C)')\n",
    "axs[0, 0].set_ylabel('PM2.5 (Âµg/mÂ³)')\n",
    "axs[0, 0].set_title('(a) PM2.5 Distribution by Air Temperature')\n",
    "axs[0, 0].tick_params(axis='x', rotation=45)\n",
    "axs[0, 0].grid(True, linestyle='--', alpha=0.7)\n",
    "axs[0, 0].set_ylim(0, y_max_with_margin)\n",
    "\n",
    "# (b) PM2.5 by humidity ranges\n",
    "sns.boxplot(x='humidity_category', y='pm2_5', data=df, ax=axs[0, 1], color='lightgreen')\n",
    "axs[0, 1].set_xlabel('Relative Humidity Range (%)')\n",
    "axs[0, 1].set_ylabel('PM2.5 (Âµg/mÂ³)')\n",
    "axs[0, 1].set_title('(b) PM2.5 Distribution by Relative Humidity')\n",
    "axs[0, 1].tick_params(axis='x', rotation=45)\n",
    "axs[0, 1].grid(True, linestyle='--', alpha=0.7)\n",
    "axs[0, 1].set_ylim(0, y_max_with_margin)\n",
    "\n",
    "# (c) PM2.5 by wind speed ranges\n",
    "sns.boxplot(x='wind_category', y='pm2_5', data=df, ax=axs[1, 0], color='lightsalmon')\n",
    "axs[1, 0].set_xlabel('Wind Speed Range (m/s)')\n",
    "axs[1, 0].set_ylabel('PM2.5 (Âµg/mÂ³)')\n",
    "axs[1, 0].set_title('(c) PM2.5 Distribution by Wind Speed')\n",
    "axs[1, 0].tick_params(axis='x', rotation=45)\n",
    "axs[1, 0].grid(True, linestyle='--', alpha=0.7)\n",
    "axs[1, 0].set_ylim(0, y_max_with_margin)\n",
    "\n",
    "# (d) PM2.5 by wind directions\n",
    "sns.boxplot(x='wind_direction_compass', y='pm2_5', data=df_filtered, \n",
    "            order=direction_order, ax=axs[1, 1], color='lightblue')\n",
    "axs[1, 1].set_xlabel('Wind Direction')\n",
    "axs[1, 1].set_ylabel('PM2.5 (Âµg/mÂ³)')\n",
    "axs[1, 1].set_title('(d) PM2.5 Distribution by Wind Direction')\n",
    "axs[1, 1].tick_params(axis='x', rotation=45)\n",
    "axs[1, 1].grid(True, linestyle='--', alpha=0.7)\n",
    "axs[1, 1].set_ylim(0, y_max_with_margin)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "output_dir = 'output_diagrams'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "plt.savefig(os.path.join(output_dir, 'pm25_distribution_by_meteo_extended.png'), dpi=600, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Additional statistical analysis of wind directions\n",
    "from scipy import stats\n",
    "\n",
    "# One-way analysis of variance (ANOVA)\n",
    "wind_groups = [group['pm2_5'].dropna() for name, group in df_filtered.groupby('wind_direction_compass') if len(group['pm2_5'].dropna()) > 0]\n",
    "\n",
    "if len(wind_groups) >= 2:\n",
    "    f_statistic, p_value = stats.f_oneway(*wind_groups)\n",
    "    print(\"\\nWind Direction ANOVA Results:\")\n",
    "    print(f\"F-statistic: {f_statistic}\")\n",
    "    print(f\"p-value: {p_value}\")\n",
    "\n",
    "# Output statistics by wind direction\n",
    "print(\"\\nWind Direction PM2.5 Statistics:\")\n",
    "wind_stats = df_filtered.groupby('wind_direction_compass')['pm2_5'].agg(['count', 'mean', 'std', 'min', 'max'])\n",
    "print(wind_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# File path\n",
    "file_path = \"weather_archive_utf8.csv\"\n",
    "\n",
    "# Select only the necessary columns (remove \"c\")\n",
    "columns_to_import = [\"time\", \"T\", \"P0\", \"P\", \"U\", \"DD\", \"Ff\", \"VV\"]\n",
    "\n",
    "# Import data\n",
    "df_weather = pd.read_csv(file_path, delimiter=\";\", usecols=columns_to_import, encoding=\"utf-8\")\n",
    "\n",
    "# Unique wind directions before transformation\n",
    "print(\"Unique wind directions in the source file:\")\n",
    "print(df_weather[\"DD\"].unique())\n",
    "\n",
    "# Create wind direction mapping\n",
    "wind_mapping = {direction: idx for idx, direction in enumerate(sorted(df_weather[\"DD\"].unique()))}\n",
    "reverse_wind_mapping = {idx: direction for direction, idx in wind_mapping.items()}\n",
    "\n",
    "print(\"\\nWind direction mapping:\")\n",
    "for idx, direction in reverse_wind_mapping.items():\n",
    "    print(f\"{idx}: {direction}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import os\n",
    "from scipy import stats\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('df_data_prepared.csv', parse_dates=['date'])\n",
    "\n",
    "# Load weather data for wind direction mapping\n",
    "df_weather = pd.read_csv('weather_archive_utf8.csv', delimiter=\";\", encoding=\"utf-8\")\n",
    "\n",
    "# Create a precise mapping for wind directions\n",
    "wind_compass_mapping = {\n",
    "    'Wind from the North': 'N', \n",
    "    'Wind from the North-Northeast': 'NNE',\n",
    "    'Wind from the Northeast': 'NE', \n",
    "    'Wind from the East-Northeast': 'ENE',\n",
    "    'Wind from the East': 'E', \n",
    "    'Wind from the East-Southeast': 'ESE',\n",
    "    'Wind from the Southeast': 'SE', \n",
    "    'Wind from the South-Southeast': 'SSE',\n",
    "    'Wind from the South': 'S', \n",
    "    'Wind from the South-Southwest': 'SSW',\n",
    "    'Wind from the Southwest': 'SW', \n",
    "    'Wind from the West-Southwest': 'WSW',\n",
    "    'Wind from the West': 'W', \n",
    "    'Wind from the West-Northwest': 'WNW',\n",
    "    'Wind from the Northwest': 'NW', \n",
    "    'Wind from the North-Northwest': 'NNW',\n",
    "    'Variable direction': 'Variable',\n",
    "    'Calm, no wind': 'Calm'\n",
    "}\n",
    "\n",
    "# Inverted mapping\n",
    "wind_reverse_mapping = {idx: wind_compass_mapping.get(direction, 'Unknown') \n",
    "                        for idx, direction in enumerate(sorted(df_weather[\"DD\"].unique()))}\n",
    "\n",
    "# Apply mapping to the data\n",
    "df['wind_direction_compass'] = df['DD'].map(wind_reverse_mapping)\n",
    "\n",
    "# Clockwise order of directions (excluding special states)\n",
    "direction_order = ['N', 'NNE', 'NE', 'ENE', \n",
    "                   'E', 'ESE', 'SE', 'SSE', \n",
    "                   'S', 'SSW', 'SW', 'WSW', \n",
    "                   'W', 'WNW', 'NW', 'NNW']\n",
    "\n",
    "# Filter data, excluding special states\n",
    "df_filtered = df[df['wind_direction_compass'].isin(direction_order)]\n",
    "\n",
    "# Create a figure for visualization\n",
    "plt.figure(figsize=(16, 10))\n",
    "\n",
    "# Data preparation\n",
    "wind_pm25_summary = df_filtered.groupby('wind_direction_compass')['pm2_5'].agg(['count', 'mean', 'std'])\n",
    "wind_pm25_summary['ci'] = 1.96 * (wind_pm25_summary['std'] / np.sqrt(wind_pm25_summary['count']))\n",
    "wind_pm25_summary = wind_pm25_summary.reindex(direction_order)\n",
    "\n",
    "# Building a boxplot\n",
    "plt.subplot(2, 1, 1)\n",
    "sns.boxplot(x='wind_direction_compass', y='pm2_5', data=df_filtered, \n",
    "            order=direction_order)\n",
    "plt.title('PM2.5 Distribution by Wind Direction (Boxplot)')\n",
    "plt.xlabel('Wind Direction')\n",
    "plt.ylabel('PM2.5 (Âµg/mÂ³)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "# Building a bar chart of mean values with confidence intervals\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.bar(direction_order, wind_pm25_summary['mean'], \n",
    "        yerr=wind_pm25_summary['ci'], \n",
    "        capsize=5, \n",
    "        color='skyblue', \n",
    "        edgecolor='navy')\n",
    "plt.title('Average PM2.5 by Wind Direction with 95% Confidence Intervals')\n",
    "plt.xlabel('Wind Direction')\n",
    "plt.ylabel('Average PM2.5 (Âµg/mÂ³)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True, axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('wind_direction_pm25_analysis.png', dpi=600)\n",
    "\n",
    "# Output statistics\n",
    "print(\"PM2.5 Statistics by Wind Direction:\")\n",
    "print(wind_pm25_summary)\n",
    "\n",
    "# Prepare data for ANOVA\n",
    "wind_groups = [group['pm2_5'].dropna() for name, group in df_filtered.groupby('wind_direction_compass') if len(group['pm2_5'].dropna()) > 0]\n",
    "\n",
    "# Check for a sufficient number of groups\n",
    "if len(wind_groups) >= 2:\n",
    "    # One-way analysis of variance (ANOVA)\n",
    "    f_statistic, p_value = stats.f_oneway(*wind_groups)\n",
    "\n",
    "    print(\"\\nOne-way ANOVA results:\")\n",
    "    print(f\"F-statistic: {f_statistic}\")\n",
    "    print(f\"p-value: {p_value}\")\n",
    "else:\n",
    "    print(\"\\nNot enough groups for ANOVA analysis\")\n",
    "\n",
    "# Visual check of distribution by direction\n",
    "plt.figure(figsize=(10, 6))\n",
    "wind_pm25_summary[['mean', 'count']].plot(kind='bar', secondary_y='count')\n",
    "plt.title('Mean PM2.5 and Sample Count by Wind Direction')\n",
    "plt.xlabel('Wind Direction')\n",
    "plt.ylabel('Mean PM2.5 (Âµg/mÂ³)')\n",
    "plt.legend(['Mean PM2.5', 'Sample Count'])\n",
    "plt.tight_layout()\n",
    "plt.savefig('wind_direction_sample_count.png', dpi=600)\n",
    "\n",
    "# Additional information on direction distribution\n",
    "print(\"\\nWind Direction Distribution:\")\n",
    "print(df['wind_direction_compass'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of non-linear dependencies using a Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# ðŸ“Œ Load data (if not already loaded)\n",
    "df_model_data = df_interpolated.copy()\n",
    "\n",
    "# ðŸ“Œ Select variables for analysis\n",
    "features = [\"air_temperature\", \"air_humidity\", \"T\", \"P0\", \"P\", \"U\", \"DD\", \"Ff\", \"VV\"]\n",
    "target = \"pm2_5\"\n",
    "\n",
    "# ðŸ“Œ Remove rows with missing values in the selected columns\n",
    "df_model_data = df_model_data.dropna(subset=[target] + features)\n",
    "\n",
    "# ðŸ“Œ Split data into training and testing sets (80% / 20%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_model_data[features], df_model_data[target], test_size=0.2, random_state=42)\n",
    "\n",
    "# ðŸ“Œ Find optimal model parameters using GridSearchCV\n",
    "param_grid = {\"max_depth\": [3, 5, 10, 15], \"min_samples_split\": [2, 5, 10]}\n",
    "grid_search = GridSearchCV(DecisionTreeRegressor(random_state=42), param_grid, cv=5, scoring=\"r2\")\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# ðŸ“Œ Best decision tree model\n",
    "best_tree = grid_search.best_estimator_\n",
    "\n",
    "# ðŸ“Œ Make a prediction\n",
    "y_pred = best_tree.predict(X_test)\n",
    "\n",
    "# ðŸ“Œ Evaluate the model's quality\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"ðŸ”¹ MAE: {mae:.2f}\")\n",
    "print(f\"ðŸ”¹ RMSE: {rmse:.2f}\")\n",
    "print(f\"ðŸ”¹ RÂ²: {r2:.3f}\")\n",
    "\n",
    "# ðŸ“Œ Visualization of predicted vs. actual values\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \"--\", color=\"red\")  # Ideal prediction line\n",
    "plt.xlabel(\"Actual PM2.5\")\n",
    "plt.ylabel(\"Predicted PM2.5\")\n",
    "plt.title(\"Actual vs Predicted values (Decision Tree)\")\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# ðŸ“Œ Feature importance\n",
    "feature_importance = pd.Series(best_tree.feature_importances_, index=features).sort_values(ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "feature_importance.plot(kind=\"bar\", color=\"royalblue\")\n",
    "plt.title(\"Feature Importance (Decision Tree)\")\n",
    "plt.xlabel(\"Features\")\n",
    "plt.ylabel(\"Importance\")\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking Gradient Boosting for finding correlations with PM2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# ðŸ“Œ Load data (assuming df_interpolated is already prepared)\n",
    "features = ['T', 'P0', 'P', 'U', 'DD', 'Ff', 'VV', 'air_temperature', 'air_humidity']\n",
    "target = 'pm2_5'\n",
    "\n",
    "# Remove missing values\n",
    "filtered_pm25_data = df_interpolated.dropna(subset=[target] + features)\n",
    "\n",
    "# Split data into features (X) and target variable (y)\n",
    "X = filtered_pm25_data[features]\n",
    "y = filtered_pm25_data[target]\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# ðŸ“Œ Train the Gradient Boosting model\n",
    "model = GradientBoostingRegressor(n_estimators=200, learning_rate=0.05, max_depth=4, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# ðŸ“Œ Model evaluation\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f'ðŸ”¹ MAE: {mae:.2f}')\n",
    "print(f'ðŸ”¹ RMSE: {rmse:.2f}')\n",
    "print(f'ðŸ”¹ RÂ²: {r2:.3f}')\n",
    "\n",
    "# ðŸ“Œ Visualization of feature importance\n",
    "feature_importance = model.feature_importances_\n",
    "feature_names = X.columns\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(feature_names, feature_importance, color='blue', alpha=0.7)\n",
    "plt.xlabel(\"Features\")\n",
    "plt.ylabel(\"Importance\")\n",
    "plt.title(\"Feature Importance (Gradient Boosting)\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "# ðŸ“Œ Visualization of predicted vs. actual values\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "plt.plot([y.min(), y.max()], [y.min(), y.max()], 'r--')  # Diagonal line of ideal prediction\n",
    "plt.xlabel(\"Actual PM2.5\")\n",
    "plt.ylabel(\"Predicted PM2.5\")\n",
    "plt.title(\"Actual vs Predicted values (Gradient Boosting)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking Random Forest for finding correlations with PM2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# ðŸ“Œ Load data (assuming df_interpolated is already prepared)\n",
    "features = ['T', 'P0', 'P', 'U', 'DD', 'Ff', 'VV', 'air_temperature', 'air_humidity']\n",
    "target = 'pm2_5'\n",
    "\n",
    "# Remove missing values\n",
    "filtered_pm25_data = df_interpolated.dropna(subset=[target] + features)\n",
    "\n",
    "# Split data into features (X) and target variable (y)\n",
    "X = filtered_pm25_data[features]\n",
    "y = filtered_pm25_data[target]\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# ðŸ“Œ Train the Random Forest model\n",
    "model = RandomForestRegressor(n_estimators=200, max_depth=10, random_state=42, n_jobs=-1)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# ðŸ“Œ Model evaluation\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f'ðŸ”¹ MAE: {mae:.2f}')\n",
    "print(f'ðŸ”¹ RMSE: {rmse:.2f}')\n",
    "print(f'ðŸ”¹ RÂ²: {r2:.3f}')\n",
    "\n",
    "# ðŸ“Œ Visualization of feature importance\n",
    "feature_importance = model.feature_importances_\n",
    "feature_names = X.columns\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(feature_names, feature_importance, color='blue', alpha=0.7)\n",
    "plt.xlabel(\"Features\")\n",
    "plt.ylabel(\"Importance\")\n",
    "plt.title(\"Feature Importance (Random Forest)\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "# ðŸ“Œ Visualization of predicted vs. actual values\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "plt.plot([y.min(), y.max()], [y.min(), y.max()], 'r--')  # Diagonal line of ideal prediction\n",
    "plt.xlabel(\"Actual PM2.5\")\n",
    "plt.ylabel(\"Predicted PM2.5\")\n",
    "plt.title(\"Actual vs Predicted values (Random Forest)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More complex approximations of non-linear dependencies. A neural network based on TensorFlow/Keras. MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# ðŸ“Œ Data loading and preprocessing\n",
    "features = ['T', 'P0', 'P', 'U', 'DD', 'Ff', 'VV', 'air_temperature', 'air_humidity']\n",
    "target = 'pm2_5'\n",
    "\n",
    "# Remove missing values\n",
    "filtered_pm25_data = df_interpolated.dropna(subset=[target] + features)\n",
    "\n",
    "# Split into features (X) and target variable (y)\n",
    "X = filtered_pm25_data[features]\n",
    "y = filtered_pm25_data[target]\n",
    "\n",
    "# Data normalization (very important for neural networks!)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# ðŸ“Œ Create the neural network model\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dense(1)  # Output layer for PM2.5 prediction\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "# ðŸ“Œ Train the model\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, batch_size=32, verbose=1)\n",
    "\n",
    "# ðŸ“Œ Evaluate the model\n",
    "loss, mae = model.evaluate(X_test, y_test)\n",
    "print(f'ðŸ”¹ MAE: {mae:.2f}')\n",
    "print(f'ðŸ”¹ RMSE: {np.sqrt(loss):.2f}')\n",
    "\n",
    "# ðŸ“Œ Visualize the training process\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history.history['mae'], label='MAE (Train)')\n",
    "plt.plot(history.history['val_mae'], label='MAE (Validation)')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"MAE\")\n",
    "plt.title(\"Model Training Process\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# ðŸ“Œ Visualization of predicted vs. actual values\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "plt.plot([y.min(), y.max()], [y.min(), y.max()], 'r--')  # Diagonal line of ideal prediction\n",
    "plt.xlabel(\"Actual PM2.5\")\n",
    "plt.ylabel(\"Predicted PM2.5\")\n",
    "plt.title(\"Actual vs Predicted values (Neural Network)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using GPU in TensorFlow\n",
    "\n",
    "This code checks for the availability of physical GPU devices for use in TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.config.list_physical_devices('GPU'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation analysis between PM2.5 and temporal parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Assuming the df_interpolated DataFrame is already loaded and contains \"date\" and \"pm2_5\" columns\n",
    "df_corr_an_pm_time = df_interpolated.copy()\n",
    "\n",
    "# Convert the 'date' column to datetime type (if not already converted)\n",
    "df_corr_an_pm_time['date'] = pd.to_datetime(df_corr_an_pm_time['date'], errors='coerce')\n",
    "\n",
    "# Extract temporal features\n",
    "df_corr_an_pm_time['month'] = df_corr_an_pm_time['date'].dt.month\n",
    "df_corr_an_pm_time['day'] = df_corr_an_pm_time['date'].dt.day\n",
    "df_corr_an_pm_time['hour'] = df_corr_an_pm_time['date'].dt.hour\n",
    "df_corr_an_pm_time['dayofweek'] = df_corr_an_pm_time['date'].dt.dayofweek\n",
    "# Simple seasonal feature: 1 â€“ winter, 2 â€“ spring, 3 â€“ summer, 4 â€“ autumn\n",
    "df_corr_an_pm_time['season'] = df_corr_an_pm_time['month'] % 12 // 3 + 1\n",
    "\n",
    "# Select columns for correlation analysis\n",
    "cols = ['pm2_5', 'month', 'day', 'hour', 'dayofweek', 'season']\n",
    "corr_matrix = df_corr_an_pm_time[cols].corr(method='pearson')\n",
    "\n",
    "# Display the correlation matrix\n",
    "print(\"Pearson Correlation Matrix:\")\n",
    "print(corr_matrix)\n",
    "\n",
    "# Visualization using a heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title(\"Pearson Correlation Matrix: PM2.5 and Temporal Features\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from scipy import stats\n",
    "\n",
    "# Adjust font sizes\n",
    "plt.rcParams.update({\n",
    "    'font.size': 14,  # Increase base font size\n",
    "    'axes.titlesize': 16,  # Plot title size\n",
    "    'axes.labelsize': 14,  # Axis label size\n",
    "    'xtick.labelsize': 14,  # X-axis tick label size\n",
    "    'ytick.labelsize': 14   # Y-axis tick label size\n",
    "})\n",
    "\n",
    "# Create a directory to save results if it doesn't exist\n",
    "output_dir = 'output_diagrams'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(\"df_data_prepared.csv\")\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "# Extract temporal features\n",
    "df['hour'] = df['date'].dt.hour\n",
    "df['day_of_week'] = df['date'].dt.dayofweek  # 0 = Monday, 6 = Sunday\n",
    "df['month'] = df['date'].dt.month\n",
    "df['season'] = (df['date'].dt.month % 12 // 3 + 1)  # 1-Winter, 2-Spring, 3-Summer, 4-Autumn\n",
    "\n",
    "# Select features for correlation analysis\n",
    "features = ['pm2_5', 'hour', 'day_of_week', 'month', 'season']\n",
    "df_corr = df[features].dropna()\n",
    "\n",
    "# Calculate Pearson and Spearman correlations\n",
    "pearson_corr = df_corr.corr(method='pearson')\n",
    "spearman_corr = df_corr.corr(method='spearman')\n",
    "\n",
    "# Function to get p-value for Pearson correlation\n",
    "def get_pearson_p_value(x, y):\n",
    "    return stats.pearsonr(x, y)[1]\n",
    "\n",
    "# Function to get p-value for Spearman correlation\n",
    "def get_spearman_p_value(x, y):\n",
    "    return stats.spearmanr(x, y)[1]\n",
    "\n",
    "# Calculate p-values for correlations\n",
    "pearson_p_values = pd.DataFrame(np.zeros((len(features), len(features))), \n",
    "                               index=features, columns=features)\n",
    "spearman_p_values = pd.DataFrame(np.zeros((len(features), len(features))), \n",
    "                                index=features, columns=features)\n",
    "\n",
    "for i, feat1 in enumerate(features):\n",
    "    for j, feat2 in enumerate(features):\n",
    "        if i != j:  # Avoid diagonal elements (correlation of a variable with itself)\n",
    "            data1 = df_corr[feat1].values\n",
    "            data2 = df_corr[feat2].values\n",
    "            pearson_p_values.iloc[i, j] = get_pearson_p_value(data1, data2)\n",
    "            spearman_p_values.iloc[i, j] = get_spearman_p_value(data1, data2)\n",
    "\n",
    "# Function to get asterisks based on p-values\n",
    "def get_stars(p_val):\n",
    "    if np.isnan(p_val):\n",
    "        return \"\"\n",
    "    if p_val < 0.001:\n",
    "        return \"***\"\n",
    "    elif p_val < 0.01:\n",
    "        return \"**\"\n",
    "    elif p_val < 0.05:\n",
    "        return \"*\"\n",
    "    return \"\"\n",
    "\n",
    "# Create annotations with asterisks below the values\n",
    "pearson_annot = np.empty_like(pearson_corr.values, dtype=object)\n",
    "spearman_annot = np.empty_like(spearman_corr.values, dtype=object)\n",
    "\n",
    "for i in range(len(features)):\n",
    "    for j in range(len(features)):\n",
    "        if i == j:  # Diagonal elements (correlation of a variable with itself)\n",
    "            pearson_annot[i, j] = f\"{pearson_corr.iloc[i, j]:.2f}\"\n",
    "            spearman_annot[i, j] = f\"{spearman_corr.iloc[i, j]:.2f}\"\n",
    "        else:\n",
    "            stars_p = get_stars(pearson_p_values.iloc[i, j])\n",
    "            stars_s = get_stars(spearman_p_values.iloc[i, j])\n",
    "            \n",
    "            pearson_annot[i, j] = f\"{pearson_corr.iloc[i, j]:.2f}\\n{stars_p}\"\n",
    "            spearman_annot[i, j] = f\"{spearman_corr.iloc[i, j]:.2f}\\n{stars_s}\"\n",
    "\n",
    "# Create a matrix for combined display\n",
    "combined_annot = pearson_annot.copy()\n",
    "combined_corr = pearson_corr.copy()\n",
    "\n",
    "# Fill the upper triangle with Spearman values\n",
    "mask_upper = np.triu_indices(len(features), k=1)\n",
    "for i, j in zip(*mask_upper):\n",
    "    combined_corr.iloc[i, j] = spearman_corr.iloc[i, j]\n",
    "    combined_annot[i, j] = spearman_annot[i, j]\n",
    "\n",
    "# Create the figure\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Create a mask - show the entire matrix\n",
    "mask = np.zeros_like(combined_corr, dtype=bool)\n",
    "\n",
    "# Build a heatmap with multi-line annotations\n",
    "sns.heatmap(combined_corr, annot=combined_annot, fmt='', cmap='coolwarm',\n",
    "            vmin=-1, vmax=1, mask=mask, cbar_kws={'label': 'Correlation coefficient'})\n",
    "\n",
    "# Create more readable labels\n",
    "feature_labels = ['PM2.5', 'Hour', 'Day of Week', 'Month', 'Season']\n",
    "plt.xticks(np.arange(len(feature_labels))+0.5, feature_labels, rotation=45)\n",
    "plt.yticks(np.arange(len(feature_labels))+0.5, feature_labels)\n",
    "\n",
    "# plt.title('Correlation between PM2.5 and Temporal Features', fontsize=14)\n",
    "\n",
    "# Add explanatory captions\n",
    "plt.figtext(0.05, 0.00, 'Pearson (lower triangle) / Spearman (upper triangle)', fontsize=12)\n",
    "plt.figtext(0.65, 0.00, '* p<0.05, ** p<0.01, *** p<0.001', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, 'temporal_correlation_heatmap.png'), dpi=600, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of PM2.5 trends by temporal features with confidence intervals\n",
    "\n",
    "- Function to calculate the 95% confidence interval for a data series.\n",
    "- Grouping data by hour, month, and day of the week, calculating the mean, standard error, and confidence intervals.\n",
    "- Visualization of PM2.5 trends by hour, month, and day of the week with confidence intervals displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import os\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "output_dir = 'output_diagrams'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Load data (if not already loaded)\n",
    "df = pd.read_csv(\"df_data_prepared.csv\")\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "# Extract temporal features\n",
    "df['hour'] = df['date'].dt.hour\n",
    "df['month'] = df['date'].dt.month\n",
    "df['dayofweek'] = df['date'].dt.dayofweek  # 0 = Monday, 6 = Sunday\n",
    "\n",
    "# Create DataFrame for correlation analysis\n",
    "df_corr_an_pm_time = df[['pm2_5', 'hour', 'month', 'dayofweek']].dropna()\n",
    "\n",
    "# Function to calculate the 95% confidence interval for a data series\n",
    "def compute_ci(series, confidence=0.95):\n",
    "    n = len(series)\n",
    "    mean = np.mean(series)\n",
    "    std_err = stats.sem(series)\n",
    "    h = std_err * stats.t.ppf((1 + confidence) / 2, n - 1)\n",
    "    return mean, mean - h, mean + h\n",
    "\n",
    "# Group by hour\n",
    "hour_stats = df_corr_an_pm_time.groupby('hour')['pm2_5'].agg(['mean', 'count', 'std']).reset_index()\n",
    "hour_stats['sem'] = hour_stats['std'] / np.sqrt(hour_stats['count'])\n",
    "hour_stats['ci_lower'] = hour_stats.apply(lambda row: row['mean'] - row['sem'] * stats.t.ppf(0.975, row['count'] - 1), axis=1)\n",
    "hour_stats['ci_upper'] = hour_stats.apply(lambda row: row['mean'] + row['sem'] * stats.t.ppf(0.975, row['count'] - 1), axis=1)\n",
    "\n",
    "# Group by month\n",
    "month_stats = df_corr_an_pm_time.groupby('month')['pm2_5'].agg(['mean', 'count', 'std']).reset_index()\n",
    "month_stats['sem'] = month_stats['std'] / np.sqrt(month_stats['count'])\n",
    "month_stats['ci_lower'] = month_stats.apply(lambda row: row['mean'] - row['sem'] * stats.t.ppf(0.975, row['count'] - 1), axis=1)\n",
    "month_stats['ci_upper'] = month_stats.apply(lambda row: row['mean'] + row['sem'] * stats.t.ppf(0.975, row['count'] - 1), axis=1)\n",
    "\n",
    "# Group by day of week\n",
    "day_stats = df_corr_an_pm_time.groupby('dayofweek')['pm2_5'].agg(['mean', 'count', 'std']).reset_index()\n",
    "day_stats['sem'] = day_stats['std'] / np.sqrt(day_stats['count'])\n",
    "day_stats['ci_lower'] = day_stats.apply(lambda row: row['mean'] - row['sem'] * stats.t.ppf(0.975, row['count'] - 1), axis=1)\n",
    "day_stats['ci_upper'] = day_stats.apply(lambda row: row['mean'] + row['sem'] * stats.t.ppf(0.975, row['count'] - 1), axis=1)\n",
    "\n",
    "# Improve plot style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams.update({\n",
    "    'font.size': 12,\n",
    "    'axes.titlesize': 14,\n",
    "    'axes.labelsize': 12,\n",
    "    'xtick.labelsize': 11,\n",
    "    'ytick.labelsize': 11\n",
    "})\n",
    "\n",
    "# Visualization of trend by hour, month, and day of week\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Trend by hour (0-23)\n",
    "axes[0].errorbar(hour_stats['hour'], hour_stats['mean'],\n",
    "                yerr=[hour_stats['mean'] - hour_stats['ci_lower'], hour_stats['ci_upper'] - hour_stats['mean']],\n",
    "                fmt='-o', capsize=5, color='#1f77b4')\n",
    "axes[0].set_xlabel(\"Hour of Day\")\n",
    "axes[0].set_ylabel(\"Mean PM2.5 (Âµg/mÂ³)\")\n",
    "axes[0].set_title(\"PM2.5 Trend by Hour of Day\")\n",
    "axes[0].set_xticks(range(0, 24, 2))\n",
    "\n",
    "# Trend by month (1-12)\n",
    "axes[1].errorbar(month_stats['month'], month_stats['mean'],\n",
    "                yerr=[month_stats['mean'] - month_stats['ci_lower'], month_stats['ci_upper'] - month_stats['mean']],\n",
    "                fmt='-o', capsize=5, color='#1f77b4')\n",
    "axes[1].set_xlabel(\"Month\")\n",
    "axes[1].set_ylabel(\"Mean PM2.5 (Âµg/mÂ³)\")\n",
    "axes[1].set_title(\"PM2.5 Trend by Month\")\n",
    "axes[1].set_xticks(range(1, 13))\n",
    "axes[1].set_xticklabels(['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'])\n",
    "\n",
    "# Trend by day of week (0 = Monday, 6 = Sunday)\n",
    "axes[2].errorbar(day_stats['dayofweek'], day_stats['mean'],\n",
    "                yerr=[day_stats['mean'] - day_stats['ci_lower'], day_stats['ci_upper'] - day_stats['mean']],\n",
    "                fmt='-o', capsize=5, color='#1f77b4')\n",
    "axes[2].set_xlabel(\"Day of Week\")\n",
    "axes[2].set_ylabel(\"Mean PM2.5 (Âµg/mÂ³)\")\n",
    "axes[2].set_title(\"PM2.5 Trend by Day of Week\")\n",
    "axes[2].set_xticks(range(0, 7))\n",
    "axes[2].set_xticklabels(['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'])\n",
    "\n",
    "# Add a figure-level caption\n",
    "# fig.text(0.5, 0.01, 'Figure 5. Temporal trends in PM2.5 concentrations with 95% confidence intervals', ha='center', fontsize=14)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 1])\n",
    "plt.savefig(os.path.join(output_dir, 'temporal_trends_PM25.png'), dpi=600, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Figure saved to {os.path.join(output_dir, 'temporal_trends_PM25.png')}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import os\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "output_dir = 'output_diagrams'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Load data (if not already loaded)\n",
    "df = pd.read_csv(\"df_data_prepared.csv\")\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "# Extract temporal features\n",
    "df['hour'] = df['date'].dt.hour\n",
    "df['month'] = df['date'].dt.month\n",
    "df['dayofweek'] = df['date'].dt.dayofweek  # 0 = Monday, 6 = Sunday\n",
    "\n",
    "# Create DataFrame for correlation analysis\n",
    "df_corr_an_pm_time = df[['pm2_5', 'hour', 'month', 'dayofweek']].dropna()\n",
    "\n",
    "# Function to calculate the 95% confidence interval for a data series\n",
    "def compute_ci(series, confidence=0.95):\n",
    "    n = len(series)\n",
    "    mean = np.mean(series)\n",
    "    std_err = stats.sem(series)\n",
    "    h = std_err * stats.t.ppf((1 + confidence) / 2, n - 1)\n",
    "    return mean, mean - h, mean + h\n",
    "\n",
    "# Group by hour\n",
    "hour_stats = df_corr_an_pm_time.groupby('hour')['pm2_5'].agg(['mean', 'count', 'std']).reset_index()\n",
    "hour_stats['sem'] = hour_stats['std'] / np.sqrt(hour_stats['count'])\n",
    "hour_stats['ci_lower'] = hour_stats.apply(lambda row: row['mean'] - row['sem'] * stats.t.ppf(0.975, row['count'] - 1), axis=1)\n",
    "hour_stats['ci_upper'] = hour_stats.apply(lambda row: row['mean'] + row['sem'] * stats.t.ppf(0.975, row['count'] - 1), axis=1)\n",
    "\n",
    "# Group by month\n",
    "month_stats = df_corr_an_pm_time.groupby('month')['pm2_5'].agg(['mean', 'count', 'std']).reset_index()\n",
    "month_stats['sem'] = month_stats['std'] / np.sqrt(month_stats['count'])\n",
    "month_stats['ci_lower'] = month_stats.apply(lambda row: row['mean'] - row['sem'] * stats.t.ppf(0.975, row['count'] - 1), axis=1)\n",
    "month_stats['ci_upper'] = month_stats.apply(lambda row: row['mean'] + row['sem'] * stats.t.ppf(0.975, row['count'] - 1), axis=1)\n",
    "\n",
    "# Group by day of week\n",
    "day_stats = df_corr_an_pm_time.groupby('dayofweek')['pm2_5'].agg(['mean', 'count', 'std']).reset_index()\n",
    "day_stats['sem'] = day_stats['std'] / np.sqrt(day_stats['count'])\n",
    "day_stats['ci_lower'] = day_stats.apply(lambda row: row['mean'] - row['sem'] * stats.t.ppf(0.975, row['count'] - 1), axis=1)\n",
    "day_stats['ci_upper'] = day_stats.apply(lambda row: row['mean'] + row['sem'] * stats.t.ppf(0.975, row['count'] - 1), axis=1)\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "plt.rcParams.update({\n",
    "    'font.size': 12,\n",
    "    'axes.titlesize': 14,\n",
    "    'axes.labelsize': 12,\n",
    "    'xtick.labelsize': 11,\n",
    "    'ytick.labelsize': 11\n",
    "})\n",
    "\n",
    "# Create a figure with three subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Configure colors\n",
    "main_color = '#2077b4'\n",
    "fill_color = '#75b3e0'\n",
    "\n",
    "# 1. Trend by hour of the day with a filled area under the plot\n",
    "axes[0].errorbar(hour_stats['hour'], hour_stats['mean'],\n",
    "                yerr=[hour_stats['mean'] - hour_stats['ci_lower'], hour_stats['ci_upper'] - hour_stats['mean']],\n",
    "                fmt='o', capsize=5, color=main_color, markersize=8, ecolor='gray', elinewidth=1)\n",
    "axes[0].plot(hour_stats['hour'], hour_stats['mean'], color=main_color, alpha=0.8)\n",
    "axes[0].fill_between(hour_stats['hour'], hour_stats['ci_lower'], hour_stats['ci_upper'], color=fill_color, alpha=0.3)\n",
    "axes[0].set_xlabel(\"Hour of Day\")\n",
    "axes[0].set_ylabel(\"Mean PM2.5 (Âµg/mÂ³)\")\n",
    "axes[0].set_title(\"(a) PM2.5 Trend by Hour of Day\")\n",
    "axes[0].set_xticks(range(0, 24, 3))  # show every 3 hours\n",
    "axes[0].xaxis.set_minor_locator(MultipleLocator(1))  # add minor ticks\n",
    "axes[0].grid(which='minor', alpha=0.2)\n",
    "axes[0].grid(which='major', alpha=0.5)\n",
    "\n",
    "# 2. Trend by month, accounting for missing data in February, March, April\n",
    "# Define groups of months for which there is data and which should be connected by lines\n",
    "month_groups = [\n",
    "    [1],              # January by itself\n",
    "    [5, 6, 7, 8, 9, 10, 11, 12]  # May - December\n",
    "]\n",
    "\n",
    "# Draw points for all months\n",
    "axes[1].errorbar(month_stats['month'], month_stats['mean'],\n",
    "                yerr=[month_stats['mean'] - month_stats['ci_lower'], month_stats['ci_upper'] - month_stats['mean']],\n",
    "                fmt='o', capsize=5, color=main_color, markersize=8, ecolor='gray', elinewidth=1, zorder=5)\n",
    "\n",
    "# Connect with lines and fill areas only for groups of consecutive months\n",
    "for group in month_groups:\n",
    "    group_months = month_stats[month_stats['month'].isin(group)]\n",
    "    if len(group) > 1:  # connect with a line only if there is more than one month in the group\n",
    "        axes[1].plot(group_months['month'], group_months['mean'], color=main_color, alpha=0.8, zorder=4)\n",
    "        axes[1].fill_between(group_months['month'], group_months['ci_lower'], group_months['ci_upper'], \n",
    "                            color=fill_color, alpha=0.3, zorder=3)\n",
    "\n",
    "axes[1].set_xlabel(\"Month\")\n",
    "axes[1].set_ylabel(\"Mean PM2.5 (Âµg/mÂ³)\")\n",
    "axes[1].set_title(\"(b) PM2.5 Trend by Month\")\n",
    "axes[1].set_xticks(range(1, 13))\n",
    "axes[1].set_xticklabels(['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'])\n",
    "\n",
    "# Add an annotation for months with no data\n",
    "for month in [2, 3, 4]:\n",
    "    axes[1].annotate('n.d.', xy=(month, axes[1].get_ylim()[0] + 5),\n",
    "                   xytext=(0, 5), textcoords='offset points',\n",
    "                   ha='center', fontsize=12, color='gray')\n",
    "\n",
    "# 3. Trend by day of week\n",
    "axes[2].errorbar(day_stats['dayofweek'], day_stats['mean'],\n",
    "                yerr=[day_stats['mean'] - day_stats['ci_lower'], day_stats['ci_upper'] - day_stats['mean']],\n",
    "                fmt='o', capsize=5, color=main_color, markersize=8, ecolor='gray', elinewidth=1)\n",
    "axes[2].plot(day_stats['dayofweek'], day_stats['mean'], color=main_color, alpha=0.8)\n",
    "axes[2].fill_between(day_stats['dayofweek'], day_stats['ci_lower'], day_stats['ci_upper'], color=fill_color, alpha=0.3)\n",
    "axes[2].set_xlabel(\"Day of Week\")\n",
    "axes[2].set_ylabel(\"Mean PM2.5 (Âµg/mÂ³)\")\n",
    "axes[2].set_title(\"(c) PM2.5 Trend by Day of Week\")\n",
    "axes[2].set_xticks(range(0, 7))\n",
    "axes[2].set_xticklabels(['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'])\n",
    "\n",
    "# Add a general title/caption\n",
    "# fig.suptitle('Temporal Patterns in PM2.5 Concentrations', fontsize=16, y=1.02)\n",
    "# fig.text(0.5, 0.01, 'Figure 5. Mean PM2.5 concentrations with 95% confidence intervals by hour of day, month, and day of week.', \n",
    "        #  ha='center', fontsize=12)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.98])\n",
    "plt.savefig(os.path.join(output_dir, 'temporal_trends_PM25.png'), dpi=600, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Figure saved to {os.path.join(output_dir, 'temporal_trends_PM25.png')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation analysis between PM2.5 and temporal features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Assuming the df_interpolated DataFrame is already loaded and contains \"date\" and \"pm2_5\" columns\n",
    "# Create a copy with a unique name\n",
    "df_corr_an_pm_time = df_interpolated.copy()\n",
    "\n",
    "# Convert the 'date' column to datetime if it is not already in that format\n",
    "df_corr_an_pm_time['date'] = pd.to_datetime(df_corr_an_pm_time['date'], errors='coerce')\n",
    "\n",
    "# Extract temporal features\n",
    "df_corr_an_pm_time['month'] = df_corr_an_pm_time['date'].dt.month\n",
    "df_corr_an_pm_time['day'] = df_corr_an_pm_time['date'].dt.day\n",
    "df_corr_an_pm_time['hour'] = df_corr_an_pm_time['date'].dt.hour\n",
    "df_corr_an_pm_time['dayofweek'] = df_corr_an_pm_time['date'].dt.dayofweek\n",
    "# Simple seasonal feature: 1 â€“ winter, 2 â€“ spring, 3 â€“ summer, 4 â€“ autumn\n",
    "df_corr_an_pm_time['season'] = df_corr_an_pm_time['month'] % 12 // 3 + 1\n",
    "\n",
    "# Select columns for analysis\n",
    "cols = ['pm2_5','month', 'day', 'hour', 'dayofweek', 'season']\n",
    "\n",
    "# Calculate the Spearman correlation matrix\n",
    "spearman_corr = df_corr_an_pm_time[cols].corr(method='spearman')\n",
    "print(\"Spearman Correlation Matrix:\")\n",
    "print(spearman_corr)\n",
    "\n",
    "# Visualization using a heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(spearman_corr, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title(\"Spearman Correlation Matrix: PM2.5 and Temporal Features\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logarithmic regression for predicting PM2.5 based on temporal features and analysis of model residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 1ï¸âƒ£ Data preparation from df_corr_an_pm_time\n",
    "# Assuming the df_corr_an_pm_time DataFrame is already loaded and contains \"date\" and \"pm2_5\" columns\n",
    "df_corr_an_pm_time['date'] = pd.to_datetime(df_corr_an_pm_time['date'], errors='coerce')\n",
    "\n",
    "# Extract temporal features\n",
    "df_corr_an_pm_time['year'] = df_corr_an_pm_time['date'].dt.year\n",
    "df_corr_an_pm_time['month'] = df_corr_an_pm_time['date'].dt.month\n",
    "df_corr_an_pm_time['day'] = df_corr_an_pm_time['date'].dt.day\n",
    "df_corr_an_pm_time['hour'] = df_corr_an_pm_time['date'].dt.hour\n",
    "df_corr_an_pm_time['dayofweek'] = df_corr_an_pm_time['date'].dt.dayofweek\n",
    "# Simple seasonal feature: 1 â€“ winter, 2 â€“ spring, 3 â€“ summer, 4 â€“ autumn\n",
    "df_corr_an_pm_time['season'] = df_corr_an_pm_time['month'] % 12 // 3 + 1\n",
    "\n",
    "# Keep only the necessary columns: PM2.5 and temporal features\n",
    "features = ['year', 'month', 'day', 'hour', 'dayofweek', 'season']\n",
    "df_regression = df_corr_an_pm_time[['pm2_5'] + features].dropna()\n",
    "\n",
    "# 2ï¸âƒ£ Logarithmic transformation of the target feature PM2.5\n",
    "df_regression[\"log_pm2_5\"] = np.log1p(df_regression[\"pm2_5\"])  # log(1 + PM2.5)\n",
    "\n",
    "# 3ï¸âƒ£ Logarithmic transformation of temporal features\n",
    "# Use log1p for all temporal features (this is acceptable if values are >= 0)\n",
    "for col in features:\n",
    "    df_regression[f\"log_{col}\"] = np.log1p(df_regression[col])\n",
    "\n",
    "# 4ï¸âƒ£ Training the logarithmic regression\n",
    "# Independent variables: log-transformed temporal features\n",
    "X = df_regression[[f\"log_{col}\" for col in features]]\n",
    "y = df_regression[\"log_pm2_5\"]\n",
    "\n",
    "X = sm.add_constant(X)  # Add a constant for the intercept\n",
    "model = sm.OLS(y, X).fit()\n",
    "\n",
    "# 5ï¸âƒ£ Display model results\n",
    "print(model.summary())\n",
    "\n",
    "# 6ï¸âƒ£ Plot of actual vs. predicted values\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.scatter(y, model.predict(X), alpha=0.5)\n",
    "plt.xlabel(\"Actual log(PM2.5)\")\n",
    "plt.ylabel(\"Predicted log(PM2.5)\")\n",
    "plt.title(\"Actual vs Predicted values (temporal features)\")\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# 7ï¸âƒ£ Histogram of model residuals\n",
    "residuals = y - model.predict(X)\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.histplot(residuals, bins=30, kde=True)\n",
    "plt.xlabel(\"Model Residuals\")\n",
    "plt.title(\"Distribution of residuals (temporal features)\")\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of temporal data and PM2.5 prediction using a tuned Decision Tree Regressor model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import seaborn as sns\n",
    "\n",
    "# 1ï¸âƒ£ Load data and create a copy with a unique name\n",
    "df_model_data = df_interpolated.copy()\n",
    "\n",
    "# Convert the 'date' column to datetime (if it is not already in datetime format)\n",
    "df_model_data['date'] = pd.to_datetime(df_model_data['date'], errors='coerce')\n",
    "\n",
    "# 2ï¸âƒ£ Extract temporal features\n",
    "df_model_data['year'] = df_model_data['date'].dt.year\n",
    "df_model_data['month'] = df_model_data['date'].dt.month\n",
    "df_model_data['day'] = df_model_data['date'].dt.day\n",
    "df_model_data['hour'] = df_model_data['date'].dt.hour\n",
    "df_model_data['dayofweek'] = df_model_data['date'].dt.dayofweek\n",
    "# Simple seasonal feature: 1 â€“ winter, 2 â€“ spring, 3 â€“ summer, 4 â€“ autumn\n",
    "df_model_data['season'] = df_model_data['month'] % 12 // 3 + 1\n",
    "\n",
    "# 3ï¸âƒ£ Select variables for analysis: target feature and temporal features\n",
    "features = ['year', 'month', 'day', 'hour', 'dayofweek', 'season']\n",
    "target = 'pm2_5'\n",
    "\n",
    "# Remove rows with missing values in the selected columns\n",
    "df_model_data = df_model_data.dropna(subset=[target] + features)\n",
    "\n",
    "# 4ï¸âƒ£ Split data into training and testing sets (80% / 20%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_model_data[features], \n",
    "                                                    df_model_data[target], \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=42)\n",
    "\n",
    "# 5ï¸âƒ£ Find optimal model parameters using GridSearchCV\n",
    "param_grid = {\"max_depth\": [3, 5, 10, 15], \"min_samples_split\": [2, 5, 10]}\n",
    "grid_search = GridSearchCV(DecisionTreeRegressor(random_state=42), \n",
    "                           param_grid, \n",
    "                           cv=5, \n",
    "                           scoring=\"r2\")\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# 6ï¸âƒ£ Best decision tree model\n",
    "best_tree = grid_search.best_estimator_\n",
    "\n",
    "# 7ï¸âƒ£ Make a prediction\n",
    "y_pred = best_tree.predict(X_test)\n",
    "\n",
    "# 8ï¸âƒ£ Evaluate the model's quality\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"ðŸ”¹ MAE: {mae:.2f}\")\n",
    "print(f\"ðŸ”¹ RMSE: {rmse:.2f}\")\n",
    "print(f\"ðŸ”¹ RÂ²: {r2:.3f}\")\n",
    "\n",
    "# 9ï¸âƒ£ Visualization of predicted vs. actual values\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \"--\", color=\"red\")  # Ideal prediction line\n",
    "plt.xlabel(\"Actual PM2.5\")\n",
    "plt.ylabel(\"Predicted PM2.5\")\n",
    "plt.title(\"Actual vs Predicted values (Decision Tree on temporal features)\")\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# 10ï¸âƒ£ Feature importance\n",
    "feature_importance = pd.Series(best_tree.feature_importances_, index=features).sort_values(ascending=False)\n",
    "plt.figure(figsize=(10, 5))\n",
    "feature_importance.plot(kind=\"bar\", color=\"royalblue\")\n",
    "plt.title(\"Feature Importance (Decision Tree on temporal features)\")\n",
    "plt.xlabel(\"Features\")\n",
    "plt.ylabel(\"Importance\")\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of temporal features and PM2.5 level prediction using Gradient Boosting Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import seaborn as sns\n",
    "\n",
    "# 1ï¸âƒ£ Load data and create a copy with a unique name\n",
    "df_model_time = df_interpolated.copy()\n",
    "\n",
    "# 2ï¸âƒ£ Convert the 'date' column to datetime (if not already converted)\n",
    "df_model_time['date'] = pd.to_datetime(df_model_time['date'], errors='coerce')\n",
    "\n",
    "# 3ï¸âƒ£ Extract temporal features\n",
    "df_model_time['year'] = df_model_time['date'].dt.year\n",
    "df_model_time['month'] = df_model_time['date'].dt.month\n",
    "df_model_time['day'] = df_model_time['date'].dt.day\n",
    "df_model_time['hour'] = df_model_time['date'].dt.hour\n",
    "df_model_time['dayofweek'] = df_model_time['date'].dt.dayofweek\n",
    "# Simple seasonal feature: 1 â€“ winter, 2 â€“ spring, 3 â€“ summer, 4 â€“ autumn\n",
    "df_model_time['season'] = df_model_time['month'] % 12 // 3 + 1\n",
    "\n",
    "# 4ï¸âƒ£ Select variables for analysis: temporal features\n",
    "features = ['year', 'month', 'day', 'hour', 'dayofweek', 'season']\n",
    "target = 'pm2_5'\n",
    "\n",
    "# 5ï¸âƒ£ Remove rows with missing values in the selected columns\n",
    "df_model_time = df_model_time.dropna(subset=[target] + features)\n",
    "\n",
    "# 6ï¸âƒ£ Split data into training and testing sets (80% / 20%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_model_time[features], \n",
    "                                                    df_model_time[target], \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=42)\n",
    "\n",
    "# 7ï¸âƒ£ Train the Gradient Boosting model (without hyperparameter tuning for simplicity)\n",
    "model = GradientBoostingRegressor(n_estimators=200, learning_rate=0.05, max_depth=4, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 8ï¸âƒ£ Predict values\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# 9ï¸âƒ£ Evaluate the model's quality\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f'ðŸ”¹ MAE: {mae:.2f}')\n",
    "print(f'ðŸ”¹ RMSE: {rmse:.2f}')\n",
    "print(f'ðŸ”¹ RÂ²: {r2:.3f}')\n",
    "\n",
    "# 10ï¸âƒ£ Visualization of predicted vs. actual values\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')  # Ideal prediction line\n",
    "plt.xlabel(\"Actual PM2.5\")\n",
    "plt.ylabel(\"Predicted PM2.5\")\n",
    "plt.title(\"Actual vs Predicted values (temporal features)\")\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# 11ï¸âƒ£ Visualization of feature importance\n",
    "feature_importance = pd.Series(model.feature_importances_, index=features).sort_values(ascending=False)\n",
    "plt.figure(figsize=(10, 5))\n",
    "feature_importance.plot(kind=\"bar\", color=\"royalblue\")\n",
    "plt.title(\"Importance of temporal features (Gradient Boosting)\")\n",
    "plt.xlabel(\"Features\")\n",
    "plt.ylabel(\"Importance\")\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}